\section{Reinforcement Learning}
\begin{frame}
    \frametitle{Reinforcement Learning Overview}
    \setlength{\itemsep}{6pt}
    \begin{itemize}
        \item \textbf{Definition:} A type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.
    \end{itemize}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item \textbf{Key Components:}
                \begin{itemize}
                    \item \textbf{Agent:} The learner or decision maker.
                    \item \textbf{Environment:} The external system with which the agent interacts.
                    \item \textbf{Actions:} Choices made by the agent to influence the environment.
                    \item \textbf{Rewards:} Feedback from the environment based on the agent's actions.
                \end{itemize}
            \end{itemize}
        \end{column}
        \hspace{-1cm}
        \begin{column}{0.5\textwidth}
            % \hspace{-1.9cm}
            \begin{figure}
                \centering
                \resizebox{.85\textwidth}{!}{%
                \begin{tikzpicture}[very thick, node distance=4cm]
                            % Colored nodes
                            \node [frame, fill=myblue!15, text=mydarkblue] (agent) {Agent};
                            \node [frame, below=1.2cm of agent, fill=mygreen!15, text=mydarkgreen] (environment) {Environment};
                            % Action (blue)
                            \draw[line, myblue] (agent) -- ++ (3.5,0) |- (environment)
                            node[right, pos=0.25, align=left, text=myblue] {Action\\ $a_t$};
                            % Separator
                            \coordinate[left=15mm of environment] (P);
                            \draw[thin, dashed, gray] (P|-environment.north) -- (P|-environment.south);
                            % State (green)
                            \draw[line, mygreen] (environment.200) -- (P |- environment.200)
                            node[midway, above, text=mydarkgreen] {$s_{t+1}$};
                            \draw[line, mygreen] (P |- environment.200) -- ++ (-1.6,0) |- (agent.160)
                            node[left, pos=0.25, align=right, text=mydarkgreen] {State\\ $s_t$};
                            % Reward (red)
                            \draw[line, thick, myred] (environment.160) -- (P |- environment.160)
                            node[midway, above, text=myred] {$r_{t+1}$};
                            \draw[line, thick, myred] (P |- environment.160) -- ++ (-1,0) |- (agent.200)
                            node[right, pos=0.25, align=left, text=myred] {Reward\\ $r_t$};
                \end{tikzpicture}%
                }
                \caption{\footnotesize Agent-Environment Interaction Loop}
                \label{fig:agent_env_en}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{State, Observations, and Actions}
    \begin{itemize}
        \item \textbf{State ($\boldsymbol{s}$):} Complete description of the environment at a given time
        \begin{itemize}\setlength{\itemsep}{4pt}
            \item Encodes all variables needed to predict future dynamics
            \item Typically hidden from the agent in real-world problems
        \end{itemize}
        
        \item \textbf{Observation ($\boldsymbol{o}$):} Information perceived by the agent
        \begin{itemize}\setlength{\itemsep}{4pt}
            \item May be noisy or incomplete (partial observability)
            \item In fully observable environments: $s = o$
            \item In partially observable settings: agent must infer hidden aspects of $s$
        \end{itemize}
        
        \item \textbf{Action Space ($\mathcal{A}$):} Set of all possible actions an agent can take
        \begin{itemize}\setlength{\itemsep}{4pt}
            \item \textbf{Discrete:} Finite set of actions (e.g., \texttt{up, down, left, right})
            \item \textbf{Continuous:} Actions represented by real values (e.g., steering angle, force applied)
            % \item Can be multi-dimensional, combining discrete and continuous aspects
        \end{itemize}
    \end{itemize}
\end{frame}



\begin{frame}
    \frametitle{Trajectory and Reward}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \textbf{Definitions:}
            \begin{itemize}\setlength{\itemsep}{6pt}
                \item Trajectory: sequence of states and actions the agent experiences over time.
                \item Reward: scalar feedback provided by the environment after taking an action.
                \item Return: accumulated reward over a trajectory (finite or discounted horizon).
            \end{itemize}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \textbf{Equations:}
            \begin{align*}
                \tau &= (s_0, a_0, s_1, a_1, \ldots) \\
                r_t &= R(s_t, a_t, s_{t+1}) \quad \text{or} \quad r_t = R(s_t, a_t) \\
                R(\tau) &= r_1 + r_2 + \cdots + r_T =
                \sum_{t = 0}^{T} r_t ~ \text{\footnotesize(finite horizon)} \\
                R(\tau) &= r_1 + \gamma r_2 + \gamma^2 r_3 + \cdots =
                \sum_{t = 0}^{\infty} \gamma^t r_t ~ \text{\footnotesize (discounted)}
            \end{align*}
        \end{column}
    \end{columns}
\end{frame}




\begin{frame}
    \frametitle{Policy}
    \begin{itemize}
        \item \textbf{Policy:} Rules that an agent uses to decide which actions to take
    \end{itemize}
    \vspace{5pt}
    \begin{columns}[T]
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item \textbf{Types:}
                \begin{itemize}\itemsep=2pt 
                    \item \textbf{Deterministic:} $a_t = \mu(s_t)\to \text{DDPG, TD3}
                    $
                    \item \textbf{Stochastic:} $a_t \sim \pi(\cdot | s_t)\to \text{PPO, SAC}
                    $
                \end{itemize}
                \item \textbf{Parameterized Policy:} Output is a function of policy parameters (neural network weights)
                \begin{itemize}\itemsep=2pt 
                    \item $a_t = \mu_{\theta}(s_t)$ or $a_t \sim \pi_{\theta}(\cdot | s_t)$
                    \item Parameters $\theta$ are optimized during learning
                \end{itemize}
            \end{itemize}
        \end{column}
        
        \begin{column}{0.35\textwidth}
            \vspace{-1cm}
            \begin{figure}
                \captionsetup[subfloat]{labelfont={color=blue,tiny},textfont=footnotesize,skip=20pt}
                \centering
                \resizebox{.9\textwidth}{!}{%
                \begin{tikzpicture}[x=2.4cm,y=1.2cm]
		\readlist\Nnod{4,5,5,2} % array of number of nodes per layer
		\readlist\Nstr{n,32,k} % array of string number of nodes per layer
		\readlist\Cstr{x,h^{(\prev)},u} % array of coefficient symbol per layer
		\def\yshift{0.55} % shift last node for dots
		% LOOP over LAYERS
		\foreachitem \N \in \Nnod{
			\def\lay{\Ncnt} % alias of index of current layer
			\pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
			\foreach \i [evaluate={\c=int(\i==\N);
				\layercheck=\ifnum\Ncnt=1 0 \else \ifnum\Ncnt=\Nnodlen 0 \else \yshift \fi \fi;
				\y=\N/2-\i*1.2-\c*\layercheck;
				\x=\lay; \n=\nstyle;
				\index=(\i<\N?int(\i):"\Nstr[\n]");}] in {1,...,\N}{ % loop over nodes
				% NODES
				\ifnum \lay=1
				\ifnum \i=1
				\node[node \n] (N\lay-\i) at (\x,\y) {$\delta x$};
				\fi
				\ifnum \i=2
				\node[node \n] (N\lay-\i) at (\x,\y) {$\delta y$};
				\fi
				\ifnum \i=3
				\node[node \n] (N\lay-\i) at (\x,\y) {$\delta {\dot{x}}$};
				\fi
				\ifnum \i=4
				\node[node \n] (N\lay-\i) at (\x,\y) {$\delta {\dot{y}}$};
				\fi
				\else \ifnum \lay=\Nnodlen
				\ifnum \i=1
				\node[node \n] (N\lay-\i) at (\x,\y) {$u_x$};
				\fi
				\ifnum \i=2
				\node[node \n] (N\lay-\i) at (\x,\y) {$u_y$};
				\fi
				\else
				\node[node \n] (N\lay-\i) at (\x,\y) {$\strut\Cstr[\n]_{\index}$};
				\fi \fi
				% CONNECTIONS
				\ifnumcomp{\lay}{>}{1}{ % connect to previous layer
					\foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
						\draw[white,line width=1.2,shorten >=1] (N\prev-\j) -- (N\lay-\i);
						\draw[connect] (N\prev-\j) -- (N\lay-\i);
					}
				}{}
			}
			% Dots (skip first and last layers)
			\ifnum \lay>1 \ifnum \lay<\Nnodlen
			\path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.6] {$\vdots$}; % dots
			\fi \fi
		}
		% LABELS
		\node[above=.1,align=center,mydarkgreen] at (N1-1.90) {Input\\[-0.2em]Layer};
		\node[above=.1,align=center,mydarkblue] at (N2-1.90) {Hidden\\[-0.2em]Layer};
		\node[above=.1,align=center,mydarkblue] at (N3-1.90) {Hidden\\[-0.2em]Layer};
		\node[above=.1,align=center,mydarkred] at (N\Nnodlen-1.90) {Output\\[-0.2em]Layer};
	\end{tikzpicture}%
                }
                \captionsetup{labelfont={color=myBlue,scriptsize},textfont=scriptsize,skip=8pt}
                \caption{Policy Neural Network Structure}
                \label{fig:actor_nn_en}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{Value and Action-Value Functions}
    \begin{itemize}
        \item \textbf{Value Function:} Expected return when following a policy
    \end{itemize}
    \vspace{5pt}
    \setlength{\itemsep}{6pt}
    \begin{columns}[T]
        \begin{column}{0.55\textwidth}
            \textbf{Value Function:}
            \begin{align*}
                V^{\pi}(s) &= \underset{\tau \sim \pi}{\mathbb{E}}\left[R(\tau)|s_0 = s\right]
            \end{align*}
            \vspace{0.3cm}
            \textbf{Action-Value Function:}
            \begin{align*}
                Q^{\pi}(s,a) &= \underset{\tau \sim \pi}{\mathbb{E}}\left[R(\tau)|s_0 = s, a_0 = a\right]
            \end{align*}
            \vspace{0.3cm}
            \textbf{Advantage Function:}
            \begin{align*}
                A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)
            \end{align*}
        \end{column}
        
        \begin{column}{0.35\textwidth}
            \vspace{-1.1cm}
            % \hspace{.9cm}
            \begin{figure}
                % \centering
                \resizebox{.90\textwidth}{!}{%
                \begin{tikzpicture}[x=2.8cm,y=1.5cm]
                    \readlist\Nnod{6,7,7,1} % array of number of nodes per layer
                    \readlist\Nstr{n,32,k} % array of string number of nodes per layer
                    \readlist\Cstr{x,h^{(\prev)},u} % array of coefficient symbol per layer
                    \def\yshift{0.55} % shift last node for dots
                    
                    % LOOP over LAYERS
                    \foreachitem \N \in \Nnod{
                        \def\lay{\Ncnt} % alias of index of current layer
                        \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
                        \foreach \i [evaluate={\c=int(\i==\N); 
                            \layercheck=\ifnum\Ncnt=1 0 \else \ifnum\Ncnt=\Nnodlen 0 \else \yshift \fi \fi;
                            \y=\N/2-\i-\c*\layercheck;
                            \x=\lay; \n=\nstyle;
                            \index=(\i<\N?int(\i):"\Nstr[\n]");}] in {1,...,\N}{ % loop over nodes
                            % NODES
                            \ifnum \lay=1
                            \ifnum \i=1
                            \node[node \n] (N\lay-\i) at (\x,\y) {$\delta x$};
                            \fi
                            \ifnum \i=2
                            \node[node \n] (N\lay-\i) at (\x,\y) {$\delta y$};
                            \fi
                            \ifnum \i=3
                            \node[node \n] (N\lay-\i) at (\x,\y) {$\delta {\dot{x}}$};
                            \fi
                            \ifnum \i=4
                            \node[node \n] (N\lay-\i) at (\x,\y) {$\delta {\dot{y}}$};
                            \fi
                            \ifnum \i=5
                            \node[node \n] (N\lay-\i) at (\x,\y) {$u_x$};
                            \fi
                            \ifnum \i=6
                            \node[node \n] (N\lay-\i) at (\x,\y) {$u_y$};
                            \fi
                            \else \ifnum \lay=\Nnodlen
                            \ifnum \i=1
                            \node[node \n] (N\lay-\i) at (\x,\y) {$Q$};
                            \fi
                            \ifnum \i=2
                            \node[node \n] (N\lay-\i) at (\x,\y) {$u_y$};
                            \fi
                            \else
                            \node[node \n] (N\lay-\i) at (\x,\y) {$\strut\Cstr[\n]_{\index}$};
                            \fi \fi
                            % CONNECTIONS
                            \ifnumcomp{\lay}{>}{1}{ % connect to previous layer
                                \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                                    \draw[white,line width=1.2,shorten >=1] (N\prev-\j) -- (N\lay-\i);
                                    \draw[connect] (N\prev-\j) -- (N\lay-\i);
                                }
                            }{}
                        }
                        
                        % Dots (skip first and last layers)
                        \ifnum \lay>1 \ifnum \lay<\Nnodlen
                        \path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.6] {$\vdots$}; % dots
                        \fi \fi
                    }
                    
                    % LABELS
                    \node[above=.1,align=center,mydarkgreen] at (N1-1.90) {Input\\[-0.2em]Layer};
                    \node[above=.1,align=center,mydarkblue] at (N2-1.90) {Hidden\\[-0.2em]Layer};
                    \node[above=.1,align=center,mydarkblue] at (N3-1.90) {Hidden\\[-0.2em]Layer};
                    \node[above=.1,align=center,mydarkred] at (N\Nnodlen-1.25) {Output\\[-0.2em]Layer};
                \end{tikzpicture}
                }
                \captionsetup{labelfont={color=myBlue,scriptsize},textfont=scriptsize,skip=4pt}
                \caption{Action-Value Function Neural Network}
                \label{fig:critic_nn_en}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

% \begin{frame}
%     \frametitle{Optimal Value Functions}
    
%     \begin{columns}[T]
%         \begin{column}{0.5\textwidth}
%             \textbf{Optimal State Value Function:}
%             \begin{align*}
%                 V^{*}(s) &= \underset{\pi}{\max} V^{\pi}(s)
%             \end{align*}
%             \vspace{0.1cm}
%             \textbf{Optimal Value Bellman Equation:}
%             \begin{align*}
%                 V^{*}(s) &= \max_a \underset{s'\sim P}{\mathrm{E}} \left[ r(s,a) + \gamma V^{*}(s') \right]
%             \end{align*}
%         \end{column}
        
%         \begin{column}{0.5\textwidth}
%             \textbf{Optimal Action-Value Function:}
%             \begin{align*}
%                 Q^{*}(s,a) &= \underset{\pi}{\max} Q^{\pi}(s,a)
%             \end{align*}
%             \vspace{0.1cm}
%             \textbf{Optimal Q Bellman Equation:}
%             \begin{align*}
%                 Q^{*}(s,a) &= r(s,a) + \gamma \underset{s'\sim P}{\mathrm{E}} \left[ \max_{a'} Q^{*}(s',a') \right]
%             \end{align*}
%         \end{column}
%     \end{columns}
    
%     % \vspace{0.4cm}
%     \begin{minipage}{0.83\textwidth}
%     \begin{block}{Value Computation}
%         How can we calculate the value of a state $V(s)$ 
%         and a state-action pair $Q(s,a)$?
%     \end{block}
% \end{minipage}

% \end{frame}

% \begin{frame}
%     \frametitle{Bellman Equations}
    
%     \textbf{For Policy Value Functions:}
%     \begin{align*}
%         V^{\pi}(s) &= \underset{\substack{a \sim \pi \\ s'\sim P}}{\mathrm{E}} 
%         \left[ r(s,a) + \gamma V^{\pi}(s') \right] \\
%         Q^{\pi}(s,a) &= r(s,a) + \gamma \underset{s'\sim P}{\mathrm{E}} 
%         \left[ \underset{a'\sim \pi}{\mathrm{E}} \left[ Q^{\pi}(s',a') \right] \right]
%     \end{align*}
    
%     \vspace{0.3cm}
%     \textbf{For Optimal Value Functions:}
%     \begin{align*}
%         V^{*}(s) &= \max_a \underset{s'\sim P}{\mathrm{E}} \left[ r(s,a) + \gamma V^{*}(s') \right] \\
%         Q^{*}(s,a) &= r(s,a) + \gamma \underset{s'\sim P}{\mathrm{E}} \left[ \max_{a'} Q^{*}(s',a') \right]
%     \end{align*}
% \end{frame}



\begin{frame}
  \frametitle{Value Computation and Bellman Equations}

  \begin{block}{Value Computation}
    How can we calculate the value of a state $V(s)$ 
    and a state-action pair $Q(s,a)$?
  \end{block}

%   \vspace{0.3cm}

  \textbf{For Policy Value Functions:}
  \begin{align*}
      V^{\pi}(s) &= \underset{\substack{a \sim \pi \\ s'\sim P}}{\mathrm{E}} 
      \left[ r(s,a) + \gamma V^{\pi}(s') \right] \\[-.5em]
      Q^{\pi}(s,a) &= r(s,a) + \gamma \underset{s'\sim P}{\mathrm{E}} 
      \left[ \underset{a'\sim \pi}{\mathrm{E}} \left[ Q^{\pi}(s',a') \right] \right]
  \end{align*}

%   \vspace{0.3cm}

  \textbf{For Optimal Value Functions:}
  \begin{align*}
    V^{*}(s) = \underset{\pi}{\max} V^{\pi}(s) \to
      V^{*}(s) &= \max_a \underset{s'\sim P}{\mathrm{E}} \left[ r(s,a) + \gamma V^{*}(s') \right] \\[-.5em]
      Q^{*}(s,a) = \underset{\pi}{\max} Q^{\pi}(s,a) \to
      Q^{*}(s,a) &= r(s,a) + \gamma \underset{s'\sim P}{\mathrm{E}} \left[ \max_{a'} Q^{*}(s',a') \right]
  \end{align*}

\end{frame}
