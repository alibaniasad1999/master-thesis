{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy and value model\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.policy_layer = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.value_layer = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def value(self, obs):\n",
    "        shared_out = self.shared_layer(obs)\n",
    "        value = self.value_layer(shared_out)\n",
    "        return value\n",
    "\n",
    "    def policy(self, obs):\n",
    "        shared_out = self.shared_layer(obs)\n",
    "        policy_logits = self.policy_layer(shared_out)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        shared_out = self.shared_layer(obs)\n",
    "        policy_logits = self.policy_layer(shared_out)\n",
    "        value = self.value_layer(shared_out)\n",
    "        return policy_logits, value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "    def __init__(self, \n",
    "                actor_critic,\n",
    "                ppo_clip_val=0.2,\n",
    "                target_k1_div=0.01,\n",
    "                max_policy_train_iters=80,\n",
    "                value_train_lters=80,\n",
    "                policy_lr=3e-4,\n",
    "                value_lr=1e-2):\n",
    "            \n",
    "        self.actor_critic = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_val\n",
    "        self.target_k1_div = target_k1_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_lters = value_train_lters\n",
    "\n",
    "        policy_params = list(self.actor_critic.shared_layer.parameters()) + list(self.actor_critic.policy_layer.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "\n",
    "        value_params = list(self.actor_critic.shared_layer.parameters()) + list(self.actor_critic.value_layer.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
    "\n",
    "    def train_policy(self, obs, actions, old_log_probs, gaes):\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "            self.policy_optim.zero_grad()\n",
    "            print(obs)\n",
    "            new_logits, new_values = self.actor_critic(obs)\n",
    "            new_log_probs = Categorical(logits=new_logits).log_prob(actions)\n",
    "            new_log_probs = new_logits.log_prob(actions)\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = torch.clamp(ratio, 1-self.ppo_clip_val, 1+self.ppo_clip_val)\n",
    "\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = ratio * gaes\n",
    "            policy_loss = -torch.min(clipped_loss, full_loss).mean()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div > self.target_k1_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_train_lters):\n",
    "            self.value_optim.zero_grad()\n",
    "            values = self.actor_critic.value(obs)\n",
    "            value_loss = nn.MSELoss()(values, returns)\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for r in rewards[-2::-1]:\n",
    "        new_rewards.append(r + gamma * new_rewards[-1])\n",
    "    return new_rewards[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, gamma=0.99, lam=0.95):\n",
    "    deltas = [r + gamma * v_ - v for r, v_, v in zip(rewards[:-1], values[1:], values[:-1])]\n",
    "    deltas.append(rewards[-1] - values[-1])\n",
    "    gaes = [deltas[-1]]\n",
    "    for delta in deltas[-2::-1]:\n",
    "        gaes.append(delta + gamma * lam * gaes[-1])\n",
    "    return gaes[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    train_data = [[], [], [], [], []]\n",
    "    obs = env.reset()\n",
    "    obs = obs[0]\n",
    "\n",
    "    ep_rewards = 0\n",
    "    for _ in range(max_steps):\n",
    "        print(obs)\n",
    "        logits, value = model(torch.tensor([obs], dtype=torch.float32, device=DEVICE))\n",
    "        action_dist = Categorical(logits=logits)\n",
    "        action = action_dist.sample()\n",
    "        log_prob = action_dist.log_prob(action).item()\n",
    "\n",
    "        new_obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "        # Append items with consistent shapes\n",
    "        train_data[0].append(np.array([obs]))  # Ensure obs has consistent shape\n",
    "        train_data[1].append(action.item())\n",
    "        train_data[2].append(log_prob)\n",
    "        train_data[3].append(reward)\n",
    "        train_data[4].append(done)\n",
    "        obs = new_obs\n",
    "        ep_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Convert each sublist to a NumPy array\n",
    "    train_data = [np.asarray(item) for item in train_data]\n",
    "\n",
    "    # Ensure consistent shapes when using np.asarray\n",
    "    train_data[0] = np.concatenate(train_data[0])  # Concatenate arrays in the first element\n",
    "\n",
    "    train_data[3] = compute_gae(train_data[2], train_data[3], gamma=0.99, lam=0.95)\n",
    "\n",
    "    return train_data, ep_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0219925  -0.03346879  0.04478336 -0.02850489]\n",
      "[ 0.02132313 -0.2292034   0.04421326  0.27796456]\n",
      "[ 0.01673906 -0.03473918  0.04977255 -0.00045225]\n",
      "[ 0.01604427 -0.2305383   0.0497635   0.3075098 ]\n",
      "[ 0.01143351 -0.4263327   0.0559137   0.6154623 ]\n",
      "[ 0.00290685 -0.23203473  0.06822295  0.34090084]\n",
      "[-0.00173384 -0.03794643  0.07504097  0.07048763]\n",
      "[-0.00249277 -0.23405953  0.07645071  0.38587075]\n",
      "[-0.00717396 -0.43017882  0.08416813  0.701645  ]\n",
      "[-0.01577754 -0.23631814  0.09820103  0.43659964]\n",
      "[-0.0205039  -0.43268305  0.10693302  0.7585527 ]\n",
      "[-0.02915756 -0.23918438  0.12210408  0.5013409 ]\n",
      "[-0.03394125 -0.43579683  0.13213089  0.82987326]\n",
      "[-0.04265719 -0.6324535   0.14872836  1.1610205 ]\n",
      "[-0.05530626 -0.8291662   0.17194878  1.4963973 ]\n",
      "[-0.07188958 -1.0259106   0.20187671  1.837466  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n).to(DEVICE)\n",
    "train_data, ep_rewards = rollout(model, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.1992503e-02 -3.3468794e-02  4.4783358e-02 -2.8504891e-02]\n",
      " [ 2.1323126e-02 -2.2920340e-01  4.4213258e-02  2.7796456e-01]\n",
      " [ 1.6739057e-02 -3.4739178e-02  4.9772549e-02 -4.5225484e-04]\n",
      " [ 1.6044274e-02 -2.3053829e-01  4.9763504e-02  3.0750981e-01]\n",
      " [ 1.1433508e-02 -4.2633271e-01  5.5913702e-02  6.1546230e-01]\n",
      " [ 2.9068540e-03 -2.3203473e-01  6.8222947e-02  3.4090084e-01]\n",
      " [-1.7338406e-03 -3.7946429e-02  7.5040966e-02  7.0487626e-02]\n",
      " [-2.4927692e-03 -2.3405953e-01  7.6450713e-02  3.8587075e-01]\n",
      " [-7.1739596e-03 -4.3017882e-01  8.4168129e-02  7.0164502e-01]\n",
      " [-1.5777536e-02 -2.3631814e-01  9.8201029e-02  4.3659964e-01]\n",
      " [-2.0503899e-02 -4.3268305e-01  1.0693302e-01  7.5855267e-01]\n",
      " [-2.9157560e-02 -2.3918438e-01  1.2210408e-01  5.0134093e-01]\n",
      " [-3.3941247e-02 -4.3579683e-01  1.3213089e-01  8.2987326e-01]\n",
      " [-4.2657185e-02 -6.3245350e-01  1.4872836e-01  1.1610205e+00]\n",
      " [-5.5306256e-02 -8.2916617e-01  1.7194878e-01  1.4963973e+00]\n",
      " [-7.1889579e-02 -1.0259106e+00  2.0187671e-01  1.8374660e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPOTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03236194 0.03110879 0.00581208 0.04430943]\n",
      "[ 0.03298412  0.22614692  0.00669827 -0.24653408]\n",
      "[0.03750705 0.03092995 0.00176759 0.04825409]\n",
      "[ 0.03812565 -0.16421731  0.00273267  0.34149417]\n",
      "[0.03484131 0.03086566 0.00956255 0.04967423]\n",
      "[ 0.03545862 -0.16439208  0.01055604  0.34535882]\n",
      "[0.03217078 0.03057812 0.01746321 0.05602321]\n",
      "[ 0.03278234  0.22544537  0.01858368 -0.23109919]\n",
      "[ 0.03729125  0.4202969   0.01396169 -0.51786274]\n",
      "[ 0.04569719  0.2249812   0.00360444 -0.22081311]\n",
      "[ 0.05019681  0.02980792 -0.00081182  0.07300462]\n",
      "[ 0.05079297 -0.16530238  0.00064827  0.3654313 ]\n",
      "[0.04748692 0.02981034 0.0079569  0.07295285]\n",
      "[ 0.04808313  0.22481732  0.00941595 -0.21720906]\n",
      "[0.05257947 0.02956204 0.00507177 0.07842913]\n",
      "[ 0.05317071 -0.16563225  0.00664035  0.3727079 ]\n",
      "[0.04985807 0.02939475 0.01409451 0.08212611]\n",
      "[ 0.05044596 -0.16592638  0.01573703  0.37922236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04712744 -0.36126825  0.02332148  0.67682534]\n",
      "[ 0.03990207 -0.16647796  0.03685799  0.39157528]\n",
      "[0.03657251 0.02810205 0.0446895  0.11073729]\n",
      "[ 0.03713455  0.22255608  0.04690424 -0.1675182 ]\n",
      "[ 0.04158568  0.41697636  0.04355388 -0.4450431 ]\n",
      "[ 0.0499252   0.22126612  0.03465302 -0.13895534]\n",
      "[0.05435053 0.02566543 0.03187391 0.1644555 ]\n",
      "[ 0.05486383 -0.16989796  0.03516302  0.46702093]\n",
      "[ 0.05146587 -0.3654986   0.04450344  0.77057636]\n",
      "[ 0.0441559  -0.17101645  0.05991497  0.49222165]\n",
      "[ 0.04073557 -0.36693007  0.0697594   0.8031692 ]\n",
      "[ 0.03339697 -0.17283046  0.08582278  0.5332206 ]\n",
      "[ 0.02994036 -0.36904794  0.09648719  0.8516644 ]\n",
      "[ 0.0225594  -0.5653436   0.11352048  1.1730616 ]\n",
      "[ 0.01125253 -0.7617432   0.13698171  1.4990681 ]\n",
      "[-0.00398233 -0.5685255   0.16696307  1.2521034 ]\n",
      "[-0.01535284 -0.3758889   0.19200514  1.0160261 ]\n",
      "tensor([[ 3.2362e-02,  3.1109e-02,  5.8121e-03,  4.4309e-02],\n",
      "        [ 3.2984e-02,  2.2615e-01,  6.6983e-03, -2.4653e-01],\n",
      "        [ 3.7507e-02,  3.0930e-02,  1.7676e-03,  4.8254e-02],\n",
      "        [ 3.8126e-02, -1.6422e-01,  2.7327e-03,  3.4149e-01],\n",
      "        [ 3.4841e-02,  3.0866e-02,  9.5626e-03,  4.9674e-02],\n",
      "        [ 3.5459e-02, -1.6439e-01,  1.0556e-02,  3.4536e-01],\n",
      "        [ 3.2171e-02,  3.0578e-02,  1.7463e-02,  5.6023e-02],\n",
      "        [ 3.2782e-02,  2.2545e-01,  1.8584e-02, -2.3110e-01],\n",
      "        [ 3.7291e-02,  4.2030e-01,  1.3962e-02, -5.1786e-01],\n",
      "        [ 4.5697e-02,  2.2498e-01,  3.6044e-03, -2.2081e-01],\n",
      "        [ 5.0197e-02,  2.9808e-02, -8.1182e-04,  7.3005e-02],\n",
      "        [ 5.0793e-02, -1.6530e-01,  6.4827e-04,  3.6543e-01],\n",
      "        [ 4.7487e-02,  2.9810e-02,  7.9569e-03,  7.2953e-02],\n",
      "        [ 4.8083e-02,  2.2482e-01,  9.4160e-03, -2.1721e-01],\n",
      "        [ 5.2579e-02,  2.9562e-02,  5.0718e-03,  7.8429e-02],\n",
      "        [ 5.3171e-02, -1.6563e-01,  6.6404e-03,  3.7271e-01],\n",
      "        [ 4.9858e-02,  2.9395e-02,  1.4095e-02,  8.2126e-02],\n",
      "        [ 5.0446e-02, -1.6593e-01,  1.5737e-02,  3.7922e-01],\n",
      "        [ 4.7127e-02, -3.6127e-01,  2.3321e-02,  6.7683e-01],\n",
      "        [ 3.9902e-02, -1.6648e-01,  3.6858e-02,  3.9158e-01],\n",
      "        [ 3.6573e-02,  2.8102e-02,  4.4689e-02,  1.1074e-01],\n",
      "        [ 3.7135e-02,  2.2256e-01,  4.6904e-02, -1.6752e-01],\n",
      "        [ 4.1586e-02,  4.1698e-01,  4.3554e-02, -4.4504e-01],\n",
      "        [ 4.9925e-02,  2.2127e-01,  3.4653e-02, -1.3896e-01],\n",
      "        [ 5.4351e-02,  2.5665e-02,  3.1874e-02,  1.6446e-01],\n",
      "        [ 5.4864e-02, -1.6990e-01,  3.5163e-02,  4.6702e-01],\n",
      "        [ 5.1466e-02, -3.6550e-01,  4.4503e-02,  7.7058e-01],\n",
      "        [ 4.4156e-02, -1.7102e-01,  5.9915e-02,  4.9222e-01],\n",
      "        [ 4.0736e-02, -3.6693e-01,  6.9759e-02,  8.0317e-01],\n",
      "        [ 3.3397e-02, -1.7283e-01,  8.5823e-02,  5.3322e-01],\n",
      "        [ 2.9940e-02, -3.6905e-01,  9.6487e-02,  8.5166e-01],\n",
      "        [ 2.2559e-02, -5.6534e-01,  1.1352e-01,  1.1731e+00],\n",
      "        [ 1.1253e-02, -7.6174e-01,  1.3698e-01,  1.4991e+00],\n",
      "        [-3.9823e-03, -5.6853e-01,  1.6696e-01,  1.2521e+00],\n",
      "        [-1.5353e-02, -3.7589e-01,  1.9201e-01,  1.0160e+00]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'log_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[284], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m returns \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(discount_rewards(train_data[\u001b[38;5;241m2\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m     12\u001b[0m act_log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_data[\u001b[38;5;241m4\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_log_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgae\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m ppo\u001b[38;5;241m.\u001b[39mtrain_value(obs, returns)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# write rewards \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[277], line 29\u001b[0m, in \u001b[0;36mPPOTrainer.train_policy\u001b[0;34m(self, obs, actions, old_log_probs, gaes)\u001b[0m\n\u001b[1;32m     27\u001b[0m new_logits, new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic(obs)\n\u001b[1;32m     28\u001b[0m new_log_probs \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mnew_logits)\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[0;32m---> 29\u001b[0m new_log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnew_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_probs\u001b[49m(actions)\n\u001b[1;32m     31\u001b[0m ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(new_log_probs \u001b[38;5;241m-\u001b[39m old_log_probs)\n\u001b[1;32m     32\u001b[0m clipped_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(ratio, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_clip_val, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_clip_val)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'log_probs'"
     ]
    }
   ],
   "source": [
    "ep_rewards = []\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    train_data, rewards = rollout(model, env)\n",
    "    ep_rewards.append(rewards)\n",
    "\n",
    "    obs = torch.tensor(train_data[0], dtype=torch.float32, device=DEVICE)\n",
    "    actions = torch.tensor(train_data[1], dtype=torch.int64, device=DEVICE)\n",
    "    old_log_probs = torch.tensor(train_data[2], dtype=torch.float32, device=DEVICE)\n",
    "    gae = torch.tensor(train_data[3], dtype=torch.float32, device=DEVICE)\n",
    "    returns = torch.tensor(discount_rewards(train_data[2]), dtype=torch.float32, device=DEVICE)\n",
    "    act_log_probs = torch.tensor(train_data[4], dtype=torch.float32, device=DEVICE)\n",
    "    \n",
    "    ppo.train_policy(obs, actions, old_log_probs, gae)\n",
    "    ppo.train_value(obs, returns)\n",
    "    # write rewards \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Episode {i}, rewards: {rewards}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
