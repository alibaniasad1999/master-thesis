{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "base_env = GymEnv('Pendulum-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "reset = base_env.reset()\n",
    "print(reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv('Pendulum-v1')\n",
    "module = torch.nn.LazyLinear(out_features=env.action_space.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TensorDictModule(\n",
    "    module=module,\n",
    "    in_keys = [\"observation\"],\n",
    "    out_keys = [\"action\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import MLP\n",
    "\n",
    "module = MLP(\n",
    "    out_features=env.action_space.shape[-1],\n",
    "    num_cells=[32, 64],\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/cymj.pyx because it changed.\n",
      "[1/1] Cythonizing /Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/cymj.pyx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "performance hint: /Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/cymj.pyx:67:5: Exception check on 'c_warning_callback' will always require the GIL to be acquired.\n",
      "Possible solutions:\n",
      "\t1. Declare 'c_warning_callback' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
      "\t2. Use an 'int' return type on 'c_warning_callback' to allow an error code to be returned.\n",
      "performance hint: /Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/cymj.pyx:104:5: Exception check on 'c_error_callback' will always require the GIL to be acquired.\n",
      "Possible solutions:\n",
      "\t1. Declare 'c_error_callback' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
      "\t2. Use an 'int' return type on 'c_error_callback' to allow an error code to be returned.\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    See c_warning_callback, which is the C wrapper to the user defined function\n",
      "    '''\n",
      "    global py_warning_callback\n",
      "    global mju_user_warning\n",
      "    py_warning_callback = warn\n",
      "    mju_user_warning = c_warning_callback\n",
      "                       ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/cymj.pyx:92:23: Cannot assign type 'void (const char *) except * nogil' to 'void (*)(const char *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'void (const char *) except * nogil'.\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    See c_warning_callback, which is the C wrapper to the user defined function\n",
      "    '''\n",
      "    global py_error_callback\n",
      "    global mju_user_error\n",
      "    py_error_callback = err_callback\n",
      "    mju_user_error = c_error_callback\n",
      "                     ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/cymj.pyx:127:21: Cannot assign type 'void (const char *) except * nogil' to 'void (*)(const char *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'void (const char *) except * nogil'.\n"
     ]
    },
    {
     "ename": "CompileError",
     "evalue": "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/cymj.pyx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 228\u001b[0m\n\u001b[1;32m    213\u001b[0m entropy_eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Define an environment\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# with another. For example, creating a wrapped gym environment can be achieved with few characters:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m base_env \u001b[38;5;241m=\u001b[39m \u001b[43mGymEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInvertedDoublePendulum-v4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# There are a few things to notice in this code: first, we created\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# the environment by calling the ``GymEnv`` wrapper. If extra keyword arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# entry and this entry only:\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    282\u001b[0m env \u001b[38;5;241m=\u001b[39m TransformedEnv(\n\u001b[1;32m    283\u001b[0m     base_env,\n\u001b[1;32m    284\u001b[0m     Compose(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m     ),\n\u001b[1;32m    290\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torchrl/envs/libs/gym.py:533\u001b[0m, in \u001b[0;36m_AsyncMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 533\u001b[0m     instance: GymWrapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# before gym 0.22, there was no final_observation\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance\u001b[38;5;241m.\u001b[39m_is_batched:\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torchrl/envs/common.py:158\u001b[0m, in \u001b[0;36m_EnvPostInit.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 158\u001b[0m     instance: EnvBase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# we create the done spec by adding a done/terminated entry if one is missing\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     instance\u001b[38;5;241m.\u001b[39m_create_done_specs()\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torchrl/envs/libs/gym.py:1251\u001b[0m, in \u001b[0;36mGymEnv.__init__\u001b[0;34m(self, env_name, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m env_name\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_gym_args(kwargs)\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torchrl/envs/libs/gym.py:721\u001b[0m, in \u001b[0;36mGymWrapper.__init__\u001b[0;34m(self, env, categorical_action_encoding, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_init()\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torchrl/envs/common.py:2765\u001b[0m, in \u001b[0;36m_EnvWrapper.__init__\u001b[0;34m(self, dtype, device, batch_size, allow_done_after_reset, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   2764\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[0;32m-> 2765\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# writes the self._env attribute\u001b[39;00m\n\u001b[1;32m   2766\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_specs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env)  \u001b[38;5;66;03m# writes the self._env attribute\u001b[39;00m\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torchrl/envs/libs/gym.py:1303\u001b[0m, in \u001b[0;36mGymEnv._build_env\u001b[0;34m(self, env_name, **kwargs)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;66;03m# we catch warnings as they may cause silent bugs\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m         env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframeskip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(w[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage):\n\u001b[1;32m   1305\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframeskip\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gymnasium/envs/registration.py:756\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 756\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    759\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gymnasium/envs/registration.py:545\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 545\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.8/3.8.18_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:961\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gymnasium/envs/mujoco/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MujocoEnv, MuJocoPyEnv  \u001b[38;5;66;03m# isort:skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ^^^^^ so that user gets the correct error\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# message if mujoco is not installed correctly\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AntEnv\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gymnasium/envs/mujoco/mujoco_env.py:13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Space\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     15\u001b[0m     MUJOCO_PY_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cymj, ignore_mujoco_warnings, functions, MujocoException\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m const\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmjrenderpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MjRenderPool\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/builder.py:504\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m__fun\n\u001b[1;32m    503\u001b[0m mujoco_path \u001b[38;5;241m=\u001b[39m discover_mujoco()\n\u001b[0;32m--> 504\u001b[0m cymj \u001b[38;5;241m=\u001b[39m \u001b[43mload_cython_ext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmujoco_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Trick to expose all mj* functions from mujoco in mujoco_py.*\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdict2\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/builder.py:110\u001b[0m, in \u001b[0;36mload_cython_ext\u001b[0;34m(mujoco_path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImport error. Trying to rebuild mujoco_py.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         cext_so_path \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m         mod \u001b[38;5;241m=\u001b[39m load_dynamic_ext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcymj\u001b[39m\u001b[38;5;124m'\u001b[39m, cext_so_path)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mod\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/builder.py:226\u001b[0m, in \u001b[0;36mMujocoExtensionBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 226\u001b[0m     built_so_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     new_so_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_so_file_path()\n\u001b[1;32m    228\u001b[0m     move(built_so_file_path, new_so_file_path)\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/builder.py:340\u001b[0m, in \u001b[0;36mMacExtensionBuilder._build_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find supported GCC executable.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHINT: On OS X, install GCC 9.x with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`brew install gcc@9`. or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    337\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`port install gcc9`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    338\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCC\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m available_c_compiler\n\u001b[0;32m--> 340\u001b[0m     so_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCC\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# User-directed c compiler\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/builder.py:239\u001b[0m, in \u001b[0;36mMujocoExtensionBuilder._build_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    235\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Distribution({\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_ext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    238\u001b[0m     })\n\u001b[0;32m--> 239\u001b[0m     dist\u001b[38;5;241m.\u001b[39mext_modules \u001b[38;5;241m=\u001b[39m \u001b[43mcythonize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     dist\u001b[38;5;241m.\u001b[39minclude_dirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    241\u001b[0m     dist\u001b[38;5;241m.\u001b[39mcmdclass \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuild_ext\u001b[39m\u001b[38;5;124m'\u001b[39m: custom_build_ext}\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/Cython/Build/Dependencies.py:1154\u001b[0m, in \u001b[0;36mcythonize\u001b[0;34m(module_list, exclude, nthreads, aliases, quiet, force, language, exclude_failures, show_all_warnings, **options)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m to_compile:\n\u001b[0;32m-> 1154\u001b[0m         \u001b[43mcythonize_one\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exclude_failures:\n\u001b[1;32m   1157\u001b[0m     failed_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/Cython/Build/Dependencies.py:1321\u001b[0m, in \u001b[0;36mcythonize_one\u001b[0;34m(pyx_file, c_file, fingerprint, quiet, options, raise_on_failure, embedded_metadata, full_module_name, show_all_warnings, progress)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_failures:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[0;32m-> 1321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CompileError(\u001b[38;5;28;01mNone\u001b[39;00m, pyx_file)\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(c_file):\n\u001b[1;32m   1323\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(c_file)\n",
      "\u001b[0;31mCompileError\u001b[0m: /Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/mujoco_py/cymj.pyx"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Reinforcement Learning (PPO) with TorchRL Tutorial\n",
    "==================================================\n",
    "**Author**: `Vincent Moens <https://github.com/vmoens>`_\n",
    "\n",
    ".. _coding_ppo:\n",
    "\n",
    "This tutorial demonstrates how to use PyTorch and :py:mod:`torchrl` to train a parametric policy\n",
    "network to solve the Inverted Pendulum task from the `OpenAI-Gym/Farama-Gymnasium\n",
    "control library <https://github.com/Farama-Foundation/Gymnasium>`__.\n",
    "\n",
    ".. figure:: /_static/img/invpendulum.gif\n",
    "   :alt: Inverted pendulum\n",
    "\n",
    "   Inverted pendulum\n",
    "\n",
    "Key learnings:\n",
    "\n",
    "- How to create an environment in TorchRL, transform its outputs, and collect data from this environment;\n",
    "- How to make your classes talk to each other using :class:`~tensordict.TensorDict`;\n",
    "- The basics of building your training loop with TorchRL:\n",
    "\n",
    "  - How to compute the advantage signal for policy gradient methods;\n",
    "  - How to create a stochastic policy using a probabilistic neural network;\n",
    "  - How to create a dynamic replay buffer and sample from it without repetition.\n",
    "\n",
    "We will cover six crucial components of TorchRL:\n",
    "\n",
    "* `environments <https://pytorch.org/rl/reference/envs.html>`__\n",
    "* `transforms <https://pytorch.org/rl/reference/envs.html#transforms>`__\n",
    "* `models (policy and value function) <https://pytorch.org/rl/reference/modules.html>`__\n",
    "* `loss modules <https://pytorch.org/rl/reference/objectives.html>`__\n",
    "* `data collectors <https://pytorch.org/rl/reference/collectors.html>`__\n",
    "* `replay buffers <https://pytorch.org/rl/reference/data.html#replay-buffers>`__\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "######################################################################\n",
    "# If you are running this in Google Colab, make sure you install the following dependencies:\n",
    "#\n",
    "# .. code-block:: bash\n",
    "#\n",
    "#    !pip3 install torchrl\n",
    "#    !pip3 install gym[mujoco]\n",
    "#    !pip3 install tqdm\n",
    "#\n",
    "# Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a\n",
    "# batch of data is being collected and directly consumed to train the policy to maximise\n",
    "# the expected return given some proximality constraints. You can think of it\n",
    "# as a sophisticated version of `REINFORCE <https://link.springer.com/content/pdf/10.1007/BF00992696.pdf>`_,\n",
    "# the foundational policy-optimization algorithm. For more information, see the\n",
    "# `Proximal Policy Optimization Algorithms <https://arxiv.org/abs/1707.06347>`_ paper.\n",
    "#\n",
    "# PPO is usually regarded as a fast and efficient method for online, on-policy\n",
    "# reinforcement algorithm. TorchRL provides a loss-module that does all the work\n",
    "# for you, so that you can rely on this implementation and focus on solving your\n",
    "# problem rather than re-inventing the wheel every time you want to train a policy.\n",
    "#\n",
    "# For completeness, here is a brief overview of what the loss computes, even though\n",
    "# this is taken care of by our :class:`~torchrl.objectives.ClipPPOLoss` module—the algorithm works as follows:\n",
    "# 1. we will sample a batch of data by playing the\n",
    "# policy in the environment for a given number of steps.\n",
    "# 2. Then, we will perform a given number of optimization steps with random sub-samples of this batch using\n",
    "# a clipped version of the REINFORCE loss.\n",
    "# 3. The clipping will put a pessimistic bound on our loss: lower return estimates will\n",
    "# be favored compared to higher ones.\n",
    "# The precise formula of the loss is:\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#     L(s,a,\\theta_k,\\theta) = \\min\\left(\n",
    "#     \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n",
    "#     g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n",
    "#     \\right),\n",
    "#\n",
    "# There are two components in that loss: in the first part of the minimum operator,\n",
    "# we simply compute an importance-weighted version of the REINFORCE loss (for example, a\n",
    "# REINFORCE loss that we have corrected for the fact that the current policy\n",
    "# configuration lags the one that was used for the data collection).\n",
    "# The second part of that minimum operator is a similar loss where we have clipped\n",
    "# the ratios when they exceeded or were below a given pair of thresholds.\n",
    "#\n",
    "# This loss ensures that whether the advantage is positive or negative, policy\n",
    "# updates that would produce significant shifts from the previous configuration\n",
    "# are being discouraged.\n",
    "#\n",
    "# This tutorial is structured as follows:\n",
    "#\n",
    "# 1. First, we will define a set of hyperparameters we will be using for training.\n",
    "#\n",
    "# 2. Next, we will focus on creating our environment, or simulator, using TorchRL's\n",
    "#    wrappers and transforms.\n",
    "#\n",
    "# 3. Next, we will design the policy network and the value model,\n",
    "#    which is indispensable to the loss function. These modules will be used\n",
    "#    to configure our loss module.\n",
    "#\n",
    "# 4. Next, we will create the replay buffer and data loader.\n",
    "#\n",
    "# 5. Finally, we will run our training loop and analyze the results.\n",
    "#\n",
    "# Throughout this tutorial, we'll be using the :mod:`tensordict` library.\n",
    "# :class:`~tensordict.TensorDict` is the lingua franca of TorchRL: it helps us abstract\n",
    "# what a module reads and writes and care less about the specific data\n",
    "# description and more about the algorithm itself.\n",
    "#\n",
    "\n",
    "# sphinx_gallery_start_ignore\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "# TorchRL prefers spawn method, that restricts creation of  ``~torchrl.envs.ParallelEnv`` inside\n",
    "# `__main__` method call, but for the easy of reading the code switch to fork\n",
    "# which is also a default spawn method in Google's Colaboratory\n",
    "try:\n",
    "    is_sphinx = __sphinx_build__\n",
    "except NameError:\n",
    "    is_sphinx = False\n",
    "\n",
    "try:\n",
    "    multiprocessing.set_start_method(\"spawn\" if is_sphinx else \"fork\")\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "# sphinx_gallery_end_ignore\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (\n",
    "    Compose,\n",
    "    DoubleToFloat,\n",
    "    ObservationNorm,\n",
    "    StepCounter,\n",
    "    TransformedEnv,\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm\n",
    "\n",
    "######################################################################\n",
    "# Define Hyperparameters\n",
    "# ----------------------\n",
    "#\n",
    "# We set the hyperparameters for our algorithm. Depending on the resources\n",
    "# available, one may choose to execute the policy on GPU or on another\n",
    "# device.\n",
    "# The ``frame_skip`` will control how for how many frames is a single\n",
    "# action being executed. The rest of the arguments that count frames\n",
    "# must be corrected for this value (since one environment step will\n",
    "# actually return ``frame_skip`` frames).\n",
    "#\n",
    "\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "######################################################################\n",
    "# Data collection parameters\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# When collecting data, we will be able to choose how big each batch will be\n",
    "# by defining a ``frames_per_batch`` parameter. We will also define how many\n",
    "# frames (such as the number of interactions with the simulator) we will allow ourselves to\n",
    "# use. In general, the goal of an RL algorithm is to learn to solve the task\n",
    "# as fast as it can in terms of environment interactions: the lower the ``total_frames``\n",
    "# the better.\n",
    "#\n",
    "frames_per_batch = 1000\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 10_000\n",
    "\n",
    "######################################################################\n",
    "# PPO parameters\n",
    "# ~~~~~~~~~~~~~~\n",
    "#\n",
    "# At each data collection (or batch collection) we will run the optimization\n",
    "# over a certain number of *epochs*, each time consuming the entire data we just\n",
    "# acquired in a nested training loop. Here, the ``sub_batch_size`` is different from the\n",
    "# ``frames_per_batch`` here above: recall that we are working with a \"batch of data\"\n",
    "# coming from our collector, which size is defined by ``frames_per_batch``, and that\n",
    "# we will further split in smaller sub-batches during the inner training loop.\n",
    "# The size of these sub-batches is controlled by ``sub_batch_size``.\n",
    "#\n",
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimisation steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "######################################################################\n",
    "# Define an environment\n",
    "# ---------------------\n",
    "#\n",
    "# In RL, an *environment* is usually the way we refer to a simulator or a\n",
    "# control system. Various libraries provide simulation environments for reinforcement\n",
    "# learning, including Gymnasium (previously OpenAI Gym), DeepMind control suite, and\n",
    "# many others.\n",
    "# As a general library, TorchRL's goal is to provide an interchangeable interface\n",
    "# to a large panel of RL simulators, allowing you to easily swap one environment\n",
    "# with another. For example, creating a wrapped gym environment can be achieved with few characters:\n",
    "#\n",
    "\n",
    "base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device)\n",
    "\n",
    "######################################################################\n",
    "# There are a few things to notice in this code: first, we created\n",
    "# the environment by calling the ``GymEnv`` wrapper. If extra keyword arguments\n",
    "# are passed, they will be transmitted to the ``gym.make`` method, hence covering\n",
    "# the most common environment construction commands.\n",
    "# Alternatively, one could also directly create a gym environment using ``gym.make(env_name, **kwargs)``\n",
    "# and wrap it in a `GymWrapper` class.\n",
    "#\n",
    "# Also the ``device`` argument: for gym, this only controls the device where\n",
    "# input action and observed states will be stored, but the execution will always\n",
    "# be done on CPU. The reason for this is simply that gym does not support on-device\n",
    "# execution, unless specified otherwise. For other libraries, we have control over\n",
    "# the execution device and, as much as we can, we try to stay consistent in terms of\n",
    "# storing and execution backends.\n",
    "#\n",
    "# Transforms\n",
    "# ~~~~~~~~~~\n",
    "#\n",
    "# We will append some transforms to our environments to prepare the data for\n",
    "# the policy. In Gym, this is usually achieved via wrappers. TorchRL takes a different\n",
    "# approach, more similar to other pytorch domain libraries, through the use of transforms.\n",
    "# To add transforms to an environment, one should simply wrap it in a :class:`~torchrl.envs.transforms.TransformedEnv`\n",
    "# instance and append the sequence of transforms to it. The transformed environment will inherit\n",
    "# the device and meta-data of the wrapped environment, and transform these depending on the sequence\n",
    "# of transforms it contains.\n",
    "#\n",
    "# Normalization\n",
    "# ~~~~~~~~~~~~~\n",
    "#\n",
    "# The first to encode is a normalization transform.\n",
    "# As a rule of thumbs, it is preferable to have data that loosely\n",
    "# match a unit Gaussian distribution: to obtain this, we will\n",
    "# run a certain number of random steps in the environment and compute\n",
    "# the summary statistics of these observations.\n",
    "#\n",
    "# We'll append two other transforms: the :class:`~torchrl.envs.transforms.DoubleToFloat` transform will\n",
    "# convert double entries to single-precision numbers, ready to be read by the\n",
    "# policy. The :class:`~torchrl.envs.transforms.StepCounter` transform will be used to count the steps before\n",
    "# the environment is terminated. We will use this measure as a supplementary measure\n",
    "# of performance.\n",
    "#\n",
    "# As we will see later, many of the TorchRL's classes rely on :class:`~tensordict.TensorDict`\n",
    "# to communicate. You could think of it as a python dictionary with some extra\n",
    "# tensor features. In practice, this means that many modules we will be working\n",
    "# with need to be told what key to read (``in_keys``) and what key to write\n",
    "# (``out_keys``) in the ``tensordict`` they will receive. Usually, if ``out_keys``\n",
    "# is omitted, it is assumed that the ``in_keys`` entries will be updated\n",
    "# in-place. For our transforms, the only entry we are interested in is referred\n",
    "# to as ``\"observation\"`` and our transform layers will be told to modify this\n",
    "# entry and this entry only:\n",
    "#\n",
    "\n",
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# As you may have noticed, we have created a normalization layer but we did not\n",
    "# set its normalization parameters. To do this, :class:`~torchrl.envs.transforms.ObservationNorm` can\n",
    "# automatically gather the summary statistics of our environment:\n",
    "#\n",
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)\n",
    "\n",
    "######################################################################\n",
    "# The :class:`~torchrl.envs.transforms.ObservationNorm` transform has now been populated with a\n",
    "# location and a scale that will be used to normalize the data.\n",
    "#\n",
    "# Let us do a little sanity check for the shape of our summary stats:\n",
    "#\n",
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)\n",
    "\n",
    "######################################################################\n",
    "# An environment is not only defined by its simulator and transforms, but also\n",
    "# by a series of metadata that describe what can be expected during its\n",
    "# execution.\n",
    "# For efficiency purposes, TorchRL is quite stringent when it comes to\n",
    "# environment specs, but you can easily check that your environment specs are\n",
    "# adequate.\n",
    "# In our example, the :class:`~torchrl.envs.libs.gym.GymWrapper` and\n",
    "# :class:`~torchrl.envs.libs.gym.GymEnv` that inherits\n",
    "# from it already take care of setting the proper specs for your environment so\n",
    "# you should not have to care about this.\n",
    "#\n",
    "# Nevertheless, let's see a concrete example using our transformed\n",
    "# environment by looking at its specs.\n",
    "# There are three specs to look at: ``observation_spec`` which defines what\n",
    "# is to be expected when executing an action in the environment,\n",
    "# ``reward_spec`` which indicates the reward domain and finally the\n",
    "# ``input_spec`` (which contains the ``action_spec``) and which represents\n",
    "# everything an environment requires to execute a single step.\n",
    "#\n",
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)\n",
    "\n",
    "######################################################################\n",
    "# the :func:`check_env_specs` function runs a small rollout and compares its output against the environment\n",
    "# specs. If no error is raised, we can be confident that the specs are properly defined:\n",
    "#\n",
    "check_env_specs(env)\n",
    "\n",
    "######################################################################\n",
    "# For fun, let's see what a simple random rollout looks like. You can\n",
    "# call `env.rollout(n_steps)` and get an overview of what the environment inputs\n",
    "# and outputs look like. Actions will automatically be drawn from the action spec\n",
    "# domain, so you don't need to care about designing a random sampler.\n",
    "#\n",
    "# Typically, at each step, an RL environment receives an\n",
    "# action as input, and outputs an observation, a reward and a done state. The\n",
    "# observation may be composite, meaning that it could be composed of more than one\n",
    "# tensor. This is not a problem for TorchRL, since the whole set of observations\n",
    "# is automatically packed in the output :class:`~tensordict.TensorDict`. After executing a rollout\n",
    "# (for example, a sequence of environment steps and random action generations) over a given\n",
    "# number of steps, we will retrieve a :class:`~tensordict.TensorDict` instance with a shape\n",
    "# that matches this trajectory length:\n",
    "#\n",
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)\n",
    "\n",
    "######################################################################\n",
    "# Our rollout data has a shape of ``torch.Size([3])``, which matches the number of steps\n",
    "# we ran it for. The ``\"next\"`` entry points to the data coming after the current step.\n",
    "# In most cases, the ``\"next\"`` data at time `t` matches the data at ``t+1``, but this\n",
    "# may not be the case if we are using some specific transformations (for example, multi-step).\n",
    "#\n",
    "# Policy\n",
    "# ------\n",
    "#\n",
    "# PPO utilizes a stochastic policy to handle exploration. This means that our\n",
    "# neural network will have to output the parameters of a distribution, rather\n",
    "# than a single value corresponding to the action taken.\n",
    "#\n",
    "# As the data is continuous, we use a Tanh-Normal distribution to respect the\n",
    "# action space boundaries. TorchRL provides such distribution, and the only\n",
    "# thing we need to care about is to build a neural network that outputs the\n",
    "# right number of parameters for the policy to work with (a location, or mean,\n",
    "# and a scale):\n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#     f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), \\sigma^{+}_{\\theta}(\\text{observation})\n",
    "#\n",
    "# The only extra-difficulty that is brought up here is to split our output in two\n",
    "# equal parts and map the second to a strictly positive space.\n",
    "#\n",
    "# We design the policy in three steps:\n",
    "#\n",
    "# 1. Define a neural network ``D_obs`` -> ``2 * D_action``. Indeed, our ``loc`` (mu) and ``scale`` (sigma) both have dimension ``D_action``.\n",
    "#\n",
    "# 2. Append a :class:`~tensordict.nn.distributions.NormalParamExtractor` to extract a location and a scale (for example, splits the input in two equal parts and applies a positive transformation to the scale parameter).\n",
    "#\n",
    "# 3. Create a probabilistic :class:`~tensordict.nn.TensorDictModule` that can generate this distribution and sample from it.\n",
    "#\n",
    "\n",
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# To enable the policy to \"talk\" with the environment through the ``tensordict``\n",
    "# data carrier, we wrap the ``nn.Module`` in a :class:`~tensordict.nn.TensorDictModule`. This\n",
    "# class will simply ready the ``in_keys`` it is provided with and write the\n",
    "# outputs in-place at the registered ``out_keys``.\n",
    "#\n",
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# We now need to build a distribution out of the location and scale of our\n",
    "# normal distribution. To do so, we instruct the\n",
    "# :class:`~torchrl.modules.tensordict_module.ProbabilisticActor`\n",
    "# class to build a :class:`~torchrl.modules.TanhNormal` out of the location and scale\n",
    "# parameters. We also provide the minimum and maximum values of this\n",
    "# distribution, which we gather from the environment specs.\n",
    "#\n",
    "# The name of the ``in_keys`` (and hence the name of the ``out_keys`` from\n",
    "# the :class:`~tensordict.nn.TensorDictModule` above) cannot be set to any value one may\n",
    "# like, as the :class:`~torchrl.modules.TanhNormal` distribution constructor will expect the\n",
    "# ``loc`` and ``scale`` keyword arguments. That being said,\n",
    "# :class:`~torchrl.modules.tensordict_module.ProbabilisticActor` also accepts\n",
    "# ``Dict[str, str]`` typed ``in_keys`` where the key-value pair indicates\n",
    "# what ``in_key`` string should be used for every keyword argument that is to be used.\n",
    "#\n",
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.action_spec.space.low,\n",
    "        \"max\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# Value network\n",
    "# -------------\n",
    "#\n",
    "# The value network is a crucial component of the PPO algorithm, even though it\n",
    "# won't be used at inference time. This module will read the observations and\n",
    "# return an estimation of the discounted return for the following trajectory.\n",
    "# This allows us to amortize learning by relying on the some utility estimation\n",
    "# that is learned on-the-fly during training. Our value network share the same\n",
    "# structure as the policy, but for simplicity we assign it its own set of\n",
    "# parameters.\n",
    "#\n",
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# let's try our policy and value modules. As we said earlier, the usage of\n",
    "# :class:`~tensordict.nn.TensorDictModule` makes it possible to directly read the output\n",
    "# of the environment to run these modules, as they know what information to read\n",
    "# and where to write it:\n",
    "#\n",
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))\n",
    "\n",
    "######################################################################\n",
    "# Data collector\n",
    "# --------------\n",
    "#\n",
    "# TorchRL provides a set of `DataCollector classes <https://pytorch.org/rl/reference/collectors.html>`__.\n",
    "# Briefly, these classes execute three operations: reset an environment,\n",
    "# compute an action given the latest observation, execute a step in the environment,\n",
    "# and repeat the last two steps until the environment signals a stop (or reaches\n",
    "# a done state).\n",
    "#\n",
    "# They allow you to control how many frames to collect at each iteration\n",
    "# (through the ``frames_per_batch`` parameter),\n",
    "# when to reset the environment (through the ``max_frames_per_traj`` argument),\n",
    "# on which ``device`` the policy should be executed, etc. They are also\n",
    "# designed to work efficiently with batched and multiprocessed environments.\n",
    "#\n",
    "# The simplest data collector is the :class:`~torchrl.collectors.collectors.SyncDataCollector`:\n",
    "# it is an iterator that you can use to get batches of data of a given length, and\n",
    "# that will stop once a total number of frames (``total_frames``) have been\n",
    "# collected.\n",
    "# Other data collectors (:class:`~torchrl.collectors.collectors.MultiSyncDataCollector` and\n",
    "# :class:`~torchrl.collectors.collectors.MultiaSyncDataCollector`) will execute\n",
    "# the same operations in synchronous and asynchronous manner over a\n",
    "# set of multiprocessed workers.\n",
    "#\n",
    "# As for the policy and environment before, the data collector will return\n",
    "# :class:`~tensordict.TensorDict` instances with a total number of elements that will\n",
    "# match ``frames_per_batch``. Using :class:`~tensordict.TensorDict` to pass data to the\n",
    "# training loop allows you to write data loading pipelines\n",
    "# that are 100% oblivious to the actual specificities of the rollout content.\n",
    "#\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# Replay buffer\n",
    "# -------------\n",
    "#\n",
    "# Replay buffers are a common building piece of off-policy RL algorithms.\n",
    "# In on-policy contexts, a replay buffer is refilled every time a batch of\n",
    "# data is collected, and its data is repeatedly consumed for a certain number\n",
    "# of epochs.\n",
    "#\n",
    "# TorchRL's replay buffers are built using a common container\n",
    "# :class:`~torchrl.data.ReplayBuffer` which takes as argument the components\n",
    "# of the buffer: a storage, a writer, a sampler and possibly some transforms.\n",
    "# Only the storage (which indicates the replay buffer capacity) is mandatory.\n",
    "# We also specify a sampler without repetition to avoid sampling multiple times\n",
    "# the same item in one epoch.\n",
    "# Using a replay buffer for PPO is not mandatory and we could simply\n",
    "# sample the sub-batches from the collected batch, but using these classes\n",
    "# make it easy for us to build the inner training loop in a reproducible way.\n",
    "#\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# Loss function\n",
    "# -------------\n",
    "#\n",
    "# The PPO loss can be directly imported from TorchRL for convenience using the\n",
    "# :class:`~torchrl.objectives.ClipPPOLoss` class. This is the easiest way of utilizing PPO:\n",
    "# it hides away the mathematical operations of PPO and the control flow that\n",
    "# goes with it.\n",
    "#\n",
    "# PPO requires some \"advantage estimation\" to be computed. In short, an advantage\n",
    "# is a value that reflects an expectancy over the return value while dealing with\n",
    "# the bias / variance tradeoff.\n",
    "# To compute the advantage, one just needs to (1) build the advantage module, which\n",
    "# utilizes our value operator, and (2) pass each batch of data through it before each\n",
    "# epoch.\n",
    "# The GAE module will update the input ``tensordict`` with new ``\"advantage\"`` and\n",
    "# ``\"value_target\"`` entries.\n",
    "# The ``\"value_target\"`` is a gradient-free tensor that represents the empirical\n",
    "# value that the value network should represent with the input observation.\n",
    "# Both of these will be used by :class:`~torchrl.objectives.ClipPPOLoss` to\n",
    "# return the policy and value losses.\n",
    "#\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")\n",
    "\n",
    "######################################################################\n",
    "# Training loop\n",
    "# -------------\n",
    "# We now have all the pieces needed to code our training loop.\n",
    "# The steps include:\n",
    "#\n",
    "# * Collect data\n",
    "#\n",
    "#   * Compute advantage\n",
    "#\n",
    "#     * Loop over the collected to compute loss values\n",
    "#     * Back propagate\n",
    "#     * Optimize\n",
    "#     * Repeat\n",
    "#\n",
    "#   * Repeat\n",
    "#\n",
    "# * Repeat\n",
    "#\n",
    "\n",
    "\n",
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.MEAN), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()\n",
    "\n",
    "######################################################################\n",
    "# Results\n",
    "# -------\n",
    "#\n",
    "# Before the 1M step cap is reached, the algorithm should have reached a max\n",
    "# step count of 1000 steps, which is the maximum number of steps before the\n",
    "# trajectory is truncated.\n",
    "#\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()\n",
    "\n",
    "######################################################################\n",
    "# Conclusion and next steps\n",
    "# -------------------------\n",
    "#\n",
    "# In this tutorial, we have learned:\n",
    "#\n",
    "# 1. How to create and customize an environment with :py:mod:`torchrl`;\n",
    "# 2. How to write a model and a loss function;\n",
    "# 3. How to set up a typical training loop.\n",
    "#\n",
    "# If you want to experiment with this tutorial a bit more, you can apply the following modifications:\n",
    "#\n",
    "# * From an efficiency perspective,\n",
    "#   we could run several simulations in parallel to speed up data collection.\n",
    "#   Check :class:`~torchrl.envs.ParallelEnv` for further information.\n",
    "#\n",
    "# * From a logging perspective, one could add a :class:`torchrl.record.VideoRecorder` transform to\n",
    "#   the environment after asking for rendering to get a visual rendering of the\n",
    "#   inverted pendulum in action. Check :py:mod:`torchrl.record` to\n",
    "#   know more.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show simulation results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
