{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JowRQEGGKQ"
   },
   "source": [
    "################################################################################\n",
    "> # **Clone GitHub repository**\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyGzuEMQF6sJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "################# Clone repository from github to colab session ################\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "run this section if you want to clone all the preTrained networks, logs, graph figures, gifs \n",
    "from the GitHub repository to this colab session\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "!git clone https://github.com/nikhilbarhate99/PPO-PyTorch\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mrn6rpJpF8Sc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "run this section if you want to copy all files and folders from cloned folder (PPO-PyTorch)\n",
    "to current directory (/content/ or ./)\n",
    "\n",
    "So you can load preTrained networks and log files without changing any paths\n",
    "\n",
    "**  This will overwrite any saved networks, logs, graph figures, or gifs \n",
    "    that are created in this session before copying having the same name (or number)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "!cp -rv ./PPO-PyTorch/* ./\n",
    "\n",
    "print(\"============================================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-7AbGA2F8Ut",
    "outputId": "c6921fe3-fa71-42df-e3fa-c9af7eee6aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "run this section if you want to delete original cloned folder and the cloned ipynb file\n",
    "(after you have copied its contents to current directory)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "# delete original cloned folder\n",
    "!rm -r ./PPO-PyTorch\n",
    "\n",
    "# delete cloned ipynb file\n",
    "!rm ./PPO_colab.ipynb\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4VJcUT2GlJz"
   },
   "source": [
    "################################################################################\n",
    "> # **Install Dependencies**\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbpSQTflGlAr"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############ install compatible version of OpenAI roboschool and gym ###########\n",
    "\n",
    "!pip install swig\n",
    "\n",
    "# !pip install roboschool==1.0.7 gym==0.15.4\n",
    "\n",
    "# !pip install box2d-py\n",
    "\n",
    "# !pip install Box2D\n",
    "\n",
    "# !pip install pybullet\n",
    "\n",
    "# !pip install gym[box2d]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzZairIiGQ11"
   },
   "source": [
    "################################################################################\n",
    "> # **Introduction**\n",
    "> The notebook is divided into 5 major parts : \n",
    "\n",
    "*   **Part I** : define actor-critic network and PPO algorithm\n",
    "*   **Part II** : train PPO algorithm and save network weights and log files\n",
    "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
    "*   **Part IV** : load log files and plot graphs\n",
    "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s37cJXAYGrTY"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - I**\n",
    "\n",
    "*   define actor critic networks\n",
    "*   define PPO algorithm\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UT6VUBg-F8Zm",
    "outputId": "2369a1ae-0ba8-41ab-aaa0-f216627b86f6",
    "ExecuteTime": {
     "end_time": "2024-03-02T07:54:00.616663Z",
     "start_time": "2024-03-02T07:53:56.665837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Device set to : cpu\n",
      "============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/envs/registration.py:440: UserWarning: \u001B[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################### Import libraries ###############################\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "# import roboschool\n",
    "import pybullet_envs\n",
    "\n",
    "\n",
    "################################## set device ##################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "    \n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################## PPO Policy ##################################\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "\n",
    "        \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, k_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = k_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        \n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if self.action_std <= min_action_std:\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "        \n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xCb_EyxF8cF"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################# End of Part I ################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr-ZjT_CGyEi"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - II**\n",
    "\n",
    "*   train PPO algorithm on environments\n",
    "*   save preTrained networks weights and log files\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YY1-DzVCF8eh",
    "ExecuteTime": {
     "end_time": "2024-03-02T07:56:26.147417Z",
     "start_time": "2024-03-02T07:54:04.145954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "training environment name : BipedalWalker-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/core.py:329: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current logging run number for BipedalWalker-v3 :  5\n",
      "logging at : PPO_logs/BipedalWalker-v3//PPO_BipedalWalker-v3_log_5.csv\n",
      "save checkpoint path : PPO_preTrained/BipedalWalker-v3/PPO_BipedalWalker-v3_0_0.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  100000\n",
      "max timesteps per episode :  1500\n",
      "model saving frequency : 20000 timesteps\n",
      "log frequency : 800 timesteps\n",
      "printing average reward over episodes in last : 1600 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  24\n",
      "action space dimension :  4\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  0.1\n",
      "decay rate of std of action distribution :  0.05\n",
      "minimum std of action distribution :  0.1\n",
      "decay frequency of std of action distribution : 25000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 6000 timesteps\n",
      "PPO K epochs :  40\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2024-03-02 11:24:06\n",
      "============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "Episode : 5 \t\t Timestep : 1600 \t\t Average Reward : -110.68\n",
      "4\n",
      "2\n",
      "Episode : 11 \t\t Timestep : 3200 \t\t Average Reward : -84.8\n",
      "1\n",
      "4\n",
      "Episode : 15 \t\t Timestep : 4800 \t\t Average Reward : -86.64\n",
      "1\n",
      "5\n",
      "Episode : 20 \t\t Timestep : 6400 \t\t Average Reward : -84.97\n",
      "2\n",
      "1\n",
      "Episode : 22 \t\t Timestep : 8000 \t\t Average Reward : -94.16\n",
      "8\n",
      "1\n",
      "Episode : 31 \t\t Timestep : 9600 \t\t Average Reward : -96.26\n",
      "1\n",
      "1\n",
      "Episode : 32 \t\t Timestep : 11200 \t\t Average Reward : -35.88\n",
      "2\n",
      "6\n",
      "Episode : 40 \t\t Timestep : 12800 \t\t Average Reward : -88.79\n",
      "3\n",
      "1\n",
      "Episode : 43 \t\t Timestep : 14400 \t\t Average Reward : -118.42\n",
      "4\n",
      "1\n",
      "Episode : 47 \t\t Timestep : 16000 \t\t Average Reward : -85.71\n",
      "2\n",
      "1\n",
      "Episode : 49 \t\t Timestep : 17600 \t\t Average Reward : -76.04\n",
      "1\n",
      "1\n",
      "Episode : 50 \t\t Timestep : 19200 \t\t Average Reward : -36.37\n",
      "3\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/BipedalWalker-v3/PPO_BipedalWalker-v3_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:29\n",
      "--------------------------------------------------------------------------------------------\n",
      "1\n",
      "Episode : 53 \t\t Timestep : 20800 \t\t Average Reward : -82.06\n",
      "3\n",
      "1\n",
      "Episode : 56 \t\t Timestep : 22400 \t\t Average Reward : -74.93\n",
      "3\n",
      "1\n",
      "Episode : 60 \t\t Timestep : 24000 \t\t Average Reward : -93.84\n",
      "1\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "3\n",
      "Episode : 63 \t\t Timestep : 25600 \t\t Average Reward : -84.39\n",
      "1\n",
      "4\n",
      "Episode : 67 \t\t Timestep : 27200 \t\t Average Reward : -86.34\n",
      "1\n",
      "1\n",
      "Episode : 68 \t\t Timestep : 28800 \t\t Average Reward : -32.46\n",
      "1\n",
      "2\n",
      "Episode : 70 \t\t Timestep : 30400 \t\t Average Reward : -77.09\n",
      "1\n",
      "2\n",
      "Episode : 72 \t\t Timestep : 32000 \t\t Average Reward : -65.5\n",
      "1\n",
      "1\n",
      "Episode : 73 \t\t Timestep : 33600 \t\t Average Reward : -34.08\n",
      "1\n",
      "1\n",
      "Episode : 74 \t\t Timestep : 35200 \t\t Average Reward : -23.04\n",
      "1\n",
      "1\n",
      "Episode : 75 \t\t Timestep : 36800 \t\t Average Reward : -35.59\n",
      "1\n",
      "1\n",
      "Episode : 77 \t\t Timestep : 38400 \t\t Average Reward : -77.81\n",
      "1\n",
      "2\n",
      "Episode : 79 \t\t Timestep : 40000 \t\t Average Reward : -77.76\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/BipedalWalker-v3/PPO_BipedalWalker-v3_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:57\n",
      "--------------------------------------------------------------------------------------------\n",
      "1\n",
      "1\n",
      "Episode : 81 \t\t Timestep : 41600 \t\t Average Reward : -82.73\n",
      "1\n",
      "2\n",
      "Episode : 83 \t\t Timestep : 43200 \t\t Average Reward : -80.24\n",
      "1\n",
      "1\n",
      "Episode : 84 \t\t Timestep : 44800 \t\t Average Reward : -36.91\n",
      "1\n",
      "2\n",
      "Episode : 87 \t\t Timestep : 46400 \t\t Average Reward : -94.28\n",
      "1\n",
      "1\n",
      "Episode : 88 \t\t Timestep : 48000 \t\t Average Reward : -38.26\n",
      "1\n",
      "2\n",
      "Episode : 90 \t\t Timestep : 49600 \t\t Average Reward : -80.49\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "1\n",
      "4\n",
      "Episode : 94 \t\t Timestep : 51200 \t\t Average Reward : -102.71\n",
      "1\n",
      "4\n",
      "Episode : 98 \t\t Timestep : 52800 \t\t Average Reward : -87.29\n",
      "2\n",
      "1\n",
      "Episode : 100 \t\t Timestep : 54400 \t\t Average Reward : -120.62\n",
      "2\n",
      "1\n",
      "Episode : 102 \t\t Timestep : 56000 \t\t Average Reward : -68.99\n",
      "1\n",
      "1\n",
      "Episode : 103 \t\t Timestep : 57600 \t\t Average Reward : -36.49\n",
      "5\n",
      "1\n",
      "Episode : 109 \t\t Timestep : 59200 \t\t Average Reward : -99.77\n",
      "1\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/BipedalWalker-v3/PPO_BipedalWalker-v3_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:24\n",
      "--------------------------------------------------------------------------------------------\n",
      "3\n",
      "Episode : 112 \t\t Timestep : 60800 \t\t Average Reward : -86.81\n",
      "1\n",
      "1\n",
      "Episode : 113 \t\t Timestep : 62400 \t\t Average Reward : -34.93\n",
      "1\n",
      "2\n",
      "Episode : 115 \t\t Timestep : 64000 \t\t Average Reward : -77.51\n",
      "1\n",
      "1\n",
      "Episode : 116 \t\t Timestep : 65600 \t\t Average Reward : -37.5\n",
      "1\n",
      "1\n",
      "Episode : 117 \t\t Timestep : 67200 \t\t Average Reward : -34.31\n",
      "1\n",
      "2\n",
      "Episode : 119 \t\t Timestep : 68800 \t\t Average Reward : -67.72\n",
      "1\n",
      "1\n",
      "Episode : 120 \t\t Timestep : 70400 \t\t Average Reward : -36.83\n",
      "1\n",
      "1\n",
      "Episode : 121 \t\t Timestep : 72000 \t\t Average Reward : -37.37\n",
      "1\n",
      "1\n",
      "Episode : 122 \t\t Timestep : 73600 \t\t Average Reward : -36.75\n",
      "1\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "1\n",
      "Episode : 124 \t\t Timestep : 75200 \t\t Average Reward : -64.11\n",
      "1\n",
      "1\n",
      "Episode : 126 \t\t Timestep : 76800 \t\t Average Reward : -81.4\n",
      "1\n",
      "1\n",
      "Episode : 127 \t\t Timestep : 78400 \t\t Average Reward : -14.99\n",
      "1\n",
      "1\n",
      "Episode : 128 \t\t Timestep : 80000 \t\t Average Reward : -145.4\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/BipedalWalker-v3/PPO_BipedalWalker-v3_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:51\n",
      "--------------------------------------------------------------------------------------------\n",
      "2\n",
      "1\n",
      "Episode : 130 \t\t Timestep : 81600 \t\t Average Reward : -83.14\n",
      "2\n",
      "1\n",
      "Episode : 132 \t\t Timestep : 83200 \t\t Average Reward : -80.22\n",
      "1\n",
      "1\n",
      "Episode : 133 \t\t Timestep : 84800 \t\t Average Reward : -35.36\n",
      "1\n",
      "1\n",
      "Episode : 134 \t\t Timestep : 86400 \t\t Average Reward : -37.07\n",
      "1\n",
      "1\n",
      "Episode : 135 \t\t Timestep : 88000 \t\t Average Reward : -35.5\n",
      "2\n",
      "1\n",
      "Episode : 137 \t\t Timestep : 89600 \t\t Average Reward : -79.43\n",
      "2\n",
      "1\n",
      "Episode : 139 \t\t Timestep : 91200 \t\t Average Reward : -67.4\n",
      "2\n",
      "1\n",
      "Episode : 141 \t\t Timestep : 92800 \t\t Average Reward : -78.18\n",
      "1\n",
      "1\n",
      "Episode : 142 \t\t Timestep : 94400 \t\t Average Reward : -35.37\n",
      "1\n",
      "1\n",
      "Episode : 144 \t\t Timestep : 96000 \t\t Average Reward : -36.56\n",
      "1\n",
      "1\n",
      "Episode : 145 \t\t Timestep : 97600 \t\t Average Reward : -38.27\n",
      "1\n",
      "1\n",
      "Episode : 146 \t\t Timestep : 99200 \t\t Average Reward : -37.29\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "--------------------------------------------------------------------------------------------\n",
      "1\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/BipedalWalker-v3/PPO_BipedalWalker-v3_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:19\n",
      "--------------------------------------------------------------------------------------------\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2024-03-02 11:24:06\n",
      "Finished training at (GMT) :  2024-03-02 11:26:26\n",
      "Total training time  :  0:02:20\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "has_continuous_action_space = False\n",
    "\n",
    "max_ep_len = 400                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = None\n",
    "\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "has_continuous_action_space = True\n",
    "max_ep_len = 1500           # max timesteps in one episode\n",
    "action_std = 0.1            # set same std for action distribution which was used while saving\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "###################### logging ######################\n",
    "\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    action_std_decay_rate = 0.05\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    min_action_std = 0.1\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    action_std_decay_freq = int(2.5e4)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            if log_running_episodes == 0:\n",
    "                log_running_episodes = 1\n",
    "            print(log_running_episodes)\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEy2qKdZF8ha"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################ End of Part II ################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHhK13_1G6zX"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - III**\n",
    "\n",
    "*   load and test preTrained networks on environments\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZWyhkq9Gxm5",
    "outputId": "eb21c926-f866-4fb7-cbd0-4c14ba7b45b6",
    "ExecuteTime": {
     "end_time": "2024-03-02T08:00:51.260326Z",
     "start_time": "2024-03-02T08:00:33.811490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "loading network from : PPO_preTrained/BipedalWalker-v3/PPO_BipedalWalker-v3_0_0.pth\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/core.py:329: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/core.py:57: DeprecationWarning: \u001B[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001B[0m\n",
      "  deprecation(\n",
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 \t\t Reward: -39.82\n",
      "Episode: 2 \t\t Reward: -121.95\n",
      "Episode: 3 \t\t Reward: -124.9\n",
      "Episode: 4 \t\t Reward: -35.88\n",
      "Episode: 5 \t\t Reward: -35.82\n",
      "Episode: 6 \t\t Reward: -24.54\n",
      "Episode: 7 \t\t Reward: -35.85\n",
      "Episode: 8 \t\t Reward: -36.35\n",
      "Episode: 9 \t\t Reward: -33.97\n",
      "Episode: 10 \t\t Reward: -93.56\n",
      "============================================================================================\n",
      "average test reward : -58.27\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "#################################### Testing ###################################\n",
    "\n",
    "\n",
    "################## hyperparameters ##################\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "has_continuous_action_space = False\n",
    "max_ep_len = 400\n",
    "action_std = None\n",
    "render = False\n",
    "\n",
    "\n",
    "# env_name = \"LunarLander-v2\"\n",
    "# has_continuous_action_space = False\n",
    "# max_ep_len = 300\n",
    "# action_std = None\n",
    "\n",
    "\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "has_continuous_action_space = True\n",
    "max_ep_len = 1500           # max timesteps in one episode\n",
    "action_std = 0.1            # set same std for action distribution which was used while saving\n",
    "render = True\n",
    "\n",
    "# env_name = \"RoboschoolWalker2d-v1\"\n",
    "# has_continuous_action_space = True\n",
    "# max_ep_len = 1000           # max timesteps in one episode\n",
    "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
    "\n",
    "\n",
    "total_test_episodes = 10    # total num of testing episodes\n",
    "\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003           # learning rate for actor\n",
    "lr_critic = 0.001           # learning rate for critic\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# preTrained weights directory\n",
    "\n",
    "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
    "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
    "\n",
    "\n",
    "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"loading network from : \" + checkpoint_path)\n",
    "\n",
    "ppo_agent.load(checkpoint_path)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "test_running_reward = 0\n",
    "\n",
    "for ep in range(1, total_test_episodes+1):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    for t in range(1, max_ep_len+1):\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # clear buffer    \n",
    "    ppo_agent.buffer.clear()\n",
    "\n",
    "    test_running_reward +=  ep_reward\n",
    "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
    "    ep_reward = 0\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "avg_test_reward = test_running_reward / total_test_episodes\n",
    "avg_test_reward = round(avg_test_reward, 2)\n",
    "print(\"average test reward : \" + str(avg_test_reward))\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6IYC_JCGxlB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################ End of Part III ###############################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZewQELovHFt4"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - IV**\n",
    "\n",
    "*   load log files using pandas\n",
    "*   plot graph using matplotlib\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "bY-E5HGcGxiu",
    "outputId": "2dd1e86f-e14a-440e-e61e-ab2ca8b0498c",
    "ExecuteTime": {
     "end_time": "2024-03-02T07:59:47.999573Z",
     "start_time": "2024-03-02T07:59:41.947598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_0.csv\n",
      "data shape :  (125, 3)\n",
      "--------------------------------------------------------------------------------------------\n",
      "============================================================================================\n",
      "figure saved at :  PPO_figs/CartPole-v1//PPO_CartPole-v1_fig_0.png\n",
      "============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "# env_name = 'LunarLander-v2'\n",
    "# env_name = 'BipedalWalker-v2'\n",
    "# env_name = 'RoboschoolWalker2d-v1'\n",
    "\n",
    "\n",
    "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
    "\n",
    "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
    "\n",
    "fig_width = 10\n",
    "fig_height = 6\n",
    "\n",
    "\n",
    "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
    "window_len_smooth = 50\n",
    "min_window_len_smooth = 1\n",
    "linewidth_smooth = 1.5\n",
    "alpha_smooth = 1\n",
    "\n",
    "window_len_var = 5\n",
    "min_window_len_var = 1\n",
    "linewidth_var = 2\n",
    "alpha_var = 0.1\n",
    "\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
    "\n",
    "\n",
    "# make directory for saving figures\n",
    "figures_dir = \"PPO_figs\"\n",
    "if not os.path.exists(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "\n",
    "# make environment directory for saving figures\n",
    "figures_dir = figures_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "\n",
    "\n",
    "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
    "\n",
    "\n",
    "# get number of log files in directory\n",
    "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
    "\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "num_runs = len(current_num_files)\n",
    "\n",
    "\n",
    "all_runs = []\n",
    "\n",
    "for run_num in range(num_runs):\n",
    "\n",
    "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "    print(\"loading data from : \" + log_f_name)\n",
    "    data = pd.read_csv(log_f_name)\n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"data shape : \", data.shape)\n",
    "    \n",
    "    all_runs.append(data)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "if plot_avg:\n",
    "    # average all runs\n",
    "    df_concat = pd.concat(all_runs)\n",
    "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
    "    data_avg = df_concat_groupby.mean()\n",
    "\n",
    "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
    "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
    "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
    "\n",
    "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
    "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
    "\n",
    "    # keep only reward_smooth in the legend and rename it\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
    "\n",
    "\n",
    "else:\n",
    "    for i, run in enumerate(all_runs):\n",
    "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
    "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
    "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
    "        \n",
    "        # plot the lines\n",
    "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
    "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
    "\n",
    "    # keep alternate elements (reward_smooth_i) in the legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    new_handles = []\n",
    "    new_labels = []\n",
    "    for i in range(len(handles)):\n",
    "        if(i%2 == 0):\n",
    "            new_handles.append(handles[i])\n",
    "            new_labels.append(labels[i])\n",
    "    ax.legend(new_handles, new_labels, loc=2)\n",
    "\n",
    "\n",
    "\n",
    "# ax.set_yticks(np.arange(0, 1800, 200))\n",
    "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
    "\n",
    "\n",
    "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
    "\n",
    "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
    "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
    "\n",
    "plt.title(env_name, fontsize=14)\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(fig_width, fig_height)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "plt.savefig(fig_save_path)\n",
    "print(\"figure saved at : \", fig_save_path)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaWPRW9EGxgH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################ End of Part IV ################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8uG43MtHNGC"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - V**\n",
    "\n",
    "*   install virtual display libraries for rendering on colab / remote server ^\n",
    "*   load preTrained networks and save images for gif\n",
    "*   generate and save gif from previously saved images\n",
    "\n",
    "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally. \n",
    "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VL3tpKf3HLAq"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### to render on colab / server / headless machine install virtual display libraries\n",
    "\n",
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "\n",
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j5Rx_IFKHK-D",
    "ExecuteTime": {
     "end_time": "2024-03-02T08:23:07.373329Z",
     "start_time": "2024-03-02T08:23:05.911200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python dylib /Library/Frameworks/Python.framework/Versions/3.5/Python referred from cpp_household.so is not found, attempting a relink...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package python3 was not found in the pkg-config search path.\n",
      "Perhaps you should add the directory containing `python3.pc'\n",
      "to the PKG_CONFIG_PATH environment variable\n",
      "No package 'python3' found\n"
     ]
    },
    {
     "ename": "BaseException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBaseException\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mglob\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mroboschool\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/roboschool/__init__.py:64\u001B[0m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stdout\u001B[38;5;241m.\u001B[39mdecode()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     63\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQT_PLUGIN_PATH\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m osp\u001B[38;5;241m.\u001B[39mjoin(osp\u001B[38;5;241m.\u001B[39mdirname(osp\u001B[38;5;241m.\u001B[39mabspath(\u001B[38;5;18m__file__\u001B[39m)), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.qt_plugins\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 64\u001B[0m \u001B[43m_link_pythonlib\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m register(\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRoboschoolInvertedPendulum-v1\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     68\u001B[0m     entry_point\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mroboschool:RoboschoolInvertedPendulum\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     71\u001B[0m     tags\u001B[38;5;241m=\u001B[39m{ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpg_complexity\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m1000000\u001B[39m },\n\u001B[1;32m     72\u001B[0m     )\n\u001B[1;32m     73\u001B[0m register(\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRoboschoolInvertedPendulumSwingup-v1\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     75\u001B[0m     entry_point\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mroboschool:RoboschoolInvertedPendulumSwingup\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     78\u001B[0m     tags\u001B[38;5;241m=\u001B[39m{ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpg_complexity\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m1000000\u001B[39m },\n\u001B[1;32m     79\u001B[0m     )\n",
      "File \u001B[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/roboschool/__init__.py:21\u001B[0m, in \u001B[0;36m_link_pythonlib\u001B[0;34m(lib)\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPython dylib \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m referred from \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m is not found, attempting a relink...\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(old_python_path, lib))\n\u001B[0;32m---> 21\u001B[0m python_pkg_config_output \u001B[38;5;241m=\u001B[39m \u001B[43m_check_call_output\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpkg-config --libs python3\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     22\u001B[0m python_lib_paths \u001B[38;5;241m=\u001B[39m [s[\u001B[38;5;241m2\u001B[39m:] \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m python_pkg_config_output\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m s\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-L\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[1;32m     23\u001B[0m python_lib_names \u001B[38;5;241m=\u001B[39m [s[\u001B[38;5;241m2\u001B[39m:] \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m python_pkg_config_output\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m s\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-lpython\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n",
      "File \u001B[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/roboschool/__init__.py:59\u001B[0m, in \u001B[0;36m_check_call_output\u001B[0;34m(cmd, errormsg, **kwargs)\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28mprint\u001B[39m(stderr \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mprint\u001B[39m(errormsg \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stdout\u001B[38;5;241m.\u001B[39mdecode()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mBaseException\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################# save images for gif ##############################\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import gym\n",
    "import roboschool\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "One frame corresponding to each timestep is saved in a folder :\n",
    "\n",
    "PPO_gif_images/env_name/000001.jpg\n",
    "PPO_gif_images/env_name/000002.jpg\n",
    "PPO_gif_images/env_name/000003.jpg\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "if this section is run multiple times or for multiple episodes for the same env_name; \n",
    "then the saved images will be overwritten.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### beginning of virtual display code section\n",
    "\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "#### end of virtual display code section\n",
    "\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################## hyperparameters ##################\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "has_continuous_action_space = False\n",
    "max_ep_len = 400\n",
    "action_std = None\n",
    "\n",
    "\n",
    "# env_name = \"LunarLander-v2\"\n",
    "# has_continuous_action_space = False\n",
    "# max_ep_len = 300\n",
    "# action_std = None\n",
    "\n",
    "# env_name = \"BipedalWalker-v2\"\n",
    "# has_continuous_action_space = True\n",
    "# max_ep_len = 1500           # max timesteps in one episode\n",
    "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
    "\n",
    "# env_name = \"RoboschoolWalker2d-v1\"\n",
    "# has_continuous_action_space = True\n",
    "# max_ep_len = 1000           # max timesteps in one episode\n",
    "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
    "\n",
    "\n",
    "total_test_episodes = 1     # save gif for only one episode\n",
    "\n",
    "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
    "\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003         # learning rate for actor\n",
    "lr_critic = 0.001         # learning rate for critic\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "# make directory for saving gif images\n",
    "gif_images_dir = \"PPO_gif_images\" + '/'\n",
    "if not os.path.exists(gif_images_dir):\n",
    "    os.makedirs(gif_images_dir)\n",
    "\n",
    "# make environment directory for saving gif images\n",
    "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(gif_images_dir):\n",
    "    os.makedirs(gif_images_dir)\n",
    "\n",
    "# make directory for gif\n",
    "gif_dir = \"PPO_gifs\" + '/'\n",
    "if not os.path.exists(gif_dir):\n",
    "    os.makedirs(gif_dir)\n",
    "\n",
    "# make environment directory for gif\n",
    "gif_dir = gif_dir + '/' + env_name  + '/'\n",
    "if not os.path.exists(gif_dir):\n",
    "    os.makedirs(gif_dir)\n",
    "\n",
    "\n",
    "\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# preTrained weights directory\n",
    "\n",
    "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
    "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
    "\n",
    "\n",
    "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"loading network from : \" + checkpoint_path)\n",
    "\n",
    "ppo_agent.load(checkpoint_path)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "test_running_reward = 0\n",
    "\n",
    "for ep in range(1, total_test_episodes+1):\n",
    "    \n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "\n",
    "        img = env.render(mode = 'rgb_array')\n",
    "\n",
    "\n",
    "        #### beginning of ipythondisplay code section 1\n",
    "\n",
    "        if render_ipython:\n",
    "            plt.imshow(img)\n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())\n",
    "\n",
    "        #### end of ipythondisplay code section 1\n",
    "\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # clear buffer    \n",
    "    ppo_agent.buffer.clear()\n",
    "    \n",
    "    test_running_reward +=  ep_reward\n",
    "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
    "    ep_reward = 0\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "#### beginning of ipythondisplay code section 2\n",
    "\n",
    "if render_ipython:\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "\n",
    "#### end of ipythondisplay code section 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "print(\"total number of frames / timesteps / images saved : \", t)\n",
    "\n",
    "avg_test_reward = test_running_reward / total_test_episodes\n",
    "avg_test_reward = round(avg_test_reward, 2)\n",
    "print(\"average test reward : \" + str(avg_test_reward))\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BoVshl_ZHK7s",
    "ExecuteTime": {
     "end_time": "2024-03-02T08:23:47.125216Z",
     "start_time": "2024-03-02T08:23:47.116565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "total frames in gif :  0\n",
      "total duration of gif : 0.0 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected at least 1, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 48\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal duration of gif : \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mround\u001B[39m(\u001B[38;5;28mlen\u001B[39m(img_paths) \u001B[38;5;241m*\u001B[39m frame_duration \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1000\u001B[39m, \u001B[38;5;241m2\u001B[39m)) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m seconds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# save gif\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m img, \u001B[38;5;241m*\u001B[39mimgs \u001B[38;5;241m=\u001B[39m [Image\u001B[38;5;241m.\u001B[39mopen(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m img_paths]\n\u001B[1;32m     49\u001B[0m img\u001B[38;5;241m.\u001B[39msave(fp\u001B[38;5;241m=\u001B[39mgif_path, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGIF\u001B[39m\u001B[38;5;124m'\u001B[39m, append_images\u001B[38;5;241m=\u001B[39mimgs, save_all\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, optimize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, duration\u001B[38;5;241m=\u001B[39mframe_duration, loop\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaved gif at : \u001B[39m\u001B[38;5;124m\"\u001B[39m, gif_path)\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected at least 1, got 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "######################## generate gif from saved images ########################\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "# env_name = 'LunarLander-v2'\n",
    "# env_name = 'BipedalWalker-v2'\n",
    "# env_name = 'RoboschoolWalker2d-v1'\n",
    "\n",
    "\n",
    "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
    "\n",
    "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
    "total_timesteps = 300\n",
    "step = 10\n",
    "frame_duration = 150\n",
    "\n",
    "\n",
    "# input images\n",
    "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
    "\n",
    "\n",
    "# ouput gif path\n",
    "gif_dir = \"PPO_gifs\"\n",
    "if not os.path.exists(gif_dir):\n",
    "    os.makedirs(gif_dir)\n",
    "\n",
    "gif_dir = gif_dir + '/' + env_name\n",
    "if not os.path.exists(gif_dir):\n",
    "    os.makedirs(gif_dir)\n",
    "\n",
    "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + '.gif'\n",
    "\n",
    "\n",
    "\n",
    "img_paths = sorted(glob.glob(gif_images_dir))\n",
    "img_paths = img_paths[:total_timesteps]\n",
    "img_paths = img_paths[::step]\n",
    "\n",
    "\n",
    "print(\"total frames in gif : \", len(img_paths))\n",
    "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# save gif\n",
    "img, *imgs = [Image.open(f) for f in img_paths]\n",
    "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
    "\n",
    "print(\"saved gif at : \", gif_path)\n",
    "\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20d1bR8xHK5j"
   },
   "outputs": [],
   "source": [
    "\n",
    "############################# check gif byte size ##############################\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "# env_name = 'LunarLander-v2'\n",
    "# env_name = 'BipedalWalker-v2'\n",
    "# env_name = 'RoboschoolWalker2d-v1'\n",
    "\n",
    "\n",
    "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
    "\n",
    "gif_paths = sorted(glob.glob(gif_dir))\n",
    "\n",
    "for gif_path in gif_paths:\n",
    "    file_size = os.path.getsize(gif_path)\n",
    "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM5UIAkcGxeA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################# End of Part V ################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YUzQOu1HYHR"
   },
   "source": [
    "################################################################################\n",
    "\n",
    "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "################################################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e7JowRQEGGKQ",
    "Z4VJcUT2GlJz"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
