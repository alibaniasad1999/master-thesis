{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Making My Own PPO for Cartpole"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b634adc07e28c"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "import gym"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.430430Z",
     "start_time": "2024-03-02T12:32:57.184685Z"
    }
   },
   "id": "bf9ed9eb4b548311"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.431160Z",
     "start_time": "2024-03-02T12:32:57.198701Z"
    }
   },
   "id": "ebfc0e753e14094d"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Define the custom Gym environment for the mass-spring-damper system\n",
    "class MassSpringDamperEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(MassSpringDamperEnv, self).__init__()\n",
    "\n",
    "        # System parameters\n",
    "        self.m = 1.0  # Mass (kg)\n",
    "        self.k = 1.0  # Spring constant (N/m)\n",
    "        self.c = 0.1  # Damping coefficient (N*s/m)\n",
    "\n",
    "        # Simulation parameters\n",
    "        self.dt = 0.01  # Time step (s)\n",
    "        self.max_steps = 1000  # Maximum simulation steps\n",
    "        self.current_step = 0\n",
    "\n",
    "        # State and action spaces\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,))\n",
    "\n",
    "        # Initial state\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self,\n",
    "              *,\n",
    "          seed: 5 = None,\n",
    "          return_info: bool = False,\n",
    "          options: 6 = None):\n",
    "        # Reset the environment to an initial state\n",
    "        self.state = np.random.uniform(low=-10, high=10, size=(2,))\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply control action and simulate one time step using Euler integration\n",
    "        force = action[0]\n",
    "        position, velocity = self.state\n",
    "\n",
    "        acceleration = (force - self.c * velocity - self.k * position) / self.m\n",
    "        velocity += acceleration * self.dt\n",
    "        position += velocity * self.dt\n",
    "\n",
    "        self.state = np.array([position, velocity])\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Calculate the reward (e.g., minimize position error)\n",
    "        reward = -abs(position)  # Negative position as the reward (minimize position error)\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Create the custom mass-spring-damper environment\n",
    "env = MassSpringDamperEnv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.431675Z",
     "start_time": "2024-03-02T12:32:57.211693Z"
    }
   },
   "id": "19b6d5db2aba27e4"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.432526Z",
     "start_time": "2024-03-02T12:32:57.240692Z"
    }
   },
   "id": "9b15799ddc9ab868"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # actor\n",
    "        if has_continuous_action_space:\n",
    "            self.actor = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, action_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "            \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "            if self.has_continuous_action_space:\n",
    "                self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "            else:\n",
    "                print(\"Policy is not stochastic, action std is not needed\")\n",
    "                \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "    \n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.477708Z",
     "start_time": "2024-03-02T12:32:57.275861Z"
    }
   },
   "id": "5c55bbaefab33807"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, action_range, lr_actor, lr_critic, gamma, k_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = k_epochs\n",
    "        self.action_range = action_range\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        else:\n",
    "            print(\"Policy is not stochastic, action std is not needed\")\n",
    "            \n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if self.action_std <= min_action_std:\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "        else:\n",
    "            print(\"Policy is not stochastic, action std is not needed\")\n",
    "            \n",
    "    def select_action(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "                action *= self.action_range\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "            \n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "        \n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "        \n",
    "        \n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.485360Z",
     "start_time": "2024-03-02T12:32:57.284364Z"
    }
   },
   "id": "b39f1699fab74418"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"MBK\"\n",
    "env_name = \"CartPole-v1\"\n",
    "has_continuous_action_space = False\n",
    "\n",
    "max_ep_len = 400                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = 0.1 \n",
    "max_ep_len = 1500           # max timesteps in one episode\n",
    "\n",
    "            \n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.485718Z",
     "start_time": "2024-03-02T12:32:57.341905Z"
    }
   },
   "id": "718df1cbc2644f50"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0 "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.486189Z",
     "start_time": "2024-03-02T12:32:57.352711Z"
    }
   },
   "id": "ea19c944145a48ed"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training environment name : CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.486815Z",
     "start_time": "2024-03-02T12:32:57.365305Z"
    }
   },
   "id": "c3c7304c735926bc"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "###################### logging ######################\n",
    "\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.487620Z",
     "start_time": "2024-03-02T12:32:57.376120Z"
    }
   },
   "id": "4566d7ea9294e642"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "# run_num = len(current_num_files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.488502Z",
     "start_time": "2024-03-02T12:32:57.383238Z"
    }
   },
   "id": "50b79741a2ff412"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current logging run number for CartPole-v1 :  0\n",
      "logging at : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_0.csv\n"
     ]
    }
   ],
   "source": [
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.657178Z",
     "start_time": "2024-03-02T12:32:57.391110Z"
    }
   },
   "id": "bd7d2d0a34c0a38c"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save checkpoint path : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n"
     ]
    }
   ],
   "source": [
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:32:57.658875Z",
     "start_time": "2024-03-02T12:32:57.403540Z"
    }
   },
   "id": "bb861f5883f8c087"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  100000\n",
      "max timesteps per episode :  1500\n",
      "model saving frequency : 20000 timesteps\n",
      "log frequency : 800 timesteps\n",
      "printing average reward over episodes in last : 1600 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  4\n",
      "action space dimension :  2\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a discrete action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 6000 timesteps\n",
      "PPO K epochs :  40\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2024-03-02 16:02:57\n",
      "============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ali/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 78 \t\t Timestep : 1600 \t\t Average Reward : 20.45\n",
      "Episode : 157 \t\t Timestep : 3200 \t\t Average Reward : 20.1\n",
      "Episode : 234 \t\t Timestep : 4800 \t\t Average Reward : 20.78\n",
      "Episode : 312 \t\t Timestep : 6400 \t\t Average Reward : 20.54\n",
      "Episode : 377 \t\t Timestep : 8000 \t\t Average Reward : 24.28\n",
      "Episode : 441 \t\t Timestep : 9600 \t\t Average Reward : 25.36\n",
      "Episode : 508 \t\t Timestep : 11200 \t\t Average Reward : 24.0\n",
      "Episode : 561 \t\t Timestep : 12800 \t\t Average Reward : 29.4\n",
      "Episode : 605 \t\t Timestep : 14400 \t\t Average Reward : 37.27\n",
      "Episode : 650 \t\t Timestep : 16000 \t\t Average Reward : 35.58\n",
      "Episode : 698 \t\t Timestep : 17600 \t\t Average Reward : 33.33\n",
      "Episode : 725 \t\t Timestep : 19200 \t\t Average Reward : 57.63\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:16\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 752 \t\t Timestep : 20800 \t\t Average Reward : 60.41\n",
      "Episode : 785 \t\t Timestep : 22400 \t\t Average Reward : 49.0\n",
      "Episode : 816 \t\t Timestep : 24000 \t\t Average Reward : 49.06\n",
      "Episode : 835 \t\t Timestep : 25600 \t\t Average Reward : 87.89\n",
      "Episode : 852 \t\t Timestep : 27200 \t\t Average Reward : 87.76\n",
      "Episode : 868 \t\t Timestep : 28800 \t\t Average Reward : 103.06\n",
      "Episode : 886 \t\t Timestep : 30400 \t\t Average Reward : 92.78\n",
      "Episode : 897 \t\t Timestep : 32000 \t\t Average Reward : 142.73\n",
      "Episode : 909 \t\t Timestep : 33600 \t\t Average Reward : 128.08\n",
      "Episode : 921 \t\t Timestep : 35200 \t\t Average Reward : 137.17\n",
      "Episode : 932 \t\t Timestep : 36800 \t\t Average Reward : 142.18\n",
      "Episode : 941 \t\t Timestep : 38400 \t\t Average Reward : 181.0\n",
      "Episode : 949 \t\t Timestep : 40000 \t\t Average Reward : 182.38\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:32\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 954 \t\t Timestep : 41600 \t\t Average Reward : 284.2\n",
      "Episode : 959 \t\t Timestep : 43200 \t\t Average Reward : 354.6\n",
      "Episode : 967 \t\t Timestep : 44800 \t\t Average Reward : 224.75\n",
      "Episode : 972 \t\t Timestep : 46400 \t\t Average Reward : 318.4\n",
      "Episode : 978 \t\t Timestep : 48000 \t\t Average Reward : 267.67\n",
      "Episode : 982 \t\t Timestep : 49600 \t\t Average Reward : 359.0\n",
      "Episode : 988 \t\t Timestep : 51200 \t\t Average Reward : 280.5\n",
      "Episode : 994 \t\t Timestep : 52800 \t\t Average Reward : 259.67\n",
      "Episode : 999 \t\t Timestep : 54400 \t\t Average Reward : 306.2\n",
      "Episode : 1004 \t\t Timestep : 56000 \t\t Average Reward : 320.8\n",
      "Episode : 1008 \t\t Timestep : 57600 \t\t Average Reward : 344.0\n",
      "Episode : 1013 \t\t Timestep : 59200 \t\t Average Reward : 360.8\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:50\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 1017 \t\t Timestep : 60800 \t\t Average Reward : 397.5\n",
      "Episode : 1021 \t\t Timestep : 62400 \t\t Average Reward : 396.75\n",
      "Episode : 1025 \t\t Timestep : 64000 \t\t Average Reward : 396.25\n",
      "Episode : 1029 \t\t Timestep : 65600 \t\t Average Reward : 370.0\n",
      "Episode : 1033 \t\t Timestep : 67200 \t\t Average Reward : 437.0\n",
      "Episode : 1038 \t\t Timestep : 68800 \t\t Average Reward : 344.4\n",
      "Episode : 1042 \t\t Timestep : 70400 \t\t Average Reward : 323.0\n",
      "Episode : 1046 \t\t Timestep : 72000 \t\t Average Reward : 471.0\n",
      "Episode : 1049 \t\t Timestep : 73600 \t\t Average Reward : 500.0\n",
      "Episode : 1052 \t\t Timestep : 75200 \t\t Average Reward : 500.0\n",
      "Episode : 1055 \t\t Timestep : 76800 \t\t Average Reward : 500.0\n",
      "Episode : 1059 \t\t Timestep : 78400 \t\t Average Reward : 492.25\n",
      "Episode : 1062 \t\t Timestep : 80000 \t\t Average Reward : 500.0\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:05\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 1065 \t\t Timestep : 81600 \t\t Average Reward : 500.0\n",
      "Episode : 1068 \t\t Timestep : 83200 \t\t Average Reward : 494.33\n",
      "Episode : 1071 \t\t Timestep : 84800 \t\t Average Reward : 500.0\n",
      "Episode : 1075 \t\t Timestep : 86400 \t\t Average Reward : 430.25\n",
      "Episode : 1079 \t\t Timestep : 88000 \t\t Average Reward : 485.5\n",
      "Episode : 1082 \t\t Timestep : 89600 \t\t Average Reward : 500.0\n",
      "Episode : 1086 \t\t Timestep : 91200 \t\t Average Reward : 405.5\n",
      "Episode : 1089 \t\t Timestep : 92800 \t\t Average Reward : 397.33\n",
      "Episode : 1093 \t\t Timestep : 94400 \t\t Average Reward : 421.0\n",
      "Episode : 1097 \t\t Timestep : 96000 \t\t Average Reward : 496.5\n",
      "Episode : 1100 \t\t Timestep : 97600 \t\t Average Reward : 500.0\n",
      "Episode : 1103 \t\t Timestep : 99200 \t\t Average Reward : 500.0\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:19\n",
      "--------------------------------------------------------------------------------------------\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2024-03-02 16:02:57\n",
      "Finished training at (GMT) :  2024-03-02 16:04:16\n",
      "Total training time  :  0:01:19\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    action_std_decay_rate = 0.05\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    min_action_std = 0.1\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    action_std_decay_freq = int(2.5e4)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "action_range = 100\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, action_range, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of output action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            if log_running_episodes == 0:\n",
    "                log_running_episodes = 1\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print total training time\n",
    "print(\"============================================================================================\")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\"============================================================================================\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:34:16.594468Z",
     "start_time": "2024-03-02T12:32:57.411735Z"
    }
   },
   "id": "128588120936a8f3"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading network from : PPO_preTrained/MBK/PPO_MBK_0_0.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCritic:\n\tsize mismatch for actor.0.weight: copying a param with shape torch.Size([64, 2]) from checkpoint, the shape in current model is torch.Size([64, 4]).\n\tsize mismatch for actor.4.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n\tsize mismatch for actor.4.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([64, 2]) from checkpoint, the shape in current model is torch.Size([64, 4]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 64\u001B[0m\n\u001B[1;32m     61\u001B[0m checkpoint_path \u001B[38;5;241m=\u001B[39m directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPPO_\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(env_name, random_seed, run_num_pretrained)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading network from : \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m checkpoint_path)\n\u001B[0;32m---> 64\u001B[0m \u001B[43mppo_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--------------------------------------------------------------------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     69\u001B[0m state_array \u001B[38;5;241m=\u001B[39m []\n",
      "Cell \u001B[0;32mIn[43], line 134\u001B[0m, in \u001B[0;36mPPO.load\u001B[0;34m(self, checkpoint_path)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mself\u001B[39m, checkpoint_path):\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_old\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloc\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(checkpoint_path, map_location\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m storage, loc: storage))\n",
      "File \u001B[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:2153\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2148\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2149\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2150\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2153\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2154\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ActorCritic:\n\tsize mismatch for actor.0.weight: copying a param with shape torch.Size([64, 2]) from checkpoint, the shape in current model is torch.Size([64, 4]).\n\tsize mismatch for actor.4.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n\tsize mismatch for actor.4.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([64, 2]) from checkpoint, the shape in current model is torch.Size([64, 4])."
     ]
    }
   ],
   "source": [
    "#################################### Testing ###################################\n",
    "\n",
    "\n",
    "################## hyperparameters ##################\n",
    "\n",
    "env_name = \"MBK\"\n",
    "has_continuous_action_space = False\n",
    "max_ep_len = 1500           # max timesteps in one episode\n",
    "action_std = 0.1            # set same std for action distribution which was used while saving\n",
    "render = None\n",
    "\n",
    "\n",
    "\n",
    "# env_name = \"LunarLander-v2\"\n",
    "# has_continuous_action_space = False\n",
    "# max_ep_len = 300\n",
    "# action_std = None\n",
    "\n",
    "\n",
    "# env_name = \"RoboschoolWalker2d-v1\"\n",
    "# has_continuous_action_space = True\n",
    "# max_ep_len = 1000           # max timesteps in one episode\n",
    "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
    "\n",
    "\n",
    "total_test_episodes = 1    # total num of testing episodes\n",
    "\n",
    "K_epochs = 80               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003           # learning rate for actor\n",
    "lr_critic = 0.001           # learning rate for critic\n",
    "\n",
    "#####################################################\n",
    "\n",
    "if env_name != \"MBK\":\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, action_range, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# preTrained weights directory\n",
    "\n",
    "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
    "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
    "\n",
    "\n",
    "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"loading network from : \" + checkpoint_path)\n",
    "\n",
    "ppo_agent.load(checkpoint_path)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "state_array = []\n",
    "action_array = []\n",
    "test_running_reward = 0\n",
    "\n",
    "for ep in range(1, total_test_episodes+1):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    for t in range(1, max_ep_len+1):\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        state_array.append(state)\n",
    "        action_array.append(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # clear buffer    \n",
    "    ppo_agent.buffer.clear()\n",
    "\n",
    "    test_running_reward +=  ep_reward\n",
    "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
    "    # ep_reward = 0\n",
    "\n",
    "\n",
    "if env_name == \"MBK\":\n",
    "    # Plot the position and velocity of the mass-spring-damper system\n",
    "    import matplotlib.pyplot as plt\n",
    "    state_array = np.array(state_array)\n",
    "    plt.figure()\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(state_array[:, 0])\n",
    "    plt.ylabel('Position (m)')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(state_array[:, 1])\n",
    "    plt.ylabel('Velocity (m/s)')\n",
    "    plt.xlabel('Time step')\n",
    "    plt.show()\n",
    "    \n",
    "    # plot action\n",
    "    action_array = np.array(action_array)\n",
    "    plt.figure()\n",
    "    plt.plot(action_array)\n",
    "    plt.ylabel('Force (N)')\n",
    "    plt.xlabel('Time step')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "avg_test_reward = test_running_reward / total_test_episodes\n",
    "avg_test_reward = round(avg_test_reward, 2)\n",
    "print(\"average test reward : \" + str(avg_test_reward))\n",
    "\n",
    "print(\"============================================================================================\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:35:08.553742Z",
     "start_time": "2024-03-02T12:35:08.379233Z"
    }
   },
   "id": "19ea586959de3bbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-02T12:34:16.664376Z"
    }
   },
   "id": "c5b8f7dd1f6ca9df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
