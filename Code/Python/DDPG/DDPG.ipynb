{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Define the custom Gym environment for the mass-spring-damper system\n",
    "class MassSpringDamperEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(MassSpringDamperEnv, self).__init__()\n",
    "\n",
    "        # System parameters\n",
    "        self.m = 1.0  # Mass (kg)\n",
    "        self.k = 1.0  # Spring constant (N/m)\n",
    "        self.c = 0.1  # Damping coefficient (N*s/m)\n",
    "\n",
    "        # Simulation parameters\n",
    "        self.dt = 0.01  # Time step (s)\n",
    "        self.max_steps = 1000  # Maximum simulation steps\n",
    "        self.current_step = 0\n",
    "\n",
    "        # State and action spaces\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,))\n",
    "\n",
    "        # Initial state\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to an initial state\n",
    "        self.state = np.array([10.0, 0.0])  # Initial position and velocity\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply control action and simulate one time step using Euler integration\n",
    "        force = action[0]\n",
    "        position, velocity = self.state\n",
    "\n",
    "        acceleration = (force - self.c * velocity - self.k * position) / self.m\n",
    "        velocity += acceleration * self.dt\n",
    "        position += velocity * self.dt\n",
    "\n",
    "        self.state = np.array([position, velocity])\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Calculate the reward (e.g., minimize position error)\n",
    "        reward = -abs(position)  # Negative position as the reward (minimize position error)\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorlayer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtl\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# add arguments in command  --train/test\u001b[39;00m\n\u001b[1;32m     42\u001b[0m parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain or test neural net motor controller.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.11/site-packages/tensorlayer/__init__.py:47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorlayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rein\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorlayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorlayer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m app\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorlayer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_imports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyImport\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Lazy Imports\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.11/site-packages/tensorlayer/app/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#! /usr/bin/python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputer_vision_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuman_pose_estimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputer_vision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.11/site-packages/tensorlayer/app/computer_vision_object_detection/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#! /usr/bin/python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolov4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLOv4\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documents/BAI/Master/master-thesis/.env/lib/python3.11/site-packages/tensorlayer/app/computer_vision_object_detection/common.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#! /usr/bin/python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcolorsys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_tf\u001b[39m(conv_output, output_size, NUM_CLASS, STRIDES, ANCHORS, i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, XYSCALE\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deep Deterministic Policy Gradient (DDPG)\n",
    "-----------------------------------------\n",
    "An algorithm concurrently learns a Q-function and a policy.\n",
    "It uses off-policy data and the Bellman equation to learn the Q-function,\n",
    "and uses the Q-function to learn the policy.\n",
    "\n",
    "Reference\n",
    "---------\n",
    "Deterministic Policy Gradient Algorithms, Silver et al. 2014\n",
    "Continuous Control With Deep Reinforcement Learning, Lillicrap et al. 2016\n",
    "MorvanZhou's tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\n",
    "Environment\n",
    "-----------\n",
    "Openai Gym Pendulum-v0, continual action space\n",
    "\n",
    "Prerequisites\n",
    "-------------\n",
    "tensorflow >=2.0.0a0\n",
    "tensorflow-proactionsbility 0.6.0\n",
    "tensorlayer >=2.0.0\n",
    "\n",
    "To run\n",
    "------\n",
    "python tutorial_DDPG.py --train/test\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorlayer as tl\n",
    "\n",
    "# add arguments in command  --train/test\n",
    "parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "ENV_ID = 'Pendulum-v0'  # environment id\n",
    "RANDOM_SEED = 2  # random seed, can be either an int number or None\n",
    "RENDER = False  # render while training\n",
    "\n",
    "ALG_NAME = 'DDPG'\n",
    "TRAIN_EPISODES = 100  # total number of episodes for training\n",
    "TEST_EPISODES = 10  # total number of episodes for training\n",
    "MAX_STEPS = 200  # total number of steps for each episode\n",
    "\n",
    "LR_A = 0.001  # learning rate for actor\n",
    "LR_C = 0.002  # learning rate for critic\n",
    "GAMMA = 0.9  # reward discount\n",
    "TAU = 0.01  # soft replacement\n",
    "MEMORY_CAPACITY = 10000  # size of replay buffer\n",
    "BATCH_SIZE = 32  # update action batch size\n",
    "VAR = 2  # control exploration\n",
    "\n",
    "###############################  DDPG  ####################################\n",
    "\n",
    "\n",
    "class DDPG(object):\n",
    "    \"\"\"\n",
    "    DDPG class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_dim, state_dim, action_range):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, state_dim * 2 + action_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.action_dim, self.state_dim, self.action_range = action_dim, state_dim, action_range\n",
    "        self.var = VAR\n",
    "\n",
    "        W_init = tf.random_normal_initializer(mean=0, stddev=0.3)\n",
    "        b_init = tf.constant_initializer(0.1)\n",
    "\n",
    "        def get_actor(input_state_shape, name=''):\n",
    "            \"\"\"\n",
    "            Build actor network\n",
    "            :param input_state_shape: state\n",
    "            :param name: name\n",
    "            :return: act\n",
    "            \"\"\"\n",
    "            input_layer = tl.layers.Input(input_state_shape, name='A_input')\n",
    "            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='A_l1')(input_layer)\n",
    "            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='A_l2')(layer)\n",
    "            layer = tl.layers.Dense(n_units=action_dim, act=tf.nn.tanh, W_init=W_init, b_init=b_init, name='A_a')(layer)\n",
    "            layer = tl.layers.Lambda(lambda x: action_range * x)(layer)\n",
    "            return tl.models.Model(inputs=input_layer, outputs=layer, name='Actor' + name)\n",
    "\n",
    "        def get_critic(input_state_shape, input_action_shape, name=''):\n",
    "            \"\"\"\n",
    "            Build critic network\n",
    "            :param input_state_shape: state\n",
    "            :param input_action_shape: act\n",
    "            :param name: name\n",
    "            :return: Q value Q(s,a)\n",
    "            \"\"\"\n",
    "            state_input = tl.layers.Input(input_state_shape, name='C_s_input')\n",
    "            action_input = tl.layers.Input(input_action_shape, name='C_a_input')\n",
    "            layer = tl.layers.Concat(1)([state_input, action_input])\n",
    "            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='C_l1')(layer)\n",
    "            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='C_l2')(layer)\n",
    "            layer = tl.layers.Dense(n_units=1, W_init=W_init, b_init=b_init, name='C_out')(layer)\n",
    "            return tl.models.Model(inputs=[state_input, action_input], outputs=layer, name='Critic' + name)\n",
    "\n",
    "        self.actor = get_actor([None, state_dim])\n",
    "        self.critic = get_critic([None, state_dim], [None, action_dim])\n",
    "        self.actor.train()\n",
    "        self.critic.train()\n",
    "\n",
    "        def copy_para(from_model, to_model):\n",
    "            \"\"\"\n",
    "            Copy parameters for soft updating\n",
    "            :param from_model: latest model\n",
    "            :param to_model: target model\n",
    "            :return: None\n",
    "            \"\"\"\n",
    "            for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):\n",
    "                j.assign(i)\n",
    "\n",
    "        self.actor_target = get_actor([None, state_dim], name='_target')\n",
    "        copy_para(self.actor, self.actor_target)\n",
    "        self.actor_target.eval()\n",
    "\n",
    "        self.critic_target = get_critic([None, state_dim], [None, action_dim], name='_target')\n",
    "        copy_para(self.critic, self.critic_target)\n",
    "        self.critic_target.eval()\n",
    "\n",
    "        self.ema = tf.train.ExponentialMovingAverage(decay=1 - TAU)  # soft replacement\n",
    "\n",
    "        self.actor_opt = tf.optimizers.Adam(LR_A)\n",
    "        self.critic_opt = tf.optimizers.Adam(LR_C)\n",
    "\n",
    "    def ema_update(self):\n",
    "        \"\"\"\n",
    "        Soft updating by exponential smoothing\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        paras = self.actor.trainable_weights + self.critic.trainable_weights\n",
    "        self.ema.apply(paras)\n",
    "        for i, j in zip(self.actor_target.trainable_weights + self.critic_target.trainable_weights, paras):\n",
    "            i.assign(self.ema.average(j))\n",
    "\n",
    "    def get_action(self, s, greedy=False):\n",
    "        \"\"\"\n",
    "        Choose action\n",
    "        :param s: state\n",
    "        :param greedy: get action greedy or not\n",
    "        :return: act\n",
    "        \"\"\"\n",
    "        a = self.actor(np.array([s], dtype=np.float32))[0]\n",
    "        if greedy:\n",
    "            return a\n",
    "        return np.clip(\n",
    "            np.random.normal(a, self.var), -self.action_range, self.action_range\n",
    "        )  # add randomness to action selection for exploration\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Update parameters\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.var *= .9995\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        datas = self.memory[indices, :]\n",
    "        states = datas[:, :self.state_dim]\n",
    "        actions = datas[:, self.state_dim:self.state_dim + self.action_dim]\n",
    "        rewards = datas[:, -self.state_dim - 1:-self.state_dim]\n",
    "        states_ = datas[:, -self.state_dim:]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions_ = self.actor_target(states_)\n",
    "            q_ = self.critic_target([states_, actions_])\n",
    "            y = rewards + GAMMA * q_\n",
    "            q = self.critic([states, actions])\n",
    "            td_error = tf.losses.mean_squared_error(y, q)\n",
    "        critic_grads = tape.gradient(td_error, self.critic.trainable_weights)\n",
    "        self.critic_opt.apply_gradients(zip(critic_grads, self.critic.trainable_weights))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            a = self.actor(states)\n",
    "            q = self.critic([states, a])\n",
    "            actor_loss = -tf.reduce_mean(q)  # maximize the q\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_weights)\n",
    "        self.actor_opt.apply_gradients(zip(actor_grads, self.actor.trainable_weights))\n",
    "        self.ema_update()\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        \"\"\"\n",
    "        Store data in data buffer\n",
    "        :param s: state\n",
    "        :param a: act\n",
    "        :param r: reward\n",
    "        :param s_: next state\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        s = s.astype(np.float32)\n",
    "        s_ = s_.astype(np.float32)\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        save trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'actor_target.hdf5'), self.actor_target)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'critic_target.hdf5'), self.critic_target)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        load trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor_target.hdf5'), self.actor_target)\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic_target.hdf5'), self.critic_target)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = MassSpringDamperEnv()\n",
    "\n",
    "    # reproducible\n",
    "    # env.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_range = env.action_space.high  # scale action, [-action_range, action_range]\n",
    "\n",
    "    agent = DDPG(action_dim, state_dim, action_range)\n",
    "\n",
    "    t0 = time.time()\n",
    "    if args.train:  # train\n",
    "        all_episode_reward = []\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            for step in range(MAX_STEPS):\n",
    "                if RENDER:\n",
    "                    env.render()\n",
    "                # Add exploration noise\n",
    "                action = agent.get_action(state)\n",
    "                state_, reward, done, info = env.step(action)\n",
    "                agent.store_transition(state, action, reward, state_)\n",
    "\n",
    "                if agent.pointer > MEMORY_CAPACITY:\n",
    "                    agent.learn()\n",
    "\n",
    "                state = state_\n",
    "                episode_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if episode == 0:\n",
    "                all_episode_reward.append(episode_reward)\n",
    "            else:\n",
    "                all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\n",
    "            print(\n",
    "                'Training  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(\n",
    "                    episode + 1, TRAIN_EPISODES, episode_reward,\n",
    "                    time.time() - t0\n",
    "                )\n",
    "            )\n",
    "        agent.save()\n",
    "        plt.plot(all_episode_reward)\n",
    "        if not os.path.exists('image'):\n",
    "            os.makedirs('image')\n",
    "        plt.savefig(os.path.join('image', '_'.join([ALG_NAME, ENV_ID])))\n",
    "\n",
    "    if args.test:\n",
    "        # test\n",
    "        agent.load()\n",
    "        for episode in range(TEST_EPISODES):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            for step in range(MAX_STEPS):\n",
    "                env.render()\n",
    "                state, reward, done, info = env.step(agent.get_action(state, greedy=True))\n",
    "                episode_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            print(\n",
    "                'Testing  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(\n",
    "                    episode + 1, TEST_EPISODES, episode_reward,\n",
    "                    time.time() - t0\n",
    "                )\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
