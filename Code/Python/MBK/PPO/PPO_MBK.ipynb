{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e51_ttMvHS-b"
      },
      "source": [
        "# Proximal Policy Optimization\n",
        "\n",
        "**Author:** [Ali BaniAsad](https://github.com/alibaniasad1999)<br>\n",
        "**Date created:** 2024/05/24<br>\n",
        "**Last modified:** 2024/05/24<br>\n",
        "**Description:** Implementation of a Proximal Policy Optimization agent for the CartPole-v1 environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE8mXLERHS-h"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This code example solves the MBK (mass spring damper) environment using a Proximal Policy Optimization (PPO) agent.\n",
        "\n",
        "### MBK\n",
        "\n",
        "The system is controlled by applying a force within the range of +20 to -20. The system starts from a random position and speed.\n",
        "\n",
        "### Proximal Policy Optimization\n",
        "\n",
        "PPO is a policy gradient method and can be used for environments with either discrete or continuous action spaces.\n",
        "It trains a stochastic policy in an on-policy way. Also, it utilizes the actor critic method. The actor maps the\n",
        "observation to an action and the critic gives an expectation of the rewards of the agent for the observation given.\n",
        "Firstly, it collects a set of trajectories for each epoch by sampling from the latest version of the stochastic policy.\n",
        "Then, the rewards-to-go and the advantage estimates are computed in order to update the policy and fit the value function.\n",
        "The policy is updated via a stochastic gradient ascent optimizer, while the value function is fitted via some gradient descent algorithm.\n",
        "This procedure is applied for many epochs until the environment is solved.\n",
        "\n",
        "![Algorithm](https://i.imgur.com/rd5tda1.png)\n",
        "\n",
        "- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
        "- [OpenAI Spinning Up docs - PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)\n",
        "\n",
        "### Note\n",
        "\n",
        "This code example uses Keras and Tensorflow v2. It is based on the PPO Original Paper,\n",
        "the OpenAI's Spinning Up docs for PPO, and the OpenAI's Spinning Up implementation of PPO using Tensorflow v1.\n",
        "\n",
        "[OpenAI Spinning Up Github - PPO](https://github.com/openai/spinningup/blob/master/spinup/algos/tf1/ppo/ppo.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdIgoSw0HS-i"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "For this example the following libraries are used:\n",
        "\n",
        "1. `numpy` for n-dimensional arrays\n",
        "2. `tensorflow` and `keras` for building the deep RL PPO agent\n",
        "3. `gymnasium` for getting everything we need about the environment\n",
        "4. `scipy.signal` for calculating the discounted cumulative sums of vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gymnasium\n",
        "# !pip install --upgrade tensorflow\n",
        "# !pip install --upgrade tf-keras\n",
        "# !pip install --upgrade keras"
      ],
      "metadata": {
        "id": "4B0Kiy_wTIyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAgTGfg1HS-j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "import scipy.signal\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MassSpringDamperEnv(gym.Env):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MassSpringDamperEnv, self).__init__()\n",
        "\n",
        "        # System parameters\n",
        "        self.m = 1.0  # Mass (kg)\n",
        "        self.k = 1.0  # Spring constant (N/m)\n",
        "        self.c = 0.1  # Damping coefficient (N*s/m)\n",
        "\n",
        "        # Simulation parameters\n",
        "        self.dt = 0.01  # Time step (s)\n",
        "        self.max_steps = 1000  # Maximum simulation steps\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Integrator\n",
        "        self.integral_error = 0\n",
        "\n",
        "        # State and action spaces\n",
        "        self.action_space = gym.spaces.Box(low=-20.0, high=20.0, shape=(1,))\n",
        "        self.observation_space = gym.spaces.Box(low=-100, high=100, shape=(3,))\n",
        "\n",
        "    def step(self, u):\n",
        "        # Apply control action and simulate one time step using Euler integration\n",
        "        force = action[0]\n",
        "        position, velocity = self.state\n",
        "\n",
        "        acceleration = (force - self.c * velocity - self.k * position) / self.m\n",
        "        velocity += acceleration * self.dt\n",
        "        position += velocity * self.dt\n",
        "\n",
        "        self.state = np.array([position, velocity])\n",
        "        self.integral_error += position * self.dt\n",
        "\n",
        "\n",
        "        costs = position ** 2 + 0.1 * velocity**2 \\\n",
        "        + 0.1 * self.integral_error**2 + 0.001 * (force**2)\n",
        "\n",
        "        self.step_num += 1\n",
        "        if self.step_num > 1000:\n",
        "            self.done = True\n",
        "\n",
        "\n",
        "        return self._get_obs(), -costs, self.done, False, {}\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        super().reset(seed=seed)\n",
        "        self.state = np.random.uniform(low=-10, high=10, size=(2,))\n",
        "        self.current_step = 0\n",
        "        self.last_u = None\n",
        "        self.done = False\n",
        "        self.step_num = 0\n",
        "        self.integral_error = 0\n",
        "\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        position, velovity = self.state\n",
        "        return np.array([position, velovity, self.integral_error], dtype=np.float32)"
      ],
      "metadata": {
        "id": "6pHb5c0-TWZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the `render_mode` parameter to show the attempts of the agent in a pop up window.\n",
        "env = MassSpringDamperEnv()\n",
        "\n",
        "num_states = env.observation_space.shape[0]\n",
        "print(\"Size of State Space ->  {}\".format(num_states))\n",
        "num_actions = env.action_space.shape[0]\n",
        "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
        "print(\"Min Value of Action ->  {}\".format(lower_bound))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OkebsfkTauT",
        "outputId": "ef8de643-9cf9-4d24-b36d-ed25d2466a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of State Space ->  3\n",
            "Size of Action Space ->  1\n",
            "Max Value of Action ->  20.0\n",
            "Min Value of Action ->  -20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aupS-5-HS-l"
      },
      "source": [
        "## Functions and class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0Kuy0iSHS-l"
      },
      "outputs": [],
      "source": [
        "def discounted_cumulative_sums(x, discount):\n",
        "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "class Buffer:\n",
        "    # Buffer for storing trajectories\n",
        "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
        "        # Buffer initialization\n",
        "        self.observation_buffer = np.zeros(\n",
        "            (size, observation_dimensions), dtype=np.float32\n",
        "        )\n",
        "        self.action_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "\n",
        "    def store(self, observation, action, reward, value, logprobability):\n",
        "        # Append one step of agent-environment interaction\n",
        "        self.observation_buffer[self.pointer] = observation\n",
        "        self.action_buffer[self.pointer] = action\n",
        "        self.reward_buffer[self.pointer] = reward\n",
        "        self.value_buffer[self.pointer] = value\n",
        "        self.logprobability_buffer[self.pointer] = logprobability\n",
        "        self.pointer += 1\n",
        "\n",
        "    def finish_trajectory(self, last_value=0):\n",
        "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
        "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
        "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
        "        values = np.append(self.value_buffer[path_slice], last_value)\n",
        "\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "\n",
        "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
        "            deltas, self.gamma * self.lam\n",
        "        )\n",
        "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
        "            rewards, self.gamma\n",
        "        )[:-1]\n",
        "\n",
        "        self.trajectory_start_index = self.pointer\n",
        "\n",
        "    def get(self):\n",
        "        # Get all data of the buffer and normalize the advantages\n",
        "        self.pointer, self.trajectory_start_index = 0, 0\n",
        "        advantage_mean, advantage_std = (\n",
        "            np.mean(self.advantage_buffer),\n",
        "            np.std(self.advantage_buffer),\n",
        "        )\n",
        "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
        "        return (\n",
        "            self.observation_buffer,\n",
        "            self.action_buffer,\n",
        "            self.advantage_buffer,\n",
        "            self.return_buffer,\n",
        "            self.logprobability_buffer,\n",
        "        )\n",
        "\n",
        "\n",
        "def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
        "    # Build a feedforward neural network\n",
        "    for size in sizes[:-1]:\n",
        "        x = layers.Dense(units=size, activation=activation)(x)\n",
        "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
        "\n",
        "# Define the log-probabilities function for continuous actions\n",
        "def logprobabilities(mean, log_std, actions):\n",
        "    std = tf.exp(log_std)\n",
        "    var = std ** 2\n",
        "    # print(actions.dtype)\n",
        "    # print(mean.dtype)\n",
        "    # print(var.dtype)\n",
        "\n",
        "    logp = -0.5 * (((actions - mean) ** 2) / var + 2 * log_std + tf.math.log(2 * np.pi))\n",
        "    return tf.reduce_sum(logp, axis=-1)\n",
        "\n",
        "\n",
        "# Sample action from actor\n",
        "@tf.function\n",
        "def sample_action(observation):\n",
        "    # Get the mean and log standard deviation from the actor network\n",
        "    mean, log_std = actor(observation)\n",
        "\n",
        "    # Convert log standard deviation to standard deviation\n",
        "    std = tf.exp(log_std)\n",
        "\n",
        "    # Sample a continuous action from the Gaussian distribution\n",
        "    action = mean + std * tf.random.normal(shape=mean.shape)\n",
        "\n",
        "    return mean, log_std, action\n",
        "\n",
        "\n",
        "# Train the policy by maximizing the PPO-Clip objective\n",
        "@tf.function\n",
        "def train_policy(\n",
        "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
        "):\n",
        "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
        "        mean, log_std = actor(observation_buffer)\n",
        "        ratio = tf.exp(\n",
        "            logprobabilities(mean, log_std, action_buffer)\n",
        "            - logprobability_buffer\n",
        "        )\n",
        "        min_advantage = tf.where(\n",
        "            advantage_buffer > 0,\n",
        "            (1 + clip_ratio) * advantage_buffer,\n",
        "            (1 - clip_ratio) * advantage_buffer,\n",
        "        )\n",
        "\n",
        "        policy_loss = -tf.reduce_mean(\n",
        "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
        "        )\n",
        "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
        "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
        "\n",
        "    mean, log_std = actor(observation_buffer)\n",
        "    kl = logprobability_buffer - logprobabilities(mean, log_std, action_buffer)\n",
        "    kl = tf.reduce_sum(kl)\n",
        "    return kl\n",
        "\n",
        "\n",
        "# Train the value function by regression on mean-squared error\n",
        "@tf.function\n",
        "def train_value_function(observation_buffer, return_buffer):\n",
        "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
        "        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n",
        "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
        "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53SbeqMQHS-m"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbvIg25-HS-m"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters of the PPO algorithm\n",
        "steps_per_epoch = 4000\n",
        "epochs = 30\n",
        "gamma = 0.99\n",
        "clip_ratio = 0.2\n",
        "policy_learning_rate = 3e-4\n",
        "value_function_learning_rate = 1e-3\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "lam = 0.97\n",
        "target_kl = 0.01\n",
        "hidden_sizes = (64, 64)\n",
        "\n",
        "# True if you want to render the environment\n",
        "render = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j84BeC6rHS-n"
      },
      "source": [
        "## Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMreSzFNHS-n"
      },
      "outputs": [],
      "source": [
        "# Initialize the environment and get the dimensionality of the\n",
        "# observation space and the number of possible actions\n",
        "observation_dimensions = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.shape[0]\n",
        "\n",
        "# Initialize the buffer\n",
        "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
        "\n",
        "# Define the observation input\n",
        "observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
        "\n",
        "# Define the actor model\n",
        "dense1_actor = keras.layers.Dense(64, activation='relu')(observation_input)\n",
        "dense2_actor = keras.layers.Dense(64, activation='relu')(dense1_actor)\n",
        "mean_output = keras.layers.Dense(num_actions)(dense2_actor)  # Output mean of the Gaussian distribution\n",
        "log_std_output = keras.layers.Dense(num_actions)(dense2_actor)  # Output log standard deviation of the Gaussian distribution\n",
        "actor = keras.Model(inputs=observation_input, outputs=[mean_output, log_std_output])\n",
        "\n",
        "# Define the critic model\n",
        "dense1_critic = keras.layers.Dense(64, activation='relu')(observation_input)\n",
        "dense2_critic = keras.layers.Dense(64, activation='relu')(dense1_critic)\n",
        "value = keras.layers.Dense(1)(dense2_critic)  # Output single value for the critic\n",
        "critic = keras.Model(inputs=observation_input, outputs=value)\n",
        "\n",
        "# Initialize the policy and the value function optimizers\n",
        "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
        "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
        "\n",
        "# Initialize the observation, episode return and episode length\n",
        "observation, _ = env.reset()\n",
        "episode_return, episode_length = 0, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLJjsnRmHS-o"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvzrdiYvHS-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed8c986-7f3c-4314-86e8-6ff23f86dabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 1. Mean Return: [-25340.152]. Mean Length: 1000.0\n",
            " Epoch: 2. Mean Return: [-25572.824]. Mean Length: 1000.0\n",
            " Epoch: 3. Mean Return: [-37354.77]. Mean Length: 1000.0\n",
            " Epoch: 4. Mean Return: [-22843.035]. Mean Length: 1000.0\n",
            " Epoch: 5. Mean Return: [-13491.779]. Mean Length: 1000.0\n",
            " Epoch: 6. Mean Return: [-16818.844]. Mean Length: 1000.0\n",
            " Epoch: 7. Mean Return: [-22544.441]. Mean Length: 1000.0\n",
            " Epoch: 8. Mean Return: [-11282.137]. Mean Length: 1000.0\n",
            " Epoch: 9. Mean Return: [-35480.49]. Mean Length: 1000.0\n",
            " Epoch: 10. Mean Return: [-39227.375]. Mean Length: 1000.0\n",
            " Epoch: 11. Mean Return: [-24006.922]. Mean Length: 1000.0\n",
            " Epoch: 12. Mean Return: [-19137.08]. Mean Length: 1000.0\n",
            " Epoch: 13. Mean Return: [-23639.291]. Mean Length: 1000.0\n",
            " Epoch: 14. Mean Return: [-31582.23]. Mean Length: 1000.0\n",
            " Epoch: 15. Mean Return: [-20814.088]. Mean Length: 1000.0\n",
            " Epoch: 16. Mean Return: [-36711.395]. Mean Length: 1000.0\n",
            " Epoch: 17. Mean Return: [-32378.447]. Mean Length: 1000.0\n",
            " Epoch: 18. Mean Return: [-23974.188]. Mean Length: 1000.0\n",
            " Epoch: 19. Mean Return: [-20360.527]. Mean Length: 1000.0\n",
            " Epoch: 20. Mean Return: [-23295.768]. Mean Length: 1000.0\n",
            " Epoch: 21. Mean Return: [-20393.299]. Mean Length: 1000.0\n",
            " Epoch: 22. Mean Return: [-26528.88]. Mean Length: 1000.0\n",
            " Epoch: 23. Mean Return: [-19315.656]. Mean Length: 1000.0\n",
            " Epoch: 24. Mean Return: [-24577.545]. Mean Length: 1000.0\n",
            " Epoch: 25. Mean Return: [-26262.268]. Mean Length: 1000.0\n",
            " Epoch: 26. Mean Return: [-22499.81]. Mean Length: 1000.0\n",
            " Epoch: 27. Mean Return: [-9454.5205]. Mean Length: 1000.0\n",
            " Epoch: 28. Mean Return: [-38712.29]. Mean Length: 1000.0\n",
            " Epoch: 29. Mean Return: [-14499.66]. Mean Length: 1000.0\n",
            " Epoch: 30. Mean Return: [-19744.027]. Mean Length: 1000.0\n"
          ]
        }
      ],
      "source": [
        "# Iterate over the number of epochs\n",
        "for epoch in range(epochs):\n",
        "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
        "    sum_return = 0\n",
        "    sum_length = 0\n",
        "    num_episodes = 0\n",
        "\n",
        "    # Iterate over the steps of each epoch\n",
        "    for t in range(steps_per_epoch):\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        # Get the logits, action, and take one step in the environment\n",
        "        observation = observation.reshape(1, -1)\n",
        "        mean, log_std, action = sample_action(observation)\n",
        "        observation_new, reward, done, _, _ = env.step(action[0].numpy())\n",
        "        episode_return += reward\n",
        "        episode_length += 1\n",
        "\n",
        "        # Get the value and log-probability of the action\n",
        "        value_t = critic(observation)\n",
        "        logprobability_t = logprobability_t = logprobabilities(mean, log_std, action)\n",
        "\n",
        "        # Store obs, act, rew, v_t, logp_pi_t\n",
        "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
        "\n",
        "        # Update the observation\n",
        "        observation = observation_new\n",
        "\n",
        "        # Finish trajectory if reached to a terminal state\n",
        "        terminal = done\n",
        "        if terminal or (t == steps_per_epoch - 1):\n",
        "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
        "            buffer.finish_trajectory(last_value)\n",
        "            sum_return += episode_return\n",
        "            sum_length += episode_length\n",
        "            num_episodes += 1\n",
        "            observation, _ = env.reset()\n",
        "            episode_return, episode_length = 0, 0\n",
        "\n",
        "    # Get values from the buffer\n",
        "    (\n",
        "        observation_buffer,\n",
        "        action_buffer,\n",
        "        advantage_buffer,\n",
        "        return_buffer,\n",
        "        logprobability_buffer,\n",
        "    ) = buffer.get()\n",
        "\n",
        "    # Update the policy and implement early stopping using KL divergence\n",
        "    for _ in range(train_policy_iterations):\n",
        "        kl = train_policy(\n",
        "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
        "        )\n",
        "        if kl > 1.5 * target_kl:\n",
        "            # Early Stopping\n",
        "            break\n",
        "\n",
        "    # Update the value function\n",
        "    for _ in range(train_value_iterations):\n",
        "        train_value_function(observation_buffer, return_buffer)\n",
        "\n",
        "    # Print mean return and length for each epoch\n",
        "    print(\n",
        "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}