\section{عامل گرادیان سیاست عمیق قطعی }

گرادیان سیاست عمیق قطعی\LTRfootnote{Deep Deterministic Policy Gradient (DDPG)}
الگوریتمی است که همزمان یک تابع \lr{Q} و یک سیاست را یاد می‌گیرد. این الگوريتم برای یادگیری تابع \lr{Q} از داده‌های غیرسیاست محور\LTRfootnote{Off-Policy}
 و معادله بلمن استفاده می‌کند. این الگوریتم برای یادگیری سیاست نیز از تابع \lr{Q} استفاده می‌کند.
 
 این رویکرد وابستگی نزدیکی به یادگیری \lr{Q} دارد. اگر تابع ارزش-عمل بهینه 
% \lr{(optimal action-value function)}
 مشخص باشد، در هر حالت داده شده عمل بهینه
%  \lr{(optimal action)} 
  را می‌توان با حل کردن
 معادله
 \eqref{eq:optimal_action_value_function}
  به دست آورد.
\begin{equation}
	\label{eq:optimal_action_value_function}
	a^*(s) = \arg \max_a Q^*(s,a)
\end{equation}
الگوریتم
\lr{DDPG}
ترکیبی از یادگیری تقریبی برای $ Q^*(s,a)$ و یادگیری تقریبی برای
 $a^*(s)$ 
 است و به نحوی طراحی شده‌است که برای محیط‌هایی با فضاهای عمل پیوسته مناسب باشد. 
% اما چه معنی دارد که DDPG به طور خاص برای محیط‌هایی با فضاهای عمل پیوسته مناسب است؟ 
% این به روش محاسبه بیشینه عمل‌ها در
%  $\max_a Q^*(s,a)$
%   مربوط است.
   روش محاسبه 
   $a^*(s)$
    در این الگوریتم
    آن را برای فضای پیوسته مناسب می‌کند.
    از آنجا که فضای عمل پیوسته است، فرض می‌شود که تابع
     $Q^*(s,a)$
      نسبت به آرگومان عمل مشتق‌پذیر است. مشتق‌پذیری این امکان را می‌دهد که یک روش یادگیری مبتنی بر گرادیان برای سیاست
       $\mu(s)$
        استفاده شود. سپس، به جای اجرای یک بهینه‌سازی زمان‌بر در هر بار محاسبه
        $\max_a Q(s,a)$،
       می‌توان آن را با رابطه
        $\max_a Q(s,a) \approx Q(s,\mu(s))$
         تقریب زد.
         
         
\subsection{ یادگیری
\lr{Q}
در \lr{DDPG}
}
معادله بلمن که تابع ارزش عمل بهینه
 $(Q^*(s,a))$
  را توصیف می‌کند، در پایین آورده شده‌است.
\begin{equation}
	Q^*(s,a) = \underset{s' \sim P}{{\mathrm E}}\left[r(s,a) + \gamma \max_{a'} Q^*(s', a')\right]
\end{equation}
جمله 
$s' \sim P$ 
به این معنی است که وضعیت بعدی $s'$ توسط محیط از توزیع احتمال
 $P(\cdot| s,a)$
 نمونه‌گرفته می‌شود.
 معادله بلمن نقطه شروع برای یادگیری
  $Q^*(s,a)$
  با یک مقداردهی تقریبی
   است. پارامترهای یک شبکه عصبی
    $Q_{\phi}(s,a)$
    با علامت
    $\phi$ 
    نشان داده شده‌است.
    یک مجموعه
     ${\mathcal D}$
      از  تغییر از یک حالت به حالت دیگر
       $(s,a,r,s',d)$
       (که $d$ نشان می‌دهد که آیا وضعیت $s'$ پایانی است یا خیر) جمع‌آوری شده‌است.
        یک تابع خطای میانگین مربعات بلمن (\lr{MSBE}) استفاده شده‌است که معیاری برای نزدیکی
         $Q_{\phi}$
          برای برآورده کردن معادله بلمن است.
          
          \begin{equation}
          	L(\phi, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
          	\Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a') \right) \Bigg)^2
          	\right]
          \end{equation}
          
          
          
          
          در الگوریتم \lr{DDPG} دو ترفند برای عمکرد بهتر استفاده شده‌است که در ادامه به بررسی آن پرداخته شده‌است.
          \begin{itemize}
          	\item بافرهای بازی
          	
          	 الگوریتم‌های یادگیری تقویتی جهت آموزش یک شبکه عصبی عمیق برای تقریب 
          	 $Q^*(s,a)$
          	  از  بافرهای بازی\LTRfootnote{Replay Buffers}
          	   تجربه‌شده استفاده می‌کنند. 
          	           این مجموعه 
          	   ${\mathcal D}$
          	   شامل تجربیات قبلی است. برای داشتن رفتار پایدار در الگوریتم، بافر بازی باید به اندازه کافی بزرگ باشد تا شامل یک دامنه گسترده از تجربیات شود. انتخاب داده‌های بافر به دقت انجام شده‌است چرا که اگر فقط از داده‌های بسیار جدید استفاده شود، بیش‌برازش\LTRfootnote{Overfit}
          	   رخ می‌دهید و اگر از تجربه بیش از حد استفاده شود، ممکن است فرآیند یادگیری کند شود.
          	\item شبکه‌های هدف
          	
          	الگوریتم‌های یادگیری \lr{Q} از شبکه‌های هدف استفاده می‌کنند. اصطلاح زیر به‌عنوان هدف شناخته می‌شود.
          	\begin{equation}\label{eq:target}
          		r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a')
          	\end{equation}
          	در هنگام کمینه کردن تابع خطای میانگین مربعات بلمن، سعی شده‌است تا تابع \lr{Q} شبیه‌تر به این هدف یعنی رابطه \eqref{eq:target} شود. اما مشکل این است که هدف بستگی به پارامترهای در حال آموزش $\phi$ دارد.
 این باعث ایجاد ناپایداری در کمینه کردن تابع خطای میانگین مربعات بلمن می‌شود. راه حل آن استفاده از یک مجموعه پارامترهایی که با تأخیر زمانی به 
          	$\phi$
          	 نزدیک می‌شوند. به عبارت دیگر، یک شبکه دوم  ایجاد می‌شود که به آن شبکه هدف گفته می‌شود. شبکه هدف دنباله‌ی شبکه اول را دنبال می‌کند. پارامترهای شبکه هدف با نشان 
          	​$\phi_{\text{targ}}$
          	نشان داده می‌شوند.
          	در الگوریتم \lr{DDPG}، شبکه هدف در هر به‌روزرسانی شبکه اصلی، با میانگین‌گیری پولیاک\LTRfootnote{Polyak Averaging}
          	 به‌روزرسانی می‌شود.
          	 \begin{equation}
          	 	\phi_{\text{targ}} \leftarrow \rho \phi_{\text{targ}} + (1 - \rho) \phi
          	 \end{equation}
          	 در رابطه بالا $\rho$ یک فراپارامتر\LTRfootnote{Hyperparameter} است که بین صفر و یک انتخاب می‌شود. در این پژوهش این مقدار نزدیک به یک درنظرگرفته شده‌است.
          \end{itemize}
          
          
          الگوریتم
          \lr{DDPG}
           نیاز به یک شبکه سیاست هدف
           $(\mu_{\theta_{\text{targ}}})$
            برای محاسبه عمل‌هایی که به‌طور تقریبی بیشینه
            $Q_{\phi_{\text{targ}}}$
             را حاصل کند، را دارد. برای رسیدن به این شبکه سیاست هدف
              از همان روشی که تابع \lr{Q} به دست می‌آید یعنی با میانگین‌گیری پولیاک از پارامترهای سیاست در طول زمان آموزش استفاده می‌شود.
          
          
          با درنظرگرفتن موارد اشاره‌شده، یادگیری \lr{Q} در \lr{DDPG} با کمینه کردن تابع خطای میانگین مربعات بلمن (\lr{MSBE}) یعنی معادله \eqref{eq:msbe} با استفاده از کاهش گرادیان تصادفی\LTRfootnote{Stochastic Gradient Descent}
           انجام می‌شود.
          \begin{equation}\label{eq:msbe}
          	L(\phi, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
          	\Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) Q_{\phi_{\text{targ}}}(s', \mu_{\theta_{\text{targ}}}(s')) \right) \Bigg)^2
          	\right]
          \end{equation}
          
          
          \subsection{ سیاست
          	در \lr{DDPG}
          }
  در این بخش یک سیاست تعیین‌شده 
  $\mu_{\theta}(s)$
   یادگرفته می‌شود تا عملی را انجام می‌دهد که بیشینه 
   $Q_{\phi}(s,a)$
    رخ دهد. از آنجا که فضای عمل پیوسته است و فرض شده‌است که تابع \lr{Q} نسبت به عمل مشتق‌پذیر است، معادله زیر با استفاده از صعود گرادیان\LTRfootnote{Gradient Ascent}
     (تنها نسبت به پارامترهای سیاست) حل می‌شود.
     \begin{equation}
     	\max_{\theta} \underset{s \sim {\mathcal D}}{{\mathrm E}}\left[ Q_{\phi}(s, \mu_{\theta}(s)) \right]
     \end{equation}
     
     
     
     \subsection{اکتشاف و بهره‌برداری در 
     \lr{DDPG}
     }
       برای بهبود اکتشاف\LTRfootnote{Exploration}
        در سیاست‌های \lr{DDPG}، در زمان آموزش نویز به عمل‌ها اضافه می‌شود. نویسندگان مقاله اصلی \lr{DDPG} توصیه کرده‌اند که نویز 
        \lr{OU}\LTRfootnote{Ornstein–Uhlenbeck} 
        با زمان‌بندی هم‌ارتباطی\LTRfootnote{Time-Correlated} اضافه شود.
%         اما نتایج به‌روزتر نشان می‌دهد که نویز گوسی بدون هم‌ارتباط\LTRfootnote{Uncorrelated} و میانگین صفر کاملاً موثر عمل می‌کند. از آنجا که نویز گوسی با میانگین صفر ساده‌تر است، در این پژوهش از این روش استفاده شده‌است.
        در زمان سنجش بهره‌برداری\LTRfootnote{Exploitation} سیاست از آنچه یادگرفته است، نویز به عمل‌ها اضافه نمی‌شود.
        
        
        \subsection{شبه‌کد}
        
       در این بخش الگوریتم
       \lr{DDPG}
        پیاده‌سازی شده آورده شده است. در این پژوهش الگوریتم~\رجوع{alg:DDPG} در محیط پایتون با استفاده از کتابخانه
         \lr{TensorFlow} \cite{tensorflow2015-whitepaper}
          پیاده‌سازی شده ‌است.
        
%        \شروع{الگوریتم}{گرادیان سیاست عمیق قطعی}
%        \ورودی پارامترهای اولیه سیاست
%         $(\theta)$، پارامترهای تابع
%         \lr{Q}
%         $(\phi)$، بافر بازی خالی $(\mathcal{D})$
%%        \خروجی یک پوشش رأسی از $G$
%\State پارامترهای هدف را برابر با پارامترهای اصلی قرار دهید
%$\theta_{\text{targ}} \leftarrow \theta$, $\phi_{\text{targ}} \leftarrow \phi$
%        
%%        \دستور قرار بده $C = \emptyset$  % \توضیحات{مقداردهی اولیه}
%\While{همگرایی رخ دهد}
%\State 
%وضعیت $(s)$ را مشاهده کرده و عمل $a = \text{clip}(\mu_{\theta}(s) + \epsilon, a_{\text{Low}}, a_{\text{High}})$ را انتخاب کنید، به‌طوری که $\epsilon \sim \mathcal{N}$ است.
%\State عمل $a$ را در محیط اجرا کنید.
%\State وضعیت بعدی $s'$، پاداش $r$ و سیگنال پایان $d$ را مشاهده کنید تا نشان دهد آیا $s'$ پایانی است یا خیر.
%\State اگر $s'$ پایانی است، وضعیت محیط را بازنشانی کنید.
%\If{زمان به‌روزرسانی فرا رسیده است}
%\For{هر تعداد به‌روزرسانی}
%\State یک دسته تصادفی گذر از ‌یک حالت به حالت دیگر، $B = \{ (s,a,r,s',d) \}$، از $\mathcal{D}$ نمونه‌گیری شود.
%\State
% اهداف را محاسبه کنید:
%\[ y(r,s',d) = r + \gamma (1-d) Q_{\phi_{\text{targ}}}(s', \mu_{\theta_{\text{targ}}}(s')) \]
%\State تابع \lr{Q} را با یک مرحله از نزول گرادیان با استفاده از رابطه زیر به‌روزرسانی کنید:
%\[ \nabla_{\phi} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi}(s,a) - y(r,s',d) \right)^2 \]
%\State سیاست را با یک مرحله از صعود گرادیان با استفاده از رابطه زیر به‌روزرسانی کنید:
%\[ \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B}Q_{\phi}(s, \mu_{\theta}(s)) \]
%\State شبکه‌های هدف را با استفاده از معادلات زیر به‌روزرسانی کنید:
%\begin{align*}
%	\phi_{\text{targ}} &\leftarrow \rho \phi_{\text{targ}} + (1-\rho) \phi \\
%	\theta_{\text{targ}} &\leftarrow \rho \theta_{\text{targ}} + (1-\rho) \theta
%\end{align*}
%\EndFor
%\EndIf
%\EndWhile
%%        \تاوقتی{$E$ تهی نیست}
%%        %\اگر{$|E| > 0$}
%%        %	\دستور{یک کاری انجام بده}
%%        %\پایان‌اگر
%%        \If{$i\geq 5$} 
%%        \State $i \gets i-1$ \do
%%        \Else
%%        \If{$i\leq 3$}
%%        \State $i \gets i+2$
%%        \EndIf
%%        \EndIf
%%        \دستور یال دل‌‌خواه $uv \in E$ را انتخاب کن
%%        \دستور رأس‌های $u$ و $v$ را به $C$ اضافه کن
%%        \دستور تمام یال‌های واقع بر $u$ یا $v$ را از $E$ حذف کن
%%        \پایان‌تاوقتی
%%        \دستور $C$ را برگردان
%        \پایان{الگوریتم}
        
        
%         \STATE Update target networks with \begin{align*} \phi_{\text{targ}} &\leftarrow \rho \phi_{\text{targ}} + (1-\rho) \phi \\ \theta_{\text{targ}} &\leftarrow \rho \theta_{\text{targ}} + (1-\rho) \theta \end{align*} \ENDFOR \ENDIF \UNTIL{convergence} \end{algorithmic} \end{algorithm}
%        
        
        
        \begin{algorithm}[H]
        	\caption{گرادیان سیاست عمیق قطعی}\label{alg:DDPG}
        	\begin{algorithmic}[1]
  \ورودی پارامترهای اولیه سیاست
$(\theta)$، پارامترهای تابع
\lr{Q}
$(\phi)$، بافر بازی خالی $(\mathcal{D})$
%        \خروجی یک پوشش رأسی از $G$
\State پارامترهای هدف را برابر با پارامترهای اصلی قرار دهید
$\theta_{\text{targ}} \leftarrow \theta$, $\phi_{\text{targ}} \leftarrow \phi$

%        \دستور قرار بده $C = \emptyset$  % \توضیحات{مقداردهی اولیه}
\While{همگرایی رخ دهد}
\State 
وضعیت $(s)$ را مشاهده کرده و عمل $a = \text{clip}(\mu_{\theta}(s) + \epsilon, a_{\text{Low}}, a_{\text{High}})$ را انتخاب کنید، به‌طوری که $\epsilon \sim \mathcal{N}$ است.
\State عمل $a$ را در محیط اجرا کنید.
\State وضعیت بعدی $s'$، پاداش $r$ و سیگنال پایان $d$ را مشاهده کنید تا نشان دهد آیا $s'$ پایانی است یا خیر.
\State اگر $s'$ پایانی است، وضعیت محیط را بازنشانی کنید.
\If{زمان به‌روزرسانی فرا رسیده است}
\For{هر تعداد به‌روزرسانی}
\State یک دسته تصادفی گذر از ‌یک حالت به حالت دیگر، $B = \{ (s,a,r,s',d) \}$، از $\mathcal{D}$ نمونه‌گیری شود.
\State
اهداف را محاسبه کنید:
\[ y(r,s',d) = r + \gamma (1-d) Q_{\phi_{\text{targ}}}(s', \mu_{\theta_{\text{targ}}}(s')) \]
\State تابع \lr{Q} را با یک مرحله از نزول گرادیان با استفاده از رابطه زیر به‌روزرسانی کنید:
\[ \nabla_{\phi} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi}(s,a) - y(r,s',d) \right)^2 \]
\State سیاست را با یک مرحله از صعود گرادیان با استفاده از رابطه زیر به‌روزرسانی کنید:
\[ \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B}Q_{\phi}(s, \mu_{\theta}(s)) \]
\State شبکه‌های هدف را با استفاده از معادلات زیر به‌روزرسانی کنید:
\begin{align*}
	\phi_{\text{targ}} &\leftarrow \rho \phi_{\text{targ}} + (1-\rho) \phi \\
	\theta_{\text{targ}} &\leftarrow \rho \theta_{\text{targ}} + (1-\rho) \theta
\end{align*}
\EndFor
\EndIf
\EndWhile
%        \تاوقتی{$E$ تهی نیست}
%        %\اگر{$|E| > 0$}
%        %	\دستور{یک کاری انجام بده}
%        %\پایان‌اگر
%        \If{$i\geq 5$} 
%        \State $i \gets i-1$ \do
%        \Else
%        \If{$i\leq 3$}
%        \State $i \gets i+2$
%        \EndIf
%        \EndIf
%        \دستور یال دل‌‌خواه $uv \in E$ را انتخاب کن
%        \دستور رأس‌های $u$ و $v$ را به $C$ اضافه کن
%        \دستور تمام یال‌های واقع بر $u$ یا $v$ را از $E$ حذف کن
%        \پایان‌تاوقتی
%        \دستور $C$ را برگردان
        	\end{algorithmic}
        \end{algorithm}
        
     


