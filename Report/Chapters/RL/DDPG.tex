\section{عامل گرادیان سیاست عمیق قطعی }

گرادیان سیاست عمیق قطعی\LTRfootnote{Deep Deterministic Policy Gradient (DDPG)}
الگوریتمی است که همزمان یک تابع \lr{Q} و یک سیاست را یاد می‌گیرد. این الگوريتم برای یادگیری تابع \lr{Q} از داده‌های غیرسیاست محور\LTRfootnote{Off-Policy}
 و معادله بلمن استفاده می‌کند. این الگوریتم برای یادگیری سیاست نیز از تابع \lr{Q} استفاده می‌کند.
 
 این رویکرد وابستگی نزدیکی به یادگیری \lr{Q} دارد. اگر تابع ارزش-عمل بهینه 
% \lr{(optimal action-value function)}
 مشخص باشد، در هر حالت داده شده عمل بهینه
%  \lr{(optimal action)} 
  را می‌توان با حل کردن
 معادله
 \eqref{eq:optimal_action_value_function}
  به دست آورد.
\begin{equation}
	\label{eq:optimal_action_value_function}
	a^*(s) = \arg \max_a Q^*(s,a)
\end{equation}
الگوریتم
\lr{DDPG}
ترکیبی از یادگیری تقریبی برای $ Q^*(s,a)$ و یادگیری تقریبی برای
 $a^*(s)$ 
 است و به نحوی طراحی شده‌است که برای محیط‌هایی با فضاهای عمل پیوسته مناسب باشد. 
% اما چه معنی دارد که DDPG به طور خاص برای محیط‌هایی با فضاهای عمل پیوسته مناسب است؟ 
% این به روش محاسبه بیشینه عمل‌ها در
%  $\max_a Q^*(s,a)$
%   مربوط است.
   روش محاسبه 
   $a^*(s)$
    در این الگوریتم
    آن را برای فضای پیوسته مناسب می‌کند.
    از آنجا که فضای عمل پیوسته است، فرض می‌شود که تابع
     $Q^*(s,a)$
      نسبت به آرگومان عمل مشتق‌پذیر است. مشتق‌پذیری این امکان را می‌دهد که یک روش یادگیری مبتنی بر گرادیان برای سیاست
       $\mu(s)$
        استفاده شود. سپس، به جای اجرای یک بهینه‌سازی زمان‌بر در هر بار محاسبه
        $\max_a Q(s,a)$،
       می‌توان آن را با رابطه
        $\max_a Q(s,a) \approx Q(s,\mu(s))$
         تقریب زد.
         
         
\subsection{بخش یادگیری
\lr{Q}
در \lr{DDPG}
}
معادله بلمن که تابع ارزش عمل بهینه
 $(Q^*(s,a))$
  را توصیف می‌کند، در پایین آورده شده‌است.
\begin{equation}
	Q^*(s,a) = \underset{s' \sim P}{{\mathrm E}}\left[r(s,a) + \gamma \max_{a'} Q^*(s', a')\right]
\end{equation}
جمله 
$s' \sim P$ 
به این معنی است که وضعیت بعدی $s'$ توسط محیط از توزیع احتمال
 $P(\cdot| s,a)$
 نمونه‌گرفته می‌شود.
 معادله بلمن نقطه شروع برای یادگیری
  $Q^*(s,a)$
  با یک مقداردهی تقریبی
   است. پارامترهای یک شبکه عصبی
    $Q_{\phi}(s,a)$
    با علامت
    $\phi$ 
    نشان داده شده‌است.
    یک مجموعه
     ${\mathcal D}$
      از  تغییر از یک حالت به حالت دیگر
       $(s,a,r,s',d)$
       (که \lr{d} نشان می‌دهد که آیا وضعیت $s'$ پایانی است یا خیر) جمع‌آوری شده‌است.
        یک تابع خطای میانگین مربعات بلمن (\lr{MSBE}) استفاده شده‌است که معیاری برای نزدیکی
         $Q_{\phi}$
          برای برآورده کردن معادله بلمن است.
          
          \begin{equation}
          	L(\phi, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
          	\Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a') \right) \Bigg)^2
          	\right]
          \end{equation}
          
          
          
          
          در الگوریتم \lr{DDPG} دو ترفند برای عمکرد بهتر استفاده شده‌است که در ادامه به بررسی آن پرداخته شده‌است.
          \begin{itemize}
          	\item بافرهای بازی
          	
          	 الگوریتم‌های یادگیری تقویتی جهت آموزش یک شبکه عصبی عمیق برای تقریب 
          	 $Q^*(s,a)$
          	  از  بافرهای بازی\LTRfootnote{Replay Buffers}
          	   تجربه‌شده استفاده می‌کنند. 
          	           این مجموعه 
          	   ${\mathcal D}$
          	   شامل تجربیات قبلی است. برای داشتن رفتار پایدار در الگوریتم، بافر بازی باید به اندازه کافی بزرگ باشد تا شامل یک دامنه گسترده از تجربیات شود. انتخاب داده‌های بافر به دقت انجام شده‌است چرا که اگر فقط از داده‌های بسیار جدید استفاده شود، بیش‌برازش\LTRfootnote{Overfit}
          	   رخ می‌دهید و اگر از تجربه بیش از حد استفاده شود، ممکن است فرآیند یادگیری کند شود.
          	\item شبکه‌های هدف
          	
          	الگوریتم‌های یادگیری \lr{Q} از شبکه‌های هدف استفاده می‌کنند. اصطلاح زیر به‌عنوان هدف شناخته می‌شود.
          	\begin{equation}
          		r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a')
          	\end{equation}
          	در هنگام کمینه کردن تابع خطای میانگین مربعات بلمن، سعی شده‌است تا تابع \lr{Q} شبیه‌تر به این هدف شود. اما مشکل این است که هدف بستگی به پارامترهای در حال آموزش $\phi$ دارد.
 این باعث ایجاد ناپایداری در کمینه کردن تابع خطای میانگین مربعات بلمن می‌شود. راه حل آن استفاده از یک مجموعه پارامترهایی که با تأخیر زمانی به 
          	$\phi$
          	 نزدیک می‌شوند. به عبارت دیگر، یک شبکه دوم  ایجاد می‌شود که به آن شبکه هدف گفته می‌شود. شبکه هدف دنباله‌ی نخست را دنبال می‌کند. پارامترهای شبکه هدف با نشان 
          	​$\phi_{\text{targ}}$
          	نشان داده می‌شوند.
          \end{itemize}
 
          
     


