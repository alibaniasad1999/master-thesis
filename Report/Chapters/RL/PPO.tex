\section{بهینه‌سازی سیاست مجاور}
الگوریتم 
بهینه‌سازی سیاست مجاور\LTRfootnote{Proximal Policy Optimization (PPO)}
 یک الگوریتم بهینه‌سازی سیاست مبتنی بر گرادیان است که برای حل مسائل کنترل مسئله‌های یادگیری تقویتی استفاده می‌شود. این الگوریتم از الگوریتم 
 \lr{TRPO}\LTRfootnote{Trust Region Policy Optimization}
 الهام گرفته شده است و با اعمال تغییراتی بر روی آن، سرعت و کارایی آن را افزایش داده است. در این بخش به بررسی این الگوریتم و نحوه عملکرد آن می‌پردازیم.
 الگوریتم \lr{PPO} همانند سایر الگوریتم‌های یادگیری تقویتی، به‌دنبال یافتن بهترین گام ممکن برای بهبود عملکرد سیایت با استفاده از داده‌های موجود است. این الگوریتم تلاش می‌کند تا از گام‌های بزرگ که می‌توانند منجر به افت ناگهانی عملکرد شوند، اجتناب کند.
 برخلاف روش‌های پیچیده‌تر مرتبه دوم مانند 
 \lr{TRPO}، \lr{PPO}
  از مجموعه‌ای از روش‌های مرتبه اول ساده‌تر برای حفظ نزدیکی سیاست‌های جدید به سیاست‌های قبلی استفاده می‌کند. این سادگی در پیاده‌سازی، \lr{PPO} را به روشی کارآمدتر تبدیل می‌کند، در حالی که از نظر تجربی نشان داده شده است که عملکردی حداقل به اندازه \lr{TRPO} دارد.
  از جمله ویژگی‌های مهم این الگوریتم می‌توان به سیاست محور بودن آن اشاره کرد.
  این الگوریتم برای عامل‌های یادگیری تقویتی که سیاست‌های پیوسته و گسسته دارند، مناسب است.


  الگوریتم
  \lr{PPO}
  داری دو گونه اصلی 
    \lr{PPO-Clip}
    و
    \lr{PPO-Penalty}
    است. در ادامه به بررسی هر یک از این دو گونه پرداخته شده است.
    \begin{itemize}
        \item
         روش\textbf{
         \lr{{PPO-Penalty}}:}
        روش
        \lr{PPO-Penalty}
         به‌دنبال حل تقریبی و به‌روز‌رسانی با 
        محدودیت واگرایی کولباک-لیبلر\LTRfootnote{Kullback-Leibler (KL) Divergence}
است، مشابه روشی که در الگوریتم \lr{TRPO} استفاده شده است.
 با این حال، به جای اعمال یک محدودیت سخت\LTRfootnote{Hard Constraint}،
  \lr{PPO-Penalty}
  واگرایی \lr{KL} را در تابع هدف جریمه می‌کند. این جریمه به طور خودکار در طول آموزش تنظیم می‌شود تا از افت ناگهانی عملکرد جلوگیری کند.
    \item
     روش\textbf{
    \lr{{PPO-Clip}}:}
    در این روش، هیچ عبارت واگرایی \lr{KL} در تابع هدف وجود ندارد و هیچ محدودیتی اعمال نمی‌شود. در عوض،
     \lr{{PPO-Clip}}
     از یک عملیات بریدن\LTRfootnote{Clipping} خاص در تابع هدف استفاده می‌کند تا انگیزه سیاست جدید برای دور شدن از سیاست قبلی را از بین ببرد.    
    \end{itemize}
    در این پژوهش از روش 
    \lr{PPO-Clip}
    برای آموزش عامل‌های یادگیری تقویتی استفاده شده است.

\subsection{
    سیاست در الگوریتم \lr{PPO}
}
تابع سیاست در الگوریتم \lr{PPO} به صورت یک شبکه عصبی پیچیده پیاده‌سازی شده است. این شبکه عصبی ورودی‌های محیط را دریافت کرده و اقدامی را که باید عامل انجام دهد را تولید می‌کند. این شبکه عصبی می‌تواند شامل چندین لایه پنهان با توابع فعال‌سازی مختلف باشد. در این پژوهش از یک شبکه عصبی با سه لایه پنهان و تابع فعال‌سازی 
\(\tanh\)
استفاده شده است.
تابع سیاست در الگوریتم \lr{PPO} به صورت زیر به‌روز‌رسانی می‌شود:
\begin{equation}
    \theta_{k+1} = \arg \max_{\theta} \underset{s,a \sim \pi_{\theta_k}}{{\mathrm E}}\left[
        L(s,a,\theta_k, \theta)\right]
\end{equation}
در این پژوهش برای به حداکثر رساندن تابع هدف، چندین گام بهینه‌سازی 
گرادیان کاهشی تصادفی\LTRfootnote{Stochastic Gradient Descent (SGD)}
اجرا شده است.
در معادله بالا
\(L\)
به‌صورت زیر تعریف شده است:
\begin{equation}
    L(s,a,\theta_k,\theta) = \min\left(
    \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
    \text{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1+\epsilon \right) A^{\pi_{\theta_k}}(s,a)
    \right)
\end{equation}
