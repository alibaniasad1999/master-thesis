\section{بهینه‌سازی سیاست مجاور}
الگوریتم 
بهینه‌سازی سیاست مجاور\LTRfootnote{Proximal Policy Optimization (PPO)}
 یک الگوریتم بهینه‌سازی سیاست مبتنی بر گرادیان است که برای حل مسائل کنترل مسئله‌های یادگیری تقویتی استفاده می‌شود. این الگوریتم از الگوریتم 
 \lr{TRPO}\LTRfootnote{Trust Region Policy Optimization}
 الهام گرفته شده است و با اعمال تغییراتی بر روی آن، سرعت و کارایی آن را افزایش داده است. در این بخش به بررسی این الگوریتم و نحوه عملکرد آن می‌پردازیم.
 الگوریتم \lr{PPO} همانند سایر الگوریتم‌های یادگیری تقویتی، به‌دنبال یافتن بهترین گام ممکن برای بهبود عملکرد سیایت با استفاده از داده‌های موجود است. این الگوریتم تلاش می‌کند تا از گام‌های بزرگ که می‌توانند منجر به افت ناگهانی عملکرد شوند، اجتناب کند.
 برخلاف روش‌های پیچیده‌تر مرتبه دوم مانند 
 \lr{TRPO}، \lr{PPO}
  از مجموعه‌ای از روش‌های مرتبه اول ساده‌تر برای حفظ نزدیکی سیاست‌های جدید به سیاست‌های قبلی استفاده می‌کند. این سادگی در پیاده‌سازی، \lr{PPO} را به روشی کارآمدتر تبدیل می‌کند، در حالی که از نظر تجربی نشان داده شده است که عملکردی حداقل به اندازه \lr{TRPO} دارد.
  از جمله ویژگی‌های مهم این الگوریتم می‌توان به سیاست محور بودن آن اشاره کرد.
  این الگوریتم برای عامل‌های یادگیری تقویتی که سیاست‌های پیوسته و گسسته دارند، مناسب است.


  الگوریتم
  \lr{PPO}
  داری دو گونه اصلی 
    \lr{PPO-Clip}
    و
    \lr{PPO-Penalty}
    است. در ادامه به بررسی هر یک از این دو گونه پرداخته شده است.
    \begin{itemize}
        \item
         روش\textbf{
         \lr{{PPO-Penalty}}:}
        روش
        \lr{PPO-Penalty}
         به‌دنبال حل تقریبی و به‌روز‌رسانی با 
        محدودیت واگرایی کولباک-لیبلر\LTRfootnote{Kullback-Leibler (KL) Divergence}
است، مشابه روشی که در الگوریتم \lr{TRPO} استفاده شده است.
 با این حال، به جای اعمال یک محدودیت سخت\LTRfootnote{Hard Constraint}،
  \lr{PPO-Penalty}
  واگرایی \lr{KL} را در تابع هدف جریمه می‌کند. این جریمه به طور خودکار در طول آموزش تنظیم می‌شود تا از افت ناگهانی عملکرد جلوگیری کند.
    \item
     روش\textbf{
    \lr{{PPO-Clip}}:}
    در این روش، هیچ عبارت واگرایی \lr{KL} در تابع هدف وجود ندارد و هیچ محدودیتی اعمال نمی‌شود. در عوض،
     \lr{{PPO-Clip}}
     از یک عملیات بریدن\LTRfootnote{Clipping} خاص در تابع هدف استفاده می‌کند تا انگیزه سیاست جدید برای دور شدن از سیاست قبلی را از بین ببرد.    
    \end{itemize}
    در این پژوهش از روش 
    \lr{PPO-Clip}
    برای آموزش عامل‌های یادگیری تقویتی استفاده شده است.

\subsection{
    سیاست در الگوریتم \lr{PPO}
}
تابع سیاست در الگوریتم \lr{PPO} به صورت یک شبکه عصبی پیچیده پیاده‌سازی شده است. این شبکه عصبی ورودی‌های محیط را دریافت کرده و اقدامی را که باید عامل انجام دهد را تولید می‌کند. این شبکه عصبی می‌تواند شامل چندین لایه پنهان با توابع فعال‌سازی مختلف باشد. در این پژوهش از یک شبکه عصبی با سه لایه پنهان و تابع فعال‌سازی 
\(\tanh\)
استفاده شده است.
تابع سیاست در الگوریتم \lr{PPO} به صورت زیر به‌روز‌رسانی می‌شود:
\begin{equation}
    \theta_{k+1} = \arg \max_{\theta} \underset{s,a \sim \pi_{\theta_k}}{{\mathrm E}}\left[
        L(s,a,\theta_k, \theta)\right]
\end{equation}
در این پژوهش برای به حداکثر رساندن تابع هدف، چندین گام بهینه‌سازی 
گرادیان کاهشی تصادفی\LTRfootnote{Stochastic Gradient Descent (SGD)}
اجرا شده است.
در معادله بالا
\(L\)
به‌صورت زیر تعریف شده است:
\begin{equation}
    L(s,a,\theta_k,\theta) = \min\left(
    \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
    \text{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1+\epsilon \right) A^{\pi_{\theta_k}}(s,a)
    \right)
\end{equation}
که در آن
\(\epsilon\)
یک فراپامتر است که مقدار آن معمولا کوچک است.
این فراپامتر مشخص می‌کند که چقدر اندازه گام بهینه‌سازی باید محدود شود.
در این پژوهش مقدار 
\(\epsilon = 0.2\)
انتخاب شده است.


در حالی که این نوع محدود کردن 
(\lr{PPO-Clip})
 تا حد زیادی به اطمینان از به‌روزرسانی‌های معقول سیاست کمک می‌کند، همچنان ممکن است با سیاست به‌دست آید که بیش از حد از سیاست قدیمی دور باشد. برای جلوگیری از این امر، پیاده‌سازی‌های مختلف \lr{PPO} از مجموعه‌ای از ترفندها استفاده می‌کنند. در پیاده‌سازی این پژوهش، از روشی ساده به نام
  \textbf{توقف زودهنگام}\LTRfootnote{Early Stopping}
   استفاده شده است. اگر میانگین واگرایی کولباک-لیبلر (\lr{KL}) خط‌مشی جدید از خط‌مشی قدیمی از یک آستانه فراتر رود، گام‌های گرادیان (بهینه‌سازی) را متوقف می‌شوند. 
   
   
        \subsection{اکتشاف و بهره‌برداری در 
   	\lr{PPO}
   }
الگوریتم \lr{PPO} از یک سیاست تصادفی به‌صورت سیاست محور برای آموزش استفاده می‌کند. این به این معنی است که اکتشاف محیط با نمونه‌گیری عمل‌ها بر اساس آخرین نسخه از این سیاست تصادفی انجام می‌شود. میزان تصادفی بودن انتخاب عمل به شرایط اولیه و فرآیند آموزش بستگی دارد.

در طول آموزش، سیاست به طور کلی به تدریج کمتر تصادفی می‌شود، زیرا قانون به‌روزرسانی آن را تشویق می‌کند تا از پاداش‌هایی که قبلاً پیدا کرده است، بهره‌برداری کند. البته این موضوع می‌تواند منجر به گیر افتادن خط‌مشی در بهینه‌های محلی\LTRfootnote{Local Optima}
 شود.
 
 
         \subsection{شبه‌کد
         \lr{PPO}}
 
 در این بخش الگوریتم
 \lr{PPO}
 پیاده‌سازی شده آورده شده است. در این پژوهش الگوریتم~\رجوع{alg:PPO} در محیط پایتون با استفاده از کتابخانه
  \lr{PyTorch} \cite{paszke2017automatic}
  پیاده‌سازی شده ‌است.
 



\vspace{1cm}
\begin{algorithm}[H]
\caption{
	بهینه‌سازی سیاست مجاور  (\lr{PPO-Clip})
	}\label{alg:PPO}
\begin{algorithmic}[1]
\ورودی 
پارامترهای اولیه سیاست
$(\theta_0)$، پارامترهای تابع ارزش
$(\phi_0)$

%        \خروجی یک پوشش رأسی از $G$
\For $k = 0,1,2,...$
	\State 
	مجموعه‌ای از مسیرها به نام
	 ${\mathcal D}_k = \{\tau_i\}$ 
		​	
	 با اجرای سیاست 
	 $\pi_k = \pi(\theta_k)$
	 در محیط جمع‌آوری شود.
	\State
	 پاداش‌های باقی‌مانده 
	 ($\hat{R}_t$)
	 محاسبه شود.
	\State
	برآوردهای مزیت را محاسبه کنید، $\hat{A}_t$ (با استفاده از هر روش تخمین مزیت) بر اساس تابع ارزش فعلی $V_{\phi_k}$
	
	\State
	سیاست را با به حداکثر رساندن تابع هدف \lr{PPO-Clip} به‌روزرسانی کنید:
	 \begin{equation*} \theta_{k+1} = \arg \max_{\theta} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \min\left( \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_k}(a_t|s_t)} A^{\pi_{\theta_k}}(s_t,a_t), \;\; g(\epsilon, A^{\pi_{\theta_k}}(s_t,a_t)) \right) \end{equation*}
	 معمولاً از طریق گرادیان افزایشی تصادفی \lr{Adam}.
	 \State
	 برازش تابع ارزش با رگرسیون بر روی میانگین مربعات خطا:
	 \begin{equation*} \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2 \end{equation*} 
	 معمولاً از طریق برخی از الگوریتم‌های کاهشی گرادیان.
\EndFor
\end{algorithmic}
\end{algorithm}














