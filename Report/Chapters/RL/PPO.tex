\section{بهینه‌سازی سیاست مجاور}
الگوریتم 
بهینه‌سازی سیاست مجاور\LTRfootnote{Proximal Policy Optimization (PPO)}
 یک الگوریتم بهینه‌سازی سیاست مبتنی بر گرادیان است که برای حل مسائل کنترل مسئله‌های یادگیری تقویتی استفاده می‌شود. این الگوریتم از الگوریتم 
 \lr{TRPO}\LTRfootnote{Trust Region Policy Optimization}
 الهام گرفته شده است و با اعمال تغییراتی بر روی آن، سرعت و کارایی آن را افزایش داده است. در این بخش به بررسی این الگوریتم و نحوه عملکرد آن می‌پردازیم.
 الگوریتم \lr{PPO} همانند سایر الگوریتم‌های یادگیری تقویتی، به‌دنبال یافتن بهترین گام ممکن برای بهبود عملکرد سیایت با استفاده از داده‌های موجود است. این الگوریتم تلاش می‌کند تا از گام‌های بزرگ که می‌توانند منجر به افت ناگهانی عملکرد شوند، اجتناب کند.
 برخلاف روش‌های پیچیده‌تر مرتبه دوم مانند 
 \lr{TRPO}، \lr{PPO}
  از مجموعه‌ای از روش‌های مرتبه اول ساده‌تر برای حفظ نزدیکی سیاست‌های جدید به سیاست‌های قبلی استفاده می‌کند. این سادگی در پیاده‌سازی، \lr{PPO} را به روشی کارآمدتر تبدیل می‌کند، در حالی که از نظر تجربی نشان داده شده است که عملکردی حداقل به اندازه \lr{TRPO} دارد.

  الگوریتم
  \lr{PPO}
  داری دو گونه اصلی 
    \lr{PPO-Clip}
    و
    \lr{PPO-Penalty}
    است. در ادامه به بررسی هر یک از این دو گونه پرداخته شده است.
    \begin{itemize}
        \item روش
         \lr{{PPO-Penalty}}:
        روش
        \lr{PPO-Penalty}
         به‌دنبال حل تقریبی و به‌روز‌رسانی با 
        محدودیت واگرایی کولباک-لیبلر
        \LTRfootnote{Kullback-Leibler (KL) Divergence}
است، مشابه روشی که در الگوریتم \lr{TRPO} استفاده شده است.
 با این حال، به جای اعمال یک محدودیت سخت\LTRfootnote{Hard Constraint}،
  \lr{PPO-Penalty}
  واگرایی \lr{KL} را در تابع هدف جریمه می‌کند. این جریمه به طور خودکار در طول آموزش تنظیم می‌شود تا از افت ناگهانی عملکرد جلوگیری کند.
    \item روش
    \lr{{PPO-Clip}}:
    در این روش، هیچ عبارت واگرایی \lr{KL} در تابع هدف وجود ندارد و هیچ محدودیتی اعمال نمی‌شود. در عوض،
     \lr{{PPO-Clip}}
     از یک عملیات بریدن\LTRfootnote{Clipping} خاص در تابع هدف استفاده می‌کند تا انگیزه سیاست جدید برای دور شدن از سیاست قبلی را از بین ببرد.    
    \end{itemize}
    در این پروژه از روش 
    \lr{PPO-Clip}
    برای آموزش عامل‌های یادگیری تقویتی استفاده شده است.
    