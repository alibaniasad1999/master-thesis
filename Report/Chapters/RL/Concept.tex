\section{مفاهیم اولیه}
دو بخش اصلی یادگیری تقویتی\LTRfootnote{Reinforcement Learning (RL)}
شامل عامل\LTRfootnote{Agent}
 و محیط\LTRfootnote{Environment}
  است. عامل در محیط قرار دارد و با آن تعامل دارد.
  در هر مرحله از تعامل بین عامل و محیط، عامل یک مشاهده جزئی از وضعیت محیط انجام می‌دهد و سپس در مورد اقدامی که باید انجام دهد تصمیم می‌گیرد. وقتی عامل بر روی محیط عمل می کند، محیط تغییر می‌کند، اما ممکن است محیط به تنهایی نیز تغییر کند.
  عامل همچنین یک سیگنال پاداش\LTRfootnote{Reward}
   از محیط دریافت می کند، سیگنالی که به آن می‌گوید وضعیت تعامل فعلی عامل محیط چقدر خوب یا بد است. هدف عامل به حداکثر رساندن پاداش انباشته خود است که بازگشت\LTRfootnote{Return}
    نام دارد. یادگیری تقویتی به روش‌هایی گفته می‌شود که در آن‌ها عامل رفتارهای مناسب برای رسیدن به هدف خود را می‌آموزد. در شکل
    \ref{fig:agent_env}
    تعامل بین محیط و عامل نشان داده شده‌است.
\begin{figure}[H]
	\begin{center}
\lr{		\begin{tikzpicture}[very thick,node distance = 4cm]
			\node [frame] (agent) {Agent};
			\node [frame, below=1.2cm of agent] (environment) {Environment};
			\draw[line] (agent) -- ++ (3.5,0) |- (environment) 
			node[right,pos=0.25,align=left] {action\\ $a_t$};
			\coordinate[left=15mm of environment] (P);
			\draw[thin,dashed] (P|-environment.north) -- (P|-environment.south);
			\draw[line] (environment.200) -- (P |- environment.200)
			node[midway,above]{$s_{t+1}$};
			\draw[line,thick] (environment.160) -- (P |- environment.160)
			node[midway,above]{$r_{t+1}$};
			\draw[line] (P |- environment.200) -- ++ (-1.6,0) |- (agent.160)
			node[left, pos=0.25, align=right] {state\\ $s_t$};
			\draw[line,thick] (P |- environment.160) -- ++ (-1,0) |- (agent.200)
			node[right,pos=0.25,align=left] {reward\\ $r_t$};
		\end{tikzpicture}}
	\end{center}
	\caption{حلقه تعامل عامل و محیط}
	\label{fig:agent_env}
\end{figure}
\subsection{حالت و مشاهدات}
حالت\LTRfootnote{State}
\((s)\)
 توصیف کاملی از وضعیت محیط است. همه‌ی اطلاعات محیط در حالت وجود دارد. مشاهده\LTRfootnote{Observation}
 \((o)\)
  یک توصیف جزئی از حالت است که ممکن است شامل تمامی اطلاعات نباشد. در این پژوهش مشاهده توصیف کاملی از محیط هست در نتیجه حالت و مشاهده برابر هستند.
\subsection{فضای عمل}
فضای عمل \((a)\) در یادگیری تقویتی، مجموعه‌ای از تمام اقداماتی است که یک عامل می‌تواند در محیط انجام دهد. این فضا می‌تواند گسسته\LTRfootnote{Discrete} یا پیوسته\LTRfootnote{Continuous} باشد. در این پژوهش فضای عمل پیوسته و محدود به یک بازه مشخص است.
\subsection{سیاست}
یک سیاست\LTRfootnote{Policy}
  قاعده‌ای است که یک عامل برای تصمیم‌گیری در مورد اقدامات خود استفاده می‌کند. در این پژوهش به تناسب الگوریتم پیاده‌سازی شده از سیاست قطعی\LTRfootnote{Deterministic}
 یا تصادفی\LTRfootnote{Stochastic}
  استفاده شده‌است، که به دو صورت زیر  نشان داده می‌شود:
  \begin{align}
  	a_t &= \mu(s_t)\\
  	a_t & \sim \pi(\cdot | s_t)
  \end{align}
  که زیروند \(t\)
   بیانگر زمان است.
  در یادگیری تقویتی عمیق از سیاست‌های پارامتری‌شده استفاده می‌شود. خروجی‌ این سیاست‌ها تابعی از مجموعه‌ای از پارامترها (برای مثال وزن‌ها و بایاس‌های یک شبکه عصبی) هستند که می‌توان از الگوریتم‌های بهینه‌سازی جهت تعیین پارامترها استفاده کرد.
  در این پژوهش پارامترهای سیاست با \( \phi\) نشان داده شده‌است و سپس نماد آن به عنوان زیروند  سیاست مانند معادله \eqref{eq:policy_par} نشان داده شده‌است.
  
\begin{align}
	 \begin{split} 	 
 	a_t &= \mu_{\phi}(s_t) \\
 		a_t & \sim \pi_{\phi}(\cdot | s_t)
 	 \end{split}
 	 	\label{eq:policy_par}
\end{align}
\subsection{مسیر}
یک مسیر\LTRfootnote{Trajectory}
 توالی‌ای از حالت‌ها و عمل‌ها در محیط است.
 \begin{equation}
		 \tau = (s_0, a_0, s_1, a_1, \cdots)
 \end{equation}
  گذار حالت\LTRfootnote{State Transition}
   به اتفاقاتی که در محیط بین 
   زمان \(t\)
   در حالت \(s_t\)
   و
      زمان
       \(t+1\)
   در حالت
    \(s_{t+1}\)
   رخ می‌دهد، گفته می‌شود. این گذارها توسط قوانین طبیعی محیط انجام می‌شوند و تنها به آخرین اقدام انجام شده توسط عامل \((a_t)\) بستگی دارند. گذار حالت را می‌توان به‌صورت زیر تعریف کرد.
   \begin{equation}
   	s_{t+1} = f(s_t, a_t)
   \end{equation}
  
  
\subsection{تابع پاداش و بازگشت}
تابع پاداش\LTRfootnote{Reward Function} به حالت فعلی محیط، آخرین عمل انجام شده و حالت بعدی محیط بستگی دارد. تابع پاداش را می‌توان به‌صورت زیر تعریف کرد.
\begin{equation}
	r_t = R(s_t, a_t, s_{t+1})
\end{equation}
در این پژوهش پاداش تنها تابعی از جفت حالت-عمل \((r_t = R(s_t, a_t))\) است.
هدف عامل این است که مجموع پاداش‌های به‌دست‌آمده در طول یک مسیر را به حداکثر برساند. در این پژوهش مجموع پاداش‌ها در طول یک مسیر را با نماد \(R(\tau)\) نشان داده‌ شده‌است و به آن تابع بازگشت\LTRfootnote{ًReturn}
 گفته می‌شود.
یکی از انواع بازگشت، بازگشت بدون تنزیل\LTRfootnote{Discount} با افق محدود\LTRfootnote{Finite-Horizon Undiscounted Return}
 است که مجموع پاداش‌های به‌دست‌آمده در یک بازه زمانی ثابت و از مسیر
 \(\tau\)
  است که در معادله 
  \eqref{eq:return_no_discount}
  نشان داده شده‌است.
 \begin{equation}
 	R(\tau) = \sum_{t = 0}^T r_t
 	\label{eq:return_no_discount}
 \end{equation}
نوع دیگری از بازگشت، بازگشت تنزیل‌شده با افق نامحدود\LTRfootnote{Infinite-Horizon Discounted Return}
 است که مجموع همه پاداش‌هایی است که تا به حال توسط عامل به‌دست آمده‌است. اما، فاصله زمانی تا دریافت پاداش باعث تنزیل ارزش آن می‌شود. این معادله بازگشت \eqref{eq:return_discount} شامل یک فاکتور تنزیل\LTRfootnote{Discount Factor}
   با نماد \(\gamma\) است که
%   \(\gamma\)
   عددی بین صفر و یک است.
    \begin{equation}
   	R(\tau) = \sum_{t = 0}^{\infty} \gamma^t r_t
   	\label{eq:return_discount}
   \end{equation}
   
   
   
 \subsection{ ارزش در یادگیری تقویتی}
   
   در یادگیری تقویتی، دانستن ارزش\LTRfootnote{Value}
    یک حالت یا جفت حالت-عمل ضروری است. منظور از ارزش، بازگشت مورد انتظار\LTRfootnote{Expected Return}
     است. یعنی اگر از آن حالت یا جفت حالت-عمل شروع شود و سپس برای همیشه طبق یک سیاست خاص عمل شود، به طور میانگین چه مقدار پاداش دریافت خواهد کرد. توابع ارزش تقریبا در تمام الگوریتم‌های یادگیری تقویتی به کار می‌روند.
   در اینجا به چهار تابع مهم اشاره شده‌است.
   \begin{enumerate}
   	
   	\item تابع ارزش تحت سیاست\LTRfootnote{On-Policy Value Function} 
   	  $(V^{\pi}(s))$:
   	  خروجی این تابع بازگشت مورد انتظار است در صورتی که از حالت $s$ شروع شود و همیشه طبق سیاست $\pi$ عمل شود و به‌صورت زیر بیان می‌شود:
   	   \begin{equation}
   	   	V^{\pi}(s) = \underset{\tau \sim \pi}{\mathbb{E}}\left[R(\tau)|s_0 = s\right]
   	   \end{equation}
   	   
   	\item تابع ارزش-عمل تحت سیاست\LTRfootnote{On-Policy Action-Value Function} 
   	$(Q^{\pi}(s, a))$:
   	خروجی این تابع بازگشت مورد انتظار است در صورتی که از حالت $s$ شروع شود، یک اقدام دلخواه $a$ (که ممکن است از سیاست $\pi$ نباشد) انجام شود و سپس برای همیشه طبق سیاست $\pi$ عمل شود و به‌صورت زیر بیان می‌شود:
		\begin{equation}
		Q^{\pi}(s, a) = \underset{\tau \sim \pi}{\mathbb{E}}\left[R(\tau)|s_0 = s, a_0 = a\right]
		\end{equation}

 	
   	\item تابع ارزش بهینه\LTRfootnote{Optimal Value Function}
   	  $(V^*(s))$: 
   	 خروجی این تابع بازگشت مورد انتظار است در صورتی که از حالت $s$ شروع شود و همیشه طبق سیاست بهینه در محیط عمل شود و به‌صورت زیر بیان می‌شود:

	 \begin{equation}
	 V^*(s) = \underset{\pi}{\mathrm{max}} (V^{\pi}(s))
	 \end{equation}
   	
   	\item تابع ارزش-عمل بهینه\LTRfootnote{Optimal Action-Value Function}
   	  $(Q^*(s, a))$:
   	خروجی این تابع بازگشت مورد انتظار است در صورتی که از حالت $s$ شروع شود، یک اقدام دلخواه $a$ انجام شود و سپس برای همیشه طبق سیاست بهینه در محیط عمل شود و به‌صورت زیر بیان می‌شود:
		\begin{equation}
		Q^*(s, a) = \underset{\pi}{\mathrm{max}} (Q^{\pi}(s, a))
		\end{equation}
   	
   \end{enumerate}
 
   
  
  
  
  
  
  
  
  