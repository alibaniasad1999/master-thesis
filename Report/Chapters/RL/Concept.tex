\section{مفاهیم اولیه}
بخش‌های اصلی یادگیری تقویتی\LTRfootnote{Reinforcement Learning (RL)}
شامل عامل\LTRfootnote{Agent}
 و محیط\LTRfootnote{Environment}
  است. عامل در محیط قرار دارد و با آن تعامل دارد.
  در هر مرحله از تعامل بین عامل و محیط، عامل یک مشاهده جزئی از وضعیت محیط انجام می‌دهد و سپس در مورد اقدامی که باید انجام دهد تصمیم می‌گیرد. وقتی عامل بر روی محیط عمل می کند، محیط تغییر می‌کند، اما ممکن است محیط به تنهایی نیز تغییر کند.
  عامل همچنین یک سیگنال پاداش\LTRfootnote{Reward}
   از محیط دریافت می کند، عددی که به آن می‌گوید وضعیت فعلی محیط چقدر خوب یا بد است. هدف عامل به حداکثر رساندن پاداش انباشته خود است که بازگشت\LTRfootnote{Return}
    نام دارد. یادگیری تقویتی روش‌هایی هستند که عامل رفتارهای مناسب برای رسیدن به هدف خود را می‌آموزد. در شکل
    \ref{fig:agent_env}
    تعامل بین محیط و عامل نشان داده شده است.
\begin{figure}[H]
	\begin{center}
\lr{		\begin{tikzpicture}[very thick,node distance = 4cm]
			\node [frame] (agent) {Agent};
			\node [frame, below=1.2cm of agent] (environment) {Environment};
			\draw[line] (environment) -- ++ (3.5,0) |- (agent) 
			node[right,pos=0.25,align=left] {action\\ $A_t$};
			\coordinate[left=10mm of environment] (P);
			\draw[thin,dashed] (P|-environment.north) -- (P|-environment.south);
			\draw[line] (environment.200) -- (P |- environment.200)
			node[midway,above]{$S_{i+1}$};
			\draw[line,thick] (environment.160) -- (P |- environment.160)
			node[midway,above]{$R_{i+1}$};
			\draw[line] (P |- environment.200) -- ++ (-1.4,0) |- (agent.160)
			node[left, pos=0.25, align=right] {state\\ $s_t$};
			\draw[line,thick] (P |- environment.160) -- ++ (-0.8,0) |- (agent.200)
			node[right,pos=0.25,align=left] {reward\\ $R_t$};
		\end{tikzpicture}}
	\end{center}
	\caption{حلقه تعامل عامل و محیط}
	\label{fig:agent_env}
\end{figure}
\subsection{حالت و مشاهدات}
حالت\LTRfootnote{State}
\((s)\)
 توصیف کاملی از وضعیت محیط است. همه‌ی اطلاعات محیط در حالت وجود دارد. مشاهده\LTRfootnote{Observation}
 \((o)\)
  یک توصیف جزئی از حالت است که ممکن است شامل تمامی اطلاعات نباشد.
\subsection{فضای عمل}
فضای عمل در یادگیری تقویتی، مجموعه‌ای از تمام اقداماتی است که یک عامل می‌تواند در محیط خود انجام دهد. این فضا می‌تواند گسسته \lr{(discrete)} یا پیوسته \lr{(continuous)} باشد. در این پژوهش فضای عمل پیوسته و در یک بازه مشخص است.
\subsection{سیاست}
یک سیاست\LTRfootnote{policy}
  قاعده‌ای است که یک عامل برای تصمیم‌گیری در مورد اقدامات خود استفاده می‌کند. در این پژوهش سیاست قطعی\LTRfootnote{deterministic}
  است، که به صورت زیر  نشان داده می‌شود:
  \begin{equation}
  	a_t = \pi(s_t)
  \end{equation}
  در یادگیری تقویتی عمیق از سیاست‌های پارامتری‌شده استفاده می‌شود. خروجی‌ این سیاست‌ها از توابعی هستند که به مجموعه‌ای از پارامترها (مثلاً وزن‌ها و بایاس‌های یک شبکه عصبی) بستگی دارند که می‌توان آنها را برای تغییر رفتار از طریق برخی الگوریتم‌های بهینه‌سازی تنظیم کرد.
  در این پژوهش پارامترهای سیاست را با \( \theta\) نشان داده شده است و سپس نماد آن به عنوان یک زیروند روی سیاست مانند معادله \eqref{eq:policy_par} نشان داده شده است.
 \begin{equation}
 	a_t = \pi_{\theta}(s_t)
 	\label{eq:policy_par}
 	 \end{equation}
 	 
\subsection{مسیر}
یک مسیر\LTRfootnote{Trajectory}
 توالی‌ای از حالت‌ها و عمل‌ها در محیط است.
 \begin{equation}
 	\tau = (s_0, a_0, s_1, a_1, \cdots)
 \end{equation}
  گذار حالت\LTRfootnote{state transition}
   به اتفاقاتی که در محیط بین حالت  در زمان
   \(s\)
   و حالت
   در زمان
   \(s+1\)
   می‌افتد، گفته می‌شود. این گذارها توسط قوانین طبیعی محیط انجام می‌شوند و تنها به آخرین اقدام انجام شده توسط عامل \((a_t)\) بستگی دارند. گذار حالت را می‌توان به‌صورت زیر تعریف کرد.
   \begin{equation}
   	s_{t+1} = f(s_t, a_t)
   \end{equation}
  
  
\subsection{تابع پاداش و بازگشت}
تابع پاداش\LTRfootnote{reward function} حالت فعلی محیط، آخرین عمل انجام شده و حالت بعدی محیط بستگی دارد. تابع پاداش را می‌توان به‌صورت زیر تعریف کرد.
\begin{equation}
	r_t = R(s_t, a_t, s_{t+1})
\end{equation}
در این پژوهش پاداش تنها تابعی از جفت حالت-عمل \((r_t = R(s_t, a_t))\) است.
هدف عامل این است که مجموع پاداش‌های به‌دست‌آمده در طول یک مسیر را به حداکثر برساند، اما این مفهوم می‌تواند چند معنی داشته باشد. در این پژوهش این موارد را با نماد \(R(\tau)\) نشان داده‌ شده است و به آن تابع بازگشت\LTRfootnote{ًReturn}
 گفته می‌شود.
یکی از انواع بازگشت، بازگشت بدون تنزیل با افق محدود\LTRfootnote{Finite-Horizon Undiscounted Return}
 است که مجموع پاداش‌های به‌دست‌آمده در یک بازه زمانی ثابت از مسیر به‌صورت زیر است.
 \begin{equation}
 	R(\tau) = \sum_{t = 0}^T r_t
 \end{equation}
نوع دیگری از بازگشت، بازگشت تنزیل‌شده با افق نامحدود\LTRfootnote{Infinite-Horizon Discounted Return}
 است که مجموع همه پاداش‌هایی است که تا به حال توسط عامل به دست آمده است، اما با در نظر گرفتن فاصله زمانی‌ای که تا دریافت آن پاداش وجود داشته، تنزیل\LTRfootnote{Discount}
  شده است. این فرمول پاداش شامل یک فاکتور تنزیل\LTRfootnote{Discount Factor}
   با نماد \(\gamma\) است.
    \begin{equation}
   	R(\tau) = \sum_{t = 0}^{\infty} \gamma^t r_t
   \end{equation}
   
   
   
 \subsection{ ارزش در یادگیری تقویتی}
   
   در یادگیری تقویتی، دانستن ارزش\LTRfootnote{Value}
    یک حالت یا جفت حالت-عمل ضروری است. منظور از ارزش، بازگشت مورد انتظار\LTRfootnote{Expected Return}
     است، یعنی اگر از آن حالت یا جفت حالت-عمل شروع شود و سپس برای همیشه طبق یک سیاست خاص عمل شود، به طور میانگین چه مقدار پاداش دریافت خواهد کرد. توابع ارزش به شکلی در تقریبا تمام الگوریتم‌های یادگیری تقویتی به کار می‌روند.
   در اینجا به چهار تابع مهم اشاره می‌کنیم.
   \begin{enumerate}
   	
   	\item تابع ارزش تحت سیاست\LTRfootnote{On-Policy Value Function} 
   	  $(V^{\pi}(s))$:
   	   این تابع، بازگشت مورد انتظار را در صورتی که از حالت $s$ شروع شود و همیشه طبق سیاست $\pi$ عمل شود، خروجی می‌دهد.
   	   \begin{equation}
   	   	V^{\pi}(s) = \underset{\tau \sim \pi}{\mathbb{E}}\left[R(\tau)|s_0 = s\right]
   	   \end{equation}
   	   
   	\item تابع ارزش-عمل تحت سیاست\LTRfootnote{On-Policy Action-Value Function} 
   	$(Q^{\pi}(s, a))$:
   	 این تابع، بازگشت مورد انتظار را در صورتی که از حالت $s$ شروع شود، یک اقدام دلخواه $a$ (که ممکن است از سیاست $\pi$ نباشد) انجام شود و سپس برای همیشه طبق سیاست $\pi$ عمل شود، خروجی می‌دهد.
		\begin{equation}
		Q^{\pi}(s, a) = \underset{\tau \sim \pi}{\mathbb{E}}\left[R(\tau)|s_0 = s, a_0 = a\right]
		\end{equation}

 	
   	\item تابع ارزش بهینه\LTRfootnote{Optimal Value Function}
   	  $(V^*(s))$: 
   	 این تابع، بازگشت مورد انتظار را در صورتی که از حالت $s$ شروع شود و همیشه طبق سیاست بهینه در محیط عمل شود، خروجی می‌دهد.

	 \begin{equation}
	 V^*(s) = \underset{\pi}{\mathrm{max}} (V^{\pi}(s))
	 \end{equation}
   	
   	\item تابع ارزش-عمل بهینه\LTRfootnote{Optimal Action-Value Function}
   	  $(Q^*(s, a))$:
   	 این تابع، بازگشت مورد انتظار را در صورتی که از حالت $s$ شروع شود، یک اقدام دلخواه $a$ انجام شود و سپس برای همیشه طبق سیاست بهینه در محیط عمل شود، خروجی می‌دهد.
		\begin{equation}
		Q^*(s, a) = \underset{\pi}{\mathrm{max}} (Q^{\pi}(s, a))
		\end{equation}
   	
   \end{enumerate}
 
   
  
  
  
  
  
  
  
  