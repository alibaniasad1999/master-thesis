\section{مفاهیم اولیه}
دو بخش اصلی یادگیری تقویتی\LTRfootnote{Reinforcement Learning (RL)}
شامل عامل\LTRfootnote{Agent}
 و محیط\LTRfootnote{Environment}
  است. عامل در محیط قرار دارد و با آن تعامل دارد.
  در هر مرحله از تعامل بین عامل و محیط، عامل یک مشاهده جزئی از وضعیت محیط انجام می‌دهد و سپس در مورد اقدامی که باید انجام دهد تصمیم می‌گیرد. وقتی عامل بر روی محیط عمل می کند، محیط تغییر می‌کند، اما ممکن است محیط به تنهایی نیز تغییر کند.
  عامل همچنین یک سیگنال پاداش\LTRfootnote{Reward}
   از محیط دریافت می کند، سیگنالی که به آن می‌گوید وضعیت تعامل فعلی عامل محیط چقدر خوب یا بد است. هدف عامل به حداکثر رساندن پاداش انباشته خود است که بازگشت\LTRfootnote{Return}
    نام دارد. یادگیری تقویتی به روش‌هایی گفته می‌شود که در آن‌ها عامل رفتارهای مناسب برای رسیدن به هدف خود را می‌آموزد. در شکل
    \ref{fig:agent_env}
    تعامل بین محیط و عامل نشان داده شده‌است.
\begin{figure}[H]
	\begin{center}
\lr{		\begin{tikzpicture}[very thick,node distance = 4cm]
			\node [frame] (agent) {Agent};
			\node [frame, below=1.2cm of agent] (environment) {Environment};
			\draw[line] (agent) -- ++ (3.5,0) |- (environment) 
			node[right,pos=0.25,align=left] {action\\ $a_t$};
			\coordinate[left=15mm of environment] (P);
			\draw[thin,dashed] (P|-environment.north) -- (P|-environment.south);
			\draw[line] (environment.200) -- (P |- environment.200)
			node[midway,above]{$s_{t+1}$};
			\draw[line,thick] (environment.160) -- (P |- environment.160)
			node[midway,above]{$r_{t+1}$};
			\draw[line] (P |- environment.200) -- ++ (-1.6,0) |- (agent.160)
			node[left, pos=0.25, align=right] {state\\ $s_t$};
			\draw[line,thick] (P |- environment.160) -- ++ (-1,0) |- (agent.200)
			node[right,pos=0.25,align=left] {reward\\ $r_t$};
		\end{tikzpicture}}
	\end{center}
	\caption{حلقه تعامل عامل و محیط}
	\label{fig:agent_env}
\end{figure}
\subsection{حالت و مشاهدات}
حالت\LTRfootnote{State}
\((s)\)
 توصیف کاملی از وضعیت محیط است. همه‌ی اطلاعات محیط در حالت وجود دارد. مشاهده\LTRfootnote{Observation}
 \((o)\)
  یک توصیف جزئی از حالت است که ممکن است شامل تمامی اطلاعات نباشد. در این پژوهش مشاهده توصیف کاملی از محیط هست در نتیجه حالت و مشاهده برابر هستند.
\subsection{فضای عمل}
فضای عمل \((a)\) در یادگیری تقویتی، مجموعه‌ای از تمام اقداماتی است که یک عامل می‌تواند در محیط انجام دهد. این فضا می‌تواند گسسته\LTRfootnote{Discrete} یا پیوسته\LTRfootnote{Continuous} باشد. در این پژوهش فضای عمل پیوسته و محدود به یک بازه مشخص است.
\subsection{سیاست}
یک سیاست\LTRfootnote{Policy}
  قاعده‌ای است که یک عامل برای تصمیم‌گیری در مورد اقدامات خود استفاده می‌کند. در این پژوهش به تناسب الگوریتم پیاده‌سازی شده از سیاست قطعی\LTRfootnote{Deterministic}
 یا تصادفی\LTRfootnote{Stochastic}
  استفاده شده‌است، که به دو صورت زیر  نشان داده می‌شود:
  \begin{align}
  	a_t &= \mu(s_t)\\
  	a_t & \sim \pi(\cdot | s_t)
  \end{align}
  که زیروند \(t\)
   بیانگر زمان است.
  در یادگیری تقویتی عمیق از سیاست‌های پارامتری‌شده استفاده می‌شود. خروجی‌ این سیاست‌ها تابعی از مجموعه‌ای از پارامترها (برای مثال وزن‌ها و بایاس‌های یک شبکه عصبی) هستند که می‌توان از الگوریتم‌های بهینه‌سازی جهت تعیین پارامترها استفاده کرد.
  در این پژوهش پارامترهای سیاست با \( \theta\) نشان داده شده‌است و سپس نماد آن به عنوان زیروند  سیاست مانند معادله \eqref{eq:policy_par} نشان داده شده‌است.
  
\begin{align}
	 \begin{split} 	 
 	a_t &= \mu_{\theta}(s_t) \\
 		a_t & \sim \pi_{\theta}(\cdot | s_t)
 	 \end{split}
 	 	\label{eq:policy_par}
\end{align}
\subsection{مسیر}
یک مسیر\LTRfootnote{Trajectory}
 توالی‌ای از حالت‌ها و عمل‌ها در محیط است.
 \begin{equation}
		 \tau = (s_0, a_0, s_1, a_1, \cdots)
 \end{equation}
  گذار حالت\LTRfootnote{State Transition}
   به اتفاقاتی که در محیط بین 
   زمان \(t\)
   در حالت \(s_t\)
   و
      زمان
       \(t+1\)
   در حالت
    \(s_{t+1}\)
   رخ می‌دهد، گفته می‌شود. این گذارها توسط قوانین طبیعی محیط انجام می‌شوند و تنها به آخرین اقدام انجام شده توسط عامل \((a_t)\) بستگی دارند. گذار حالت را می‌توان به‌صورت زیر تعریف کرد.
   \begin{equation}
   	s_{t+1} = f(s_t, a_t)
   \end{equation}
  
  
\subsection{تابع پاداش و بازگشت}
تابع پاداش\LTRfootnote{Reward Function} به حالت فعلی محیط، آخرین عمل انجام شده و حالت بعدی محیط بستگی دارد. تابع پاداش را می‌توان به‌صورت زیر تعریف کرد.
\begin{equation}
	r_t = R(s_t, a_t, s_{t+1})
\end{equation}
در این پژوهش پاداش تنها تابعی از جفت حالت-عمل \((r_t = R(s_t, a_t))\) است.
هدف عامل این است که مجموع پاداش‌های به‌دست‌آمده در طول یک مسیر را به حداکثر برساند. در این پژوهش مجموع پاداش‌ها در طول یک مسیر را با نماد \(R(\tau)\) نشان داده‌ شده‌است و به آن تابع بازگشت\LTRfootnote{ًReturn}
 گفته می‌شود.
یکی از انواع بازگشت، بازگشت بدون تنزیل\LTRfootnote{Discount} با افق محدود\LTRfootnote{Finite-Horizon Undiscounted Return}
 است که مجموع پاداش‌های به‌دست‌آمده در یک بازه زمانی ثابت و از مسیر
 \(\tau\)
  است که در معادله 
  \eqref{eq:return_no_discount}
  نشان داده شده‌است.
 \begin{equation}
 	R(\tau) = \sum_{t = 0}^T r_t
 	\label{eq:return_no_discount}
 \end{equation}
نوع دیگری از بازگشت، بازگشت تنزیل‌شده با افق نامحدود\LTRfootnote{Infinite-Horizon Discounted Return}
 است که مجموع همه پاداش‌هایی است که تا به حال توسط عامل به‌دست آمده‌است. اما، فاصله زمانی تا دریافت پاداش باعث تنزیل ارزش آن می‌شود. این معادله بازگشت \eqref{eq:return_discount} شامل یک فاکتور تنزیل\LTRfootnote{Discount Factor}
   با نماد \(\gamma\) است که
%   \(\gamma\)
   عددی بین صفر و یک است.
    \begin{equation}
   	R(\tau) = \sum_{t = 0}^{\infty} \gamma^t r_t
   	\label{eq:return_discount}
   \end{equation}
   
   
   
 \subsection{ ارزش در یادگیری تقویتی}
   
   در یادگیری تقویتی، دانستن ارزش\LTRfootnote{Value}
    یک حالت یا جفت حالت-عمل ضروری است. منظور از ارزش، بازگشت مورد انتظار\LTRfootnote{Expected Return}
     است. یعنی اگر از آن حالت یا جفت حالت-عمل شروع شود و سپس برای همیشه طبق یک سیاست خاص عمل شود، به طور میانگین چه مقدار پاداش دریافت خواهد کرد. توابع ارزش تقریبا در تمام الگوریتم‌های یادگیری تقویتی به کار می‌روند.
   در اینجا به چهار تابع مهم اشاره شده‌است.
   \begin{enumerate}
   	
   	\item تابع ارزش تحت سیاست\LTRfootnote{On-Policy Value Function} 
   	  $(V^{\pi}(s))$:
   	  خروجی این تابع بازگشت مورد انتظار است در صورتی که از حالت $s$ شروع شود و همیشه طبق سیاست $\pi$ عمل شود و به‌صورت زیر بیان می‌شود:
   	   \begin{equation}
   	   	V^{\pi}(s) = \underset{\tau \sim \pi}{\mathbb{E}}\left[R(\tau)|s_0 = s\right]
   	   \end{equation}
   	   
   	\item تابع ارزش-عمل تحت سیاست\LTRfootnote{On-Policy Action-Value Function} 
   	$(Q^{\pi}(s, a))$:
   	خروجی این تابع بازگشت مورد انتظار است در صورتی که از حالت $s$ شروع شود، یک اقدام دلخواه $a$ (که ممکن است از سیاست $\pi$ نباشد) انجام شود و سپس برای همیشه طبق سیاست $\pi$ عمل شود و به‌صورت زیر بیان می‌شود:
		\begin{equation}
		Q^{\pi}(s, a) = \underset{\tau \sim \pi}{\mathbb{E}}\left[R(\tau)|s_0 = s, a_0 = a\right]
		\end{equation}

 	
   	\item تابع ارزش بهینه\LTRfootnote{Optimal Value Function}
   	  $(V^*(s))$: 
   	 خروجی این تابع بازگشت مورد انتظار است در صورتی که از حالت $s$ شروع شود و همیشه طبق سیاست بهینه در محیط عمل شود و به‌صورت زیر بیان می‌شود:

	 \begin{equation}
	 V^*(s) = \underset{\pi}{\mathrm{max}} (V^{\pi}(s))
	 \end{equation}
   	
   	\item تابع ارزش-عمل بهینه\LTRfootnote{Optimal Action-Value Function}
   	  $(Q^*(s, a))$:
   	خروجی این تابع بازگشت مورد انتظار است در صورتی که از حالت $s$ شروع شود، یک اقدام دلخواه $a$ انجام شود و سپس برای همیشه طبق سیاست بهینه در محیط عمل شود و به‌صورت زیر بیان می‌شود:
		\begin{equation}
		Q^*(s, a) = \underset{\pi}{\mathrm{max}} (Q^{\pi}(s, a))
		\end{equation}
   	
   \end{enumerate}
   
  \subsection{ معادلات بلمن}
   
   توابع ارزش اشاره شده از معادلات خاصی که به آن‌ها معادلات بلمن گفته می‌شود، پیروی می‌کنند. ایده اصلی پشت معادلات بلمن اسن است که
   ارزش نقطه شروع برابر است با پاداشی است که انتظار دارید از آنجا دریافت کنید، به علاوه ارزش مکانی که بعداً به آنجا می‌رسید.
   معادلات بلمن برای توابع ارزش سیاست محور به شرح زیر هستند:
  
  \begin{align*}
  	V^{\pi}(s) &=  \underset{\substack{a \sim \pi \\ s'\sim P}}{\mathrm{E}} 
  	\left[
  	{r(s,a) + \gamma V^{\pi}(s')}
  	\right] \\
  	  	Q^{\pi}(s,a) &= \underE{s'\sim P}
  	  	\left[
  	  	{r(s,a) + \gamma \underE{a'\sim \pi}\left[
  	  		{Q^{\pi}(s',a')}
  	  		\right]
  	  	}
  	  	\right]
  \end{align*}
  
  که در آن: \( V^\pi(s) \) تابع ارزش حالت \( s \) تحت سیاست \( \pi \) است؛ \( Q^\pi(s,a) \) تابع ارزش عمل \( a \) در حالت \( s \) تحت سیاست \( \pi \) است؛ \( R(s,a) \) پاداش دریافتی پس از انجام عمل \( a \) در حالت \( s \) است؛ \( \gamma \) ضریب تخفیف است که ارزش پاداش‌های آینده را کاهش می‌دهد؛ \( s' \sim P(\cdot|s,a) \) نشان می‌دهد که حالت بعدی \( s' \) از توزیع انتقال محیط \( P \) با شرط‌های \( s \) و \( a \) نمونه‌برداری می‌شود؛ و \( a' \sim \pi(\cdot|s') \) نشان می‌دهد که عمل بعدی \( a' \) از سیاست \( \pi \) با شرط حالت جدید \( s' \) نمونه‌برداری می‌شود.
  این معادلات بیانگر این هستند که ارزش یک حالت یا عمل، مجموع پاداش مورد انتظار آن و ارزش حالت بعدی است که بر اساس سیاست فعلی تعیین می‌شود.
  معادلات بلمن برای توابع ارزش بهینه به شرح زیر هستند:
\begin{align*}
	V^*(s) &= \max_a \underE{s'\sim P}\left[
	{r(s,a) + \gamma V^*(s')}\right] \\
	Q^*(s,a) &= \underE{s'\sim P}\left[
	{r(s,a) + \gamma \max_{a'} Q^*(s',a')}\right]
\end{align*}
  
  تفاوت حیاتی بین معادلات بلمن برای توابع ارزش سیاست محور و توابع ارزش بهینه، عدم حضور یا حضور عملگر \(\max\) بر روی اعمال است. حضور آن منعکس‌کننده این است که هرگاه عامل بتواند عمل خود را انتخاب کند، برای عمل بهینه، باید هر عملی را که منجر به بالاترین ارزش می‌شود انتخاب کند.
  
  
  \subsection{تابع مزیت}
  
  گاهی در یادگیری تقویتی، نیازی به توصیف میزان خوبی یک عمل به صورت مطلق نیست، بلکه تنها می‌خواهیم بدانیم که چه مقدار بهتر از سایر اعمال به طور متوسط است. به عبارت دیگر، مزیت نسبی آن عمل مورد بررسی قرار می‌گیرد. این مفهوم با تابع مزیت\LTRfootnote{Advantage Function} توضیح داده می‌شود.
  
  تابع مزیت \( A^{\pi}(s,a) \) که مربوط به سیاست \( \pi \) است، توصیف می‌کند که انجام یک عمل خاص \( a \) در حالت \( s \) چقدر بهتر از انتخاب تصادفی یک عمل بر اساس \( \pi(\cdot|s) \) است، با فرض اینکه شما برای همیشه پس از آن مطابق با \( \pi \) عمل می‌کنید. به صورت ریاضی، تابع مزیت به صورت زیر تعریف می‌شود:
  \[
  A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)
  \]
که در آن \( A^{\pi}(s,a) \) تابع مزیت برای عمل \( a \) در حالت \( s \) است. \( Q^{\pi}(s,a) \) تابع ارزش عمل \( a \) در حالت \( s \) تحت سیاست \( \pi \) است و \( V^{\pi}(s) \) تابع ارزش حالت \( s \) تحت سیاست \( \pi \) است.
  این تابع مزیت نشان می‌دهد که انجام عمل \( a \) در حالت \( s \) نسبت به میانگین اعمال تحت سیاست \( \pi \) چقدر مزیت دارد. اگر \( A^{\pi}(s,a) \) مثبت باشد، نشان‌دهنده این است که عمل \( a \) بهتر از میانگین اعمال است و اگر منفی باشد، نشان‌دهنده کمتر بودن عملکرد آن نسبت به میانگین است.
  
 
   
  
  
  
  
  
  
  
  