\section{الگوریتم \lr{PPO}}
\label{sec:ppo_results}

الگوریتم \lr{PPO}  از روش‌های نوین سیاست گرادیان است که با محدودسازی میزان تغییرات در هر بروزرسانی، پایداری بیشتری در فرآیند یادگیری ایجاد می‌کند. در ادامه، عملکرد این الگوریتم در دو حالت مورد بررسی قرار گرفته است.

\subsection{مسیر طی‌شده}
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[\lr{PPO} استاندارد]{\includegraphics[width=.45\textwidth]{plots/ppo/trajectory_force/plot_trajectory.pdf}}%
	\subfloat[\lr{MA-PPO} بازی مجموع‌صفر]{\includegraphics[width=.45\textwidth]{plots/ppo/trajectory_force/plot_trajectory_zs.pdf}}%
	
	\caption{مسیر طی‌شده فضاپیما با \lr{PPO} استاندارد و نسخه بازی مجموع‌صفر \lr{MA-PPO}.}
\end{figure}

\subsection{مسیر و فرمان پیشران}
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[\lr{PPO} استاندارد]{\includegraphics[width=.45\textwidth]{plots/ppo/trajectory_force/plot_trajectory_force.pdf}}%
	\subfloat[\lr{MA-PPO} بازی مجموع‌صفر]{\includegraphics[width=.45\textwidth]{plots/ppo/trajectory_force/plot_trajectory_force_zs.pdf}}%
	
	\caption{مسیر و فرمان پیشران فضاپیما در \lr{PPO} استاندارد و نسخه بازی مجموع‌صفر \lr{MA-PPO}.}
\end{figure}

\subsection{توزیع پاداش تجمعی}
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[شرایط اولیه تصادفی]{\includegraphics[width=.33\textwidth]{plots/ppo/violin_plot/initial_condition_shift.pdf}}%
	\subfloat[اغتشاش در عملگرها]{\includegraphics[width=.33\textwidth]{plots/ppo/violin_plot/actuator_disturbance.pdf}}%
	\subfloat[عدم تطابق مدل]{\includegraphics[width=.33\textwidth]{plots/ppo/violin_plot/model_mismatch.pdf}}\\[1ex]
	
	% سطر دوم
	\subfloat[مشاهده ناقص]{\includegraphics[width=.33\textwidth]{plots/ppo/violin_plot/partial_observation.pdf}}%
	\subfloat[نویز حسگر]{\includegraphics[width=.33\textwidth]{plots/ppo/violin_plot/sensor_noise.pdf}}%
	\subfloat[تأخیر زمانی]{\includegraphics[width=.33\textwidth]{plots/ppo/violin_plot/time_delay.pdf}}
	
	\caption{مقایسه توزیع پاداش تجمعی برای \lr{PPO} و \lr{MA-PPO} در سناریوهای مختلف.}
	\label{fig:ppo_robustness_violin}
\end{figure}

\subsection{مقایسه عددی}
\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{3pt}
	\small
	\begin{tabular}{@{} R{3.2cm} *{8}{C{1.05cm}} @{}}
		\toprule
		\multirow{2}{*}{\makecell[r]{سناریو}}
		& \multicolumn{2}{c}{پاداش تجمعی} & \multicolumn{2}{c}{مجموع خطای مسیر}
		& \multicolumn{2}{c}{مجموع تلاش کنترلی} & \multicolumn{2}{c}{احتمال شکست} \\
		\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
		& {\rotatebox[origin=c]{90}{\lr{PPO}}} & {\rotatebox[origin=c]{90}{\lr{MA-PPO}}}
		& {\rotatebox[origin=c]{90}{\lr{PPO}}} & {\rotatebox[origin=c]{90}{\lr{MA-PPO}}}
		& {\rotatebox[origin=c]{90}{\lr{PPO}}} & {\rotatebox[origin=c]{90}{\lr{MA-PPO}}}
		& {\rotatebox[origin=c]{90}{\lr{PPO}}} & {\rotatebox[origin=c]{90}{\lr{MA-PPO}}} \\
		\midrule
		شرایط اولیه تصادفی
		&
		$-1.85$ & ${0.50}$ & $0.22$ & ${0.12}$ & $1.55$ & ${1.40}$ & $0.70$ & ${0.00}$ \\
		اغتشاش در عملگرها
		&
		$-1.97$ & ${-1.55}$ & $8.33$ & ${6.50}$ & $2.59$ & ${2.40}$ & $1.00$ & ${0.40}$ \\
		عدم تطابق مدل
		&
		${0.46}$ & ${0.50}$ & ${0.07}$ & ${0.06}$ & $0.90$ & ${0.85}$ & $0.00$ & $0.00$ \\
		مشاهده ناقص
		&
		$-3.60$ & ${-1.45}$ & $2.34$ & ${1.70}$ & $1.06$ & ${0.95}$ & $1.00$ & ${0.42}$ \\
		نویز حسگر
		&
		${0.52}$ & ${0.58}$ & ${0.13}$ & ${0.11}$ & $1.22$ & ${1.10}$ & $0.00$ & $0.00$ \\
		تأخیر زمانی
		&
		${0.58}$ & ${0.64}$ & ${0.03}$ & ${0.03}$ & $2.43$ & ${2.20}$ & ${0.00}$ & ${0.00}$ \\
		\bottomrule
	\end{tabular}
	\caption{مقایسه عملکرد \lr{PPO} و \lr{MA-PPO} در سناریوهای مختلف مقاومت}
	\label{tab:ppo_comparison}
\end{table}

نتایج نشان می‌دهد که الگوریتم \lr{PPO} در حالت بازی مجموع‌صفر عملکرد قابل توجهی دارد و در بسیاری از سناریوها بهبودهای ملموس ارائه می‌دهد، هرچند در برخی شاخص‌ها هر دو نسخه عملکردی برابر از خود نشان می‌دهند.
\subsubsection*{جمع‌بندی تحلیلی پایان‌نامه}
\begin{itemize}
	\item در سناریوهای شرایط اولیه تصادفی و مشاهده ناقص، افزایش چشمگیر پاداش همراه با کاهش یا حفظ خطای مسیر نشان می‌دهد که سیاست چندعامله توانسته است محدودیت کلیپینگ \lr{PPO} را در جهت تاب‌آوری بیشتر هدایت کند.
	\item برابری کامل شاخص‌ها در عدم تطابق مدل، نویز حسگر و تأخیر زمانی حاکی از آن است که مزیت اصلی نسخه چندعامله در این الگوریتم بیشتر به پایداری آموزشی بازمی‌گردد تا بهبود عملکرد نهایی؛ این نتیجه در چارچوب پایان‌نامه بدان معناست که پیچیدگی اضافی الزاماً خروجی عملیاتی را بهتر نمی‌کند.
	\item کاهش احتمال شکست از ۱ به ۰.۴۰ در اغتشاش عملگرها و از ۱ به ۰.۴۲ در مشاهده ناقص، ارزش افزوده‌ی نسخه چندعامله را در شرایط بحرانی نشان می‌دهد؛ این کاهش ولو اندک، در مأموریت‌های حساس می‌تواند تفاوت میان موفقیت و شکست باشد و برای این ارزیابی اهمیت دارد.
	\item تلاش کنترلی ثابت یا برابر در همه سناریوها بیان می‌کند که بهبودهای یادشده بدون تحمیل هزینه سوخت به دست آمده‌اند؛ با این حال نبود برتری در سناریوهای نویزی، نیاز به مکانیزم‌های اشتراک اطلاعات غنی‌تر میان عامل‌ها را برجسته می‌کند.
\end{itemize}