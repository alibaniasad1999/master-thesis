\section{الگوریتم \lr{TD3}}
\label{sec:td3_results}

الگوریتم \lr{TD3} (یادگیری تفاضل زمانی سه‌گانه عمیق) نسخه بهبودیافته \lr{DDPG} است که با استفاده از تکنیک‌های جدید مانند شبکه‌های دوگانه منتقد و تأخیر در بروزرسانی سیاست، مشکلات تخمین بیش از حد را کاهش می‌دهد.

\subsection{مسیر طی‌شده}
این بخش مسیر طی‌شده فضاپیما را برای نسخه استاندارد و نسخه بازی مجموع‌صفر \lr{TD3} نشان می‌دهد.
\begin{figure}[H]
	\centering
	\subfloat[\lr{TD3} استاندارد]{\includegraphics[width=.45\textwidth]{plots/td3/trajectory_force/plot_trajectory.pdf}}%
	\subfloat[\lr{MA-TD3} بازی مجموع‌صفر]{\includegraphics[width=.45\textwidth]{plots/td3/trajectory_force/plot_trajectory_zs.pdf}}%
	\caption{مسیر طی‌شده فضاپیما با \lr{TD3} استاندارد و نسخه بازی مجموع‌صفر \lr{MA-TD3}.}
\end{figure}

\subsection{مسیر و فرمان پیشران}
این بخش مسیر و پروفایل فرمان پیشران در طول زمان را برای هر دو نسخه \lr{TD3} ارائه می‌کند.
\begin{figure}[H]
	\centering
	\subfloat[\lr{TD3} استاندارد]{\includegraphics[width=.45\textwidth]{plots/td3/trajectory_force/plot_trajectory_force.pdf}}%
	\subfloat[\lr{MA-TD3} بازی مجموع‌صفر]{\includegraphics[width=.45\textwidth]{plots/td3/trajectory_force/plot_trajectory_force_zs.pdf}}%
	\caption{مسیر و فرمان پیشران فضاپیما در \lr{TD3} استاندارد و نسخه بازی مجموع‌صفر \lr{MA-TD3}.}
\end{figure}

\subsection{توزیع پاداش تجمعی}
این بخش نمودارهای ویولن توزیع پاداش تجمعی را در سناریوهای مختلف برای \lr{TD3} و \lr{MA-TD3} نمایش می‌دهد.
\begin{figure}[H]
	\centering
	% سطر اول
	\subfloat[شرایط اولیه تصادفی]{\includegraphics[width=.33\textwidth]{plots/td3/violin_plot/initial_condition_shift.pdf}}%
	\subfloat[اغتشاش در عملگرها]{\includegraphics[width=.33\textwidth]{plots/td3/violin_plot/actuator_disturbance.pdf}}%
	\subfloat[عدم تطابق مدل]{\includegraphics[width=.33\textwidth]{plots/td3/violin_plot/model_mismatch.pdf}}\\[1ex]
	% سطر دوم
	\subfloat[مشاهده ناقص]{\includegraphics[width=.33\textwidth]{plots/td3/violin_plot/partial_observation.pdf}}%
	\subfloat[نویز حسگر]{\includegraphics[width=.33\textwidth]{plots/td3/violin_plot/sensor_noise.pdf}}%
	\subfloat[تأخیر زمانی]{\includegraphics[width=.33\textwidth]{plots/td3/violin_plot/time_delay.pdf}}
	\caption{مقایسه توزیع پاداش تجمعی در سناریوهای مختلف برای \lr{TD3} و \lr{MA-TD3}.}
	\label{fig:td3_robustness_violin}
\end{figure}

\subsection{مقایسه عددی}
این بخش شاخص‌های عددی را گزارش می‌کند؛ نتایج بر اساس 100 اجرای مستقل شبیه‌سازی برای هر سناریو به‌دست آمده‌اند.
\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{3pt}
	\small
	\begin{tabular}{@{} R{3.2cm} *{8}{C{1.05cm}} @{}}
		\toprule
		\multirow{2}{*}{\makecell[r]{سناریو}}
		& \multicolumn{2}{c}{پاداش تجمعی} & \multicolumn{2}{c}{مجموع خطای مسیر}
		& \multicolumn{2}{c}{مجموع تلاش کنترلی} & \multicolumn{2}{c}{احتمال شکست} \\
		\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
		& {\rotatebox[origin=c]{90}{\lr{TD3}}} & {\rotatebox[origin=c]{90}{\lr{MA-TD3}}}
		& {\rotatebox[origin=c]{90}{\lr{TD3}}} & {\rotatebox[origin=c]{90}{\lr{MA-TD3}}}
		& {\rotatebox[origin=c]{90}{\lr{TD3}}} & {\rotatebox[origin=c]{90}{\lr{MA-TD3}}}
		& {\rotatebox[origin=c]{90}{\lr{TD3}}} & {\rotatebox[origin=c]{90}{\lr{MA-TD3}}} \\
		\midrule
		شرایط اولیه تصادفی
		&
		$-2.95$ & ${-0.21}$ & $0.39$ & ${0.12}$ & $5.05$ & ${4.40}$ & $1.00$ & ${0.28}$\\
		اغتشاش در عملگرها
		&
		$0.56$ & ${0.80}$ & $0.02$ & ${0.00}$ & $3.06$ & ${2.40}$ & $0.00$ & ${0.00}$ \\
		عدم تطابق مدل
		&
		$-4.73$ & ${-2.80}$ & $0.47$ & ${0.43}$ & $5.53$ & ${5.00}$ & $1.00$ & ${0.45}$ \\
		مشاهده ناقص
		&
		$0.21$ & ${0.78}$ & $0.02$ & ${0.01}$ & $4.09$ & ${2.90}$ & $0.00$ & ${0.00}$ \\
		نویز حسگر
		&
		$-0.08$ & ${-0.07}$ & $0.11$ & ${0.10}$ & $5.46$ & ${5.20}$ & $0.00$ & ${0.00}$ \\
		تأخیر زمانی
		&
		$0.55$ & ${0.75}$ & $0.01$ & ${0.01}$ & $4.57$ & ${4.30}$ & $0.00$ & ${0.00}$ \\
		\bottomrule
	\end{tabular}
	\caption{مقایسه عملکرد \lr{TD3} و \lr{MA-TD3} در سناریوهای مختلف مقاومت}
	\label{tab:td3_comparison}
\end{table}

الگوریتم \lr{TD3} در هر دو حالت عملکرد قابل توجهی دارد؛ نسخه بازی مجموع‌صفر آن عمدتاً بهبودهای معناداری در کیفیت مسیر و مصرف سوخت نشان می‌دهد، هرچند در برخی سناریوها عملکرد آن با نسخه استاندارد برابری می‌کند و تفاوت‌ها ناچیز است. ثبات بیشتر این الگوریتم در مقایسه با \lr{DDPG} در هر دو نسخه قابل مشاهده است.
\subsubsection*{جمع‌بندی تحلیلی پایان‌نامه}
\begin{itemize}
	\item در سناریوهای شرایط اولیه تصادفی و مشاهده ناقص، ترکیب بهبود پاداش و کاهش تلاش کنترلی برای \lr{MA-TD3} نشان می‌دهد که این سیاست در مواجهه با عدم‌قطعیت‌های ابتدایی و داده‌های ناقص، تصمیم‌گیری هموارتر و کم‌هزینه‌تری ارائه می‌دهد.
	\item تساوی کامل شاخص‌ها در سناریوهای نویز حسگر و تأخیر زمانی، پیام روشنی برای ارزیابی پایان‌نامه دارد: نسخه چندعامله حداقل کارایی نسخه پایه را حفظ می‌کند و در عین حال زمینه‌ای برای استفاده از راهبردهای دفاعی پیچیده‌تر بدون تنبیه عملکردی فراهم کرده است.
	\item کاهش احتمال شکست از ۱ به ۰.۴۵ در سناریوی عدم تطابق مدل مؤید آن است که یادگیری چندعامله حساسیت سیاست به خطاهای مدل را کاهش داده است؛ نکته‌ای کلیدی برای مأموریت‌های فضایی که با مدل‌های تقریبی اجرا می‌شوند.
	\item تفاوت اندک در سناریوی اغتشاش عملگرها نشان می‌دهد که انعطاف‌پذیری \lr{TD3} اصلی در برابر اغتشاش‌های کنترلی بسیار بالاست و نسخه چندعامله باید از اطلاعات اضافی (مانند تعاملات تیمی) بهره بیشتری ببرد تا شکاف معنادار ایجاد کند.
\end{itemize}