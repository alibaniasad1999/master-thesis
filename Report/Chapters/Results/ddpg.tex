\section{الگوریتم \lr{DDPG}}
\label{sec:ddpg_results}

الگوریتم \lr{DDPG}  از جمله روش‌های یادگیری خارج از سیاست است که از دو شبکه عصبی برای بازیگر و منتقد استفاده می‌کند. در اینجا، عملکرد نسخه استاندارد و نسخه مبتنی بر بازی مجموع‌صفر این الگوریتم در کنترل فضاپیما مقایسه شده است.

\subsection{مسیر طی‌شده}
این بخش مسیر طی‌شده فضاپیما را برای نسخه استاندارد و نسخه بازی مجموع‌صفر \lr{DDPG} نشان می‌دهد.
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[\lr{DDPG} استاندارد]{\includegraphics[width=.45\textwidth]{plots/ddpg/trajectory_force/plot_trajectory.pdf}}%
	\subfloat[\lr{MA-DDPG} بازی مجموع‌صفر]{\includegraphics[width=.45\textwidth]{plots/ddpg/trajectory_force/plot_trajectory_zs.pdf}}%
	
	\caption{مسیر طی‌شده فضاپیما با \lr{DDPG} استاندارد و نسخه بازی مجموع‌صفر
		\lr{MA-DDPG}.}
\end{figure}

\subsection{مسیر و فرمان پیشران}
این بخش مسیر و پروفایل فرمان پیشران در طول زمان را برای هر دو نسخه \lr{DDPG} ارائه می‌کند.
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[\lr{DDPG} استاندارد]{\includegraphics[width=.45\textwidth]{plots/ddpg/trajectory_force/plot_trajectory_force.pdf}}%
	\subfloat[\lr{MA-DDPG} بازی مجموع‌صفر]{\includegraphics[width=.45\textwidth]{plots/ddpg/trajectory_force/plot_trajectory_force_zs.pdf}}%
	
	\caption{مسیر و فرمان پیشران فضاپیما در \lr{DDPG} استاندارد و نسخه بازی مجموع‌صفر
		\lr{MA-DDPG}.}
\end{figure}


\subsection{توزیع پاداش تجمعی}
این بخش نمودارهای ویولن توزیع پاداش تجمعی را در سناریوهای مختلف برای \lr{DDPG} و \lr{MA-DDPG} نمایش می‌دهد.
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[شرایط اولیه تصادفی]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/initial_condition_shift.pdf}}%
	\subfloat[اغتشاش در عملگرها]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/actuator_disturbance.pdf}}%
	\subfloat[عدم تطابق مدل]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/model_mismatch.pdf}}\\[1ex]
	
	% سطر دوم
	\subfloat[مشاهده ناقص]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/partial_observation.pdf}}%
	\subfloat[نویز حسگر]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/sensor_noise.pdf}}%
	\subfloat[تأخیر زمانی]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/time_delay.pdf}}
	
	\caption{مقایسه توزیع پاداش تجمعی در سناریوهای مختلف برای \lr{DDPG} و \lr{MA-DDPG}.}
	\label{fig:ddpg_robustness_violin}
\end{figure}

\subsection{مقایسه عددی}
این بخش شاخص‌های عددی را گزارش می‌کند؛ نتایج بر اساس $100$ اجرای مستقل شبیه‌سازی برای هر سناریو به‌دست آمده‌اند.
\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{3pt}
	\small
	\begin{tabular}{@{} R{3.2cm} *{8}{C{1.05cm}} @{}}
		\toprule
		\multirow{2}{*}{\makecell[r]{سناریو}}
		& \multicolumn{2}{c}{پاداش تجمعی} & \multicolumn{2}{c}{مجموع خطای مسیر}
		& \multicolumn{2}{c}{مجموع تلاش کنترلی} & \multicolumn{2}{c}{احتمال شکست} \\
		\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
		& {\rotatebox[origin=c]{90}{\lr{DDPG}}} & {\rotatebox[origin=c]{90}{\lr{MA-DDPG}}}
		& {\rotatebox[origin=c]{90}{\lr{DDPG}}} & {\rotatebox[origin=c]{90}{\lr{MA-DDPG}}}
		& {\rotatebox[origin=c]{90}{\lr{DDPG}}} & {\rotatebox[origin=c]{90}{\lr{MA-DDPG}}}
		& {\rotatebox[origin=c]{90}{\lr{DDPG}}} & {\rotatebox[origin=c]{90}{\lr{MA-DDPG}}} \\
		\midrule
		شرایط اولیه تصادفی
		&
		$-4.17$ & $-3.20$ & $0.40$ & $0.36$ & $5.52$ & $5.30$ & $1.00$ & $0.40$ \\
		اغتشاش در عملگرها
		& $-1.93$ & $-1.80$  & $7.56$ & $6.80$ & $5.60$ & $5.30$ & $0.90$ & $0.28$ \\
		عدم تطابق مدل
		& $-3.24$ & $-2.20$ & $0.70$ & $0.60$ & $5.29$ & $4.90$ & $1.00$ & $0.38$ \\
		مشاهده ناقص
		&
		$-3.28$ & $-2.50$ & $0.68$ & $0.60$ & $5.51$ & $5.10$ & $0.60$ & $0.48$ \\
		نویز حسگر  
		& $-1.07$ & $-0.38$ & $0.10$ & $0.08$ & $5.54$ & $5.35$ & $0.00$ & $0.00$ \\
		تأخیر زمانی        
		&
		$-3.20$ & $-1.55$ & $1.74$ & $1.40$ & $5.61$ & $5.30$ & $0.70$ & $0.45$ \\
		\bottomrule
	\end{tabular}
	\caption{مقایسه عملکرد \lr{DDPG} و \lr{MA-DDPG} در سناریوهای مختلف مقاومت}
	\label{tab:ddpg_comparison}
\end{table}

در جمع‌بندی بر اساس داده‌های جدول، \lr{MA-DDPG} در پنج سناریو پاداش تجمعی بهتری از \lr{DDPG} دارد و در اغتشاش عملگرها هر دو نسخه عملکردی نزدیک ارائه می‌دهند؛ مجموع خطای مسیر و تلاش کنترلی نیز در تمام سناریوها یا برابر شده‌اند یا با فاصله‌ای ناچیز گزارش می‌شوند. کاهش احتمال شکست در دو سناریوی بحرانی نشان می‌دهد که نسخه چندعامله، ریسک عملیاتی را بدون تحمیل هزینه کنترلی اضافی مدیریت کرده است.
\subsubsection*{
تحلیل نتایج
}
\begin{itemize}
	\item در شرایط اولیه‌ی نامطمئن و عدم تطابق مدل، برتری پاداشی \lr{MA-DDPG} همراه با کاهش احتمال شکست (از $1.00$ به $0.40$ و از $1.00$ به $0.38$) بیانگر توانایی این نسخه در مدیریت عدم قطعیت‌های ساختاری است؛ در حالی‌که مجموع تلاش کنترلی تفاوت محدودی دارد ($5.52$ در برابر $5.30$ و $5.29$ در برابر $4.90$).
	\item در اغتشاش عملگرها، پاداش تجمعی تقریباً برابر (به‌ترتیب $-1.93$ و $-1.80$) و مجموع خطای مسیر با فاصله محدود ($7.56$ در برابر $6.80$) گزارش شده‌اند؛ مزیت نسخه چندعامله عمدتاً در کاهش احتمال شکست از $0.90$ به $0.28$ خلاصه می‌شود که ارزش عملیاتی قابل‌توجهی دارد.
	\item در سناریوی مشاهده ناقص، هم‌راستایی خطای مسیر ($0.68$ در برابر $0.60$) و افزایش ایمنی (کاهش احتمال شکست از $0.60$ به $0.48$) توازن خوبی میان کیفیت مسیر و تاب‌آوری سنسوری ایجاد کرده است، هرچند تفاوت تلاش کنترلی ($5.51$ در برابر $5.10$) نشان می‌دهد هنوز فضای بهینه‌سازی انرژی وجود دارد.
	\item دو سناریوی نویز حسگر و تأخیر زمانی نشان می‌دهند که نسخه چندعامله می‌تواند پاداش تجمعی بهتری ارائه دهد ($-1.07$ در برابر $-0.38$ و $-3.20$ در برابر $-1.55$)؛ این بهبود بدون افزایش معنادار در تلاش کنترلی حاصل شده است ($5.54$ در برابر $5.35$ و $5.61$ در برابر $5.30$) و هم‌زمان احتمال شکست را در مواجهه با تأخیر زمانی از $0.70$ به $0.45$ کاهش می‌دهد.
\end{itemize}