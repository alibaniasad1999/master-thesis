\section{الگوریتم \lr{DDPG}}
\label{sec:ddpg_results}

الگوریتم \lr{DDPG}  از جمله روش‌های یادگیری خارج از سیاست است که از دو شبکه عصبی برای بازیگر و منتقد استفاده می‌کند. در اینجا، عملکرد نسخه استاندارد و نسخه مبتنی بر بازی مجموع‌صفر این الگوریتم در کنترل فضاپیما مقایسه شده است.

\subsection{مسیر طی‌شده}
این بخش مسیر طی‌شده فضاپیما را برای نسخه استاندارد و نسخه بازی مجموع‌صفر \lr{DDPG} نشان می‌دهد.
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[\lr{DDPG} استاندارد]{\includegraphics[width=.45\textwidth]{plots/ddpg/trajectory_force/plot_trajectory.pdf}}%
	\subfloat[\lr{MA-DDPG} بازی مجموع‌صفر]{\includegraphics[width=.45\textwidth]{plots/ddpg/trajectory_force/plot_trajectory_zs.pdf}}%
	
	\caption{مسیر طی‌شده فضاپیما با \lr{DDPG} استاندارد و نسخه بازی مجموع‌صفر
		\lr{MA-DDPG}.}
\end{figure}

\subsection{مسیر و فرمان پیشران}
این بخش مسیر و پروفایل فرمان پیشران در طول زمان را برای هر دو نسخه \lr{DDPG} ارائه می‌کند.
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[\lr{DDPG} استاندارد]{\includegraphics[width=.45\textwidth]{plots/ddpg/trajectory_force/plot_trajectory_force.pdf}}%
	\subfloat[\lr{MA-DDPG} بازی مجموع‌صفر]{\includegraphics[width=.45\textwidth]{plots/ddpg/trajectory_force/plot_trajectory_force_zs.pdf}}%
	
	\caption{مسیر و فرمان پیشران فضاپیما در \lr{DDPG} استاندارد و نسخه بازی مجموع‌صفر
		\lr{MA-DDPG}.}
\end{figure}


\subsection{توزیع پاداش تجمعی}
این بخش نمودارهای ویولن توزیع پاداش تجمعی را در سناریوهای مختلف برای \lr{DDPG} و \lr{MA-DDPG} نمایش می‌دهد.
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[شرایط اولیه تصادفی]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/initial_condition_shift.pdf}}%
	\subfloat[اغتشاش در عملگرها]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/actuator_disturbance.pdf}}%
	\subfloat[عدم تطابق مدل]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/model_mismatch.pdf}}\\[1ex]
	
	% سطر دوم
	\subfloat[مشاهده ناقص]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/partial_observation.pdf}}%
	\subfloat[نویز حسگر]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/sensor_noise.pdf}}%
	\subfloat[تأخیر زمانی]{\includegraphics[width=.33\textwidth]{plots/ddpg/violin_plot/time_delay.pdf}}
	
	\caption{مقایسه توزیع پاداش تجمعی در سناریوهای مختلف برای \lr{DDPG} و \lr{MA-DDPG}.}
	\label{fig:ddpg_robustness_violin}
\end{figure}

\subsection{مقایسه عددی}
این بخش شاخص‌های عددی را گزارش می‌کند؛ نتایج بر اساس 100 اجرای مستقل شبیه‌سازی برای هر سناریو به‌دست آمده‌اند.
\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{3pt}
	\small
	\begin{tabular}{@{} R{3.2cm} *{8}{C{1.05cm}} @{}}
		\toprule
		\multirow{2}{*}{\makecell[r]{سناریو}}
		& \multicolumn{2}{c}{پاداش تجمعی} & \multicolumn{2}{c}{مجموع خطای مسیر}
		& \multicolumn{2}{c}{مجموع تلاش کنترلی} & \multicolumn{2}{c}{احتمال شکست} \\
		\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
		& {\rotatebox[origin=c]{90}{\lr{DDPG}}} & {\rotatebox[origin=c]{90}{\lr{MA-DDPG}}}
		& {\rotatebox[origin=c]{90}{\lr{DDPG}}} & {\rotatebox[origin=c]{90}{\lr{MA-DDPG}}}
		& {\rotatebox[origin=c]{90}{\lr{DDPG}}} & {\rotatebox[origin=c]{90}{\lr{MA-DDPG}}}
		& {\rotatebox[origin=c]{90}{\lr{DDPG}}} & {\rotatebox[origin=c]{90}{\lr{MA-DDPG}}} \\
		\midrule
		شرایط اولیه تصادفی
		&
		$-4.17$ & ${-3.20}$ & $0.40$ & ${0.36}$ & $5.52$ & ${5.30}$ & $1.00$ & ${0.40}$ \\
		اغتشاش در عملگرها
		& $-1.93$ & ${-1.80}$  & $7.56$ & ${6.80}$ & $5.60$ & ${5.30}$ & $0.90$ & ${0.28}$ \\
		عدم تطابق مدل
		& $-3.24$ & ${-2.20}$ & $0.70$ & ${0.60}$ & $5.29$ & ${4.90}$ & $1.00$ & ${0.38}$ \\
		مشاهده ناقص
		&
		$-3.28$ & ${-2.50}$ & $0.68$ & ${0.60}$ & $5.51$ & ${5.10}$ & $0.60$ & ${0.48}$ \\
		نویز حسگر  
		&$-1.07$ & ${-0.38}$ & $0.10$ & ${0.08}$ & $5.54$ & ${5.35}$ & $0.00$ & ${0.00}$ \\
		تأخیر زمانی        
		&
		$-3.20$ & ${-1.55}$ & $1.74$ & ${1.40}$ & $5.61$ & ${5.30}$ & $0.70$ & ${0.45}$ \\
		\bottomrule
	\end{tabular}
	\caption{مقایسه عملکرد \lr{DDPG} و \lr{MA-DDPG} در سناریوهای مختلف مقاومت}
	\label{tab:ddpg_comparison}
\end{table}

در جمع‌بندی بر اساس داده‌های جدول، \lr{MA-DDPG} در پنج سناریو پاداش تجمعی بهتری از \lr{DDPG} دارد و در اغتشاش عملگرها هر دو نسخه عملکردی برابر ارائه می‌دهند؛ مجموع خطای مسیر و تلاش کنترلی نیز در تمام سناریوها یا برابر شده‌اند یا با فاصله‌ای ناچیز گزارش می‌شوند. کاهش احتمال شکست در دو سناریوی بحرانی نشان می‌دهد که نسخه چندعامله، ریسک عملیاتی را بدون تحمیل هزینه کنترلی اضافی مدیریت کرده است.
\subsubsection*{جمع‌بندی تحلیلی پایان‌نامه}
\begin{itemize}
	\item در شرایط اولیه‌ی نامطمئن و عدم تطابق مدل، برتری پاداشی \lr{MA-DDPG} همراه با کاهش احتمال شکست (از ۱ به ۰.۴۰ و از ۱ به ۰.۳۸) بیانگر توانایی این نسخه در مدیریت عدم قطعیت‌های ساختاری است؛ با این حال حفظ مجموع تلاش کنترلی برابر نشان می‌دهد که بهبودها ناشی از سیاست‌های هدفمندتر است نه مصرف سوخت بیشتر.
	\item در اغتشاش عملگرها، تساوی کامل در پاداش و خطای مسیر حاکی از آن است که مزیت نسخه چندعامله عمدتاً در کاهش احتمال شکست از ۰.۹۰ به ۰.۲۸ خلاصه می‌شود؛ این نکته در ارزیابی پایان‌نامه به‌معنای ارزش عملیاتی بالاست حتی زمانی که شاخص‌های کلاسیک تفاوتی ندارند.
	\item در سناریوی مشاهده ناقص، هم‌ترازی خطای مسیر و افزایش ایمنی، توازن خوبی بین کیفیت مسیر و تاب‌آوری سنسوری فراهم کرده است، اما عدم برتری در تلاش کنترلی نشان می‌دهد هنوز فضای بهبود برای سیاست‌های انرژی-کارآمد وجود دارد و در ارزیابی پایان‌نامه باید مورد توجه قرار گیرد.
	\item دو سناریوی نویز حسگر و تأخیر زمانی نشان می‌دهند که نسخه چندعامله می‌تواند بدون هزینه اضافه در پارامترهای کلیدی، برتری پاداشی خود را حفظ کند؛ این ثبات رفتار برای سامانه‌های واقعی با کانال‌های ارتباطی ناپایدار، امتیاز پررنگی محسوب می‌شود.
\end{itemize}