\section{نتایج نسخه استاندارد}
\label{sec:std_results}
در این بخش، نتایج نسخه‌های تک‌عاملی الگوریتم‌ها در سناریوهای مقاومت مختلف ارائه و تحلیل می‌شود.

\subsection{توزیع پاداش تجمعی}
\begin{figure}[H]
	\centering
	
	% سطر اول
	\subfloat[شرایط اولیه تصادفی]{\includegraphics[width=.33\textwidth]{plots/standard/violin_plot/initial_condition_shift.pdf}}%
	\subfloat[اغتشاش در عملگرها]{\includegraphics[width=.33\textwidth]{plots/standard/violin_plot/actuator_disturbance.pdf}}%
	\subfloat[عدم تطابق مدل]{\includegraphics[width=.33\textwidth]{plots/standard/violin_plot/model_mismatch.pdf}}\\[1ex]
	
	% سطر دوم
	\subfloat[مشاهده ناقص]{\includegraphics[width=.33\textwidth]{plots/standard/violin_plot/partial_observation.pdf}}%
	\subfloat[نویز حسگر]{\includegraphics[width=.33\textwidth]{plots/standard/violin_plot/sensor_noise.pdf}}%
	\subfloat[تأخیر زمانی]{\includegraphics[width=.33\textwidth]{plots/standard/violin_plot/time_delay.pdf}}
	
	\caption{مقایسه توزیع پاداش تجمعی برای نسخه‌های تک‌عاملی  در سناریوهای مختلف.}
	\label{fig:std_robustness_violin}
\end{figure}

\subsection{مقایسه عددی}
\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{6pt}
	\renewcommand{\arraystretch}{1.35}
	\scriptsize
	\makebox[\linewidth][c]{%
		\parbox{.48\linewidth}{
			\centering
			\footnotesize
			\begin{tabular}{@{}l*{5}{c}}
				\toprule
				{سناریو} & \lr{DDPG} & \lr{PPO} & \lr{SAC} & \lr{TD3} & \lr{PID} \\
				\midrule
				شرایط اولیه تصادفی & $-0.27$ & $0.61$ & $-0.76$ & $0.56$ & $-1.99$ \\
				اغتشاش در عملگرها & $-0.38$ & $0.61$ & $-0.72$ & $0.55$ & $0.72$ \\
				عدم تطابق مدل      & $-0.84$ & $0.58$ & $-2.98$ & $0.51$ & $-1.97$ \\
				مشاهده ناقص        & $-0.88$ & $0.36$ & $-3.65$ & $0.23$ & $-1.92$ \\
				\bottomrule
			\end{tabular}
			\caption*{\normalfont پاداش تجمعی}
		}
		\hspace{0.04\linewidth}
		\parbox{.48\linewidth}{
			\centering
			\footnotesize
			\begin{tabular}{@{}l*{5}{c}}
				\toprule
			 \lr{DDPG} & \lr{PPO} & \lr{SAC} & \lr{TD3} & \lr{PID} \\
				\midrule
		 $329.59$ & $255.94$ & $806.11$ & $72.03$ & $181.38$ \\
	 $373.66$ & $257.85$ & $791.08$ & $76.55$ & $273.42$ \\
		 $1087.24$ & $306.32$ & $1712.44$ & $109.01$ & $976.93$ \\
			 $817.56$ & $334.29$ & $1547.22$ & $177.19$ & $103.00$ \\
				\bottomrule
			\end{tabular}
			\caption*{\normalfont مجموع خطای مسیر}
		}%
	}
	
	\vspace{0.6em}
	
	\makebox[\linewidth][c]{%
		\parbox{.48\linewidth}{
			\centering
			\footnotesize
			\begin{tabular}{@{}l*{5}{c}}
				\toprule
				{سناریو} & \lr{DDPG} & \lr{PPO} & \lr{SAC} & \lr{TD3} & \lr{PID} \\
				\midrule
				شرایط اولیه تصادفی & $5.11$ & $0.77$ & $1.76$ & $3.31$ & $81802.96$ \\
				اغتشاش در عملگرها & $4.89$ & $0.77$ & $1.71$ & $3.07$ & $5.65$ \\
				عدم تطابق مدل      & $5.48$ & $0.86$ & $2.37$ & $4.32$ & $479167.85$ \\
				مشاهده ناقص        & $5.37$ & $1.03$ & $2.33$ & $4.10$ & $489.92$ \\
				\bottomrule
			\end{tabular}
			\caption*{\normalfont مجموع تلاش کنترلی}
		}
		\hspace{0.04\linewidth}
		\parbox{.48\linewidth}{
			\centering
			\footnotesize
			\begin{tabular}{@{}l*{5}{c}}
				\toprule
		\lr{DDPG} & \lr{PPO} & \lr{SAC} & \lr{TD3} & \lr{PID} \\
				\midrule
		$0.00$ & $0.00$ & $0.00$ & $0.00$ & $1.00$ \\
				$0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ \\
			 $0.00$ & $0.00$ & $1.00$ & $0.00$ & $1.00$ \\
		 $0.00$ & $0.00$ & $1.00$ & $0.00$ & $1.00$ \\
				\bottomrule
			\end{tabular}
			\caption*{\normalfont احتمال شکست}
		}%
	}
	
	\caption{مقایسه سناریوهای مقاومتی نسخه استاندارد با ستون \lr{PID}.}
	\label{tab:std_pid_grid}
\end{table}

\noindent داده‌های جدول \ref{tab:std_pid_grid} نشان می‌دهد که اگرچه \lr{PID} در برخی سناریوها می‌تواند پایداری اولیه ایجاد کند، اما بازده آن نسبت به یادگیری تقویتی به‌ویژه از نظر پاداش مثبت، خطای مسیر و تلاش کنترلی چندین مرتبه بدتر است؛ برای نمونه در عدم تطابق مدل، خطای \lr{PID} حدود دو مرتبه‌بزرگ‌تر از حتی ضعیف‌ترین عامل \lr{RL} است و در مشاهده ناقص، \lr{TD3} با پاداش $0.23$ و خطای $177.19$ به‌مراتب متوازن‌تر از \lr{PID} با پاداش منفی و تلاش $489.92$ عمل می‌کند.

بر اساس داده‌ها، \lr{TD3} به‌طور پایدار بالاترین پاداش و کمترین خطای مسیر را ثبت می‌کند، درحالی‌که \lr{PPO} کمترین تلاش کنترلی را دارد. \lr{SAC} در برخی سناریوهای دشوار (عدم تطابق مدل، مشاهده ناقص، نویز حسگر، تأخیر زمانی) نرخ شکست بالاتری نشان می‌دهد و \lr{DDPG} عموماً از نظر پاداش و خطا ضعیف‌تر از \lr{PPO} و \lr{TD3} است.

\subsubsection*{ تحلیل نتایج}
\begin{itemize}
	\item \lr{TD3} در نسخه تک‌عاملی نیز پاداش‌های مثبت میان $0.23$ تا $0.77$ و خطای مسیر محدود به $0.72$ تا $1.77$ دارد که آن را پایدارترین گزینه از نظر پیگیری مسیر می‌کند.
	\item \lr{PPO} با مجموع تلاش کنترلی در بازه $0.76$ تا $1.03$ بهره‌ورترین مصرف سوخت را ارائه می‌دهد، در حالی‌که پاداش‌های $0.36$ تا $0.61$ آن نشان می‌دهد تعادل کارآمدی میان کارایی و هزینه برقرار شده است.
	\item \lr{SAC} اگرچه در برخی سناریوها پاداش‌های نزدیک به صفر (مثلاً $-0.72$) دارد، نرخ شکست $1.00$ در چهار سناریو و خطای مسیر تا $17.12$ اهمیت بهبود روش‌های سهیم‌سازی اطلاعات را برجسته می‌کند.
	\item \lr{DDPG} با خطای مسیرهای بین $3.30$ تا $10.87$ و پاداش‌های منفی تا $-0.88$ نسبت به \lr{PPO} و \lr{TD3} عملکرد ضعیف‌تری دارد و در جمع‌بندی پایان‌نامه تنها به عنوان پایه مقایسه مطرح می‌شود.
	\item کنترل‌کننده \lr{PID} در هر چهار سناریو یا پاداش منفی، یا خطای بسیار بزرگ و تلاش کنترلی سنگین‌تری نسبت به \lr{PPO} و \lr{TD3} دارد؛ بنابراین رویکردهای یادگیری تقویتی نه‌تنها هزینه سوخت کمتری ثبت می‌کنند بلکه پایداری مسیر را نیز با حاشیه امن بیشتری تضمین می‌کنند.
\end{itemize}

