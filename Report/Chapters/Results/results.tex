\chapter{ارزیابی و نتایج یادگیری}

در این فصل، نتایج حاصل از فرآیند یادگیری تقویتی در محیط سه‌جسمی ارائه و تحلیل شده است. هدف، بررسی عملکرد الگوریتم‌های استفاده‌شده و ارزیابی توانایی آن‌ها در دستیابی به اهداف تعیین‌شده می‌باشد.

\section{تنظیمات آزمایشی}

تنظیمات شبیه‌سازی، شامل پارامترهای محیط، نرخ یادگیری، و اندازه بافر تجربه، در این بخش تشریح شده است.

\section{نتایج عملکرد الگوریتم‌ها}

نتایج عملکرد الگوریتم‌های \lr{DDPG}، \lr{PPO}، \lr{SAC}، و \lr{TD3} با معیارهایی نظیر زمان رسیدن به هدف و مصرف سوخت گزارش شده است.

\section{تحلیل پایداری و همگرایی}

پایداری و سرعت همگرایی فرآیند یادگیری با استفاده از نمودارهای پاداش و معیارهای عددی مورد بررسی قرار گرفته است.

\section{مقایسه با معیارهای مرجع}

عملکرد الگوریتم‌ها با روش‌های مرجع مقایسه شده تا برتری‌ها و محدودیت‌های آن‌ها مشخص گردد.

\begin{figure}[H]
	\centering
	\myviolinplot{plots/ddpg/violin_plot/initial_condition_shift.dat}{0}{4}{-5}{5}{Zero_Sum_DDPG,Standard_DDPG}{Zero Sum DDPG, Standard DDPG}
\caption{مقایسه مجموع پاداش دو الگوریتم تک‌عاملی و چندعاملی
	 \lr{DDPG}
	  در شرایط اولیه تصادفی
	 }



\end{figure}
