\chapter{ارزیابی و نتایج یادگیری}\label{ch:results}

%در این فصل، چارچوب ارزیابی و نتایج تجربی چهار روش مطرح یادگیری تقویتی برای کنترل فضاپیما در میدان گرانشی سه‌جسمی ارائه می‌شود: \lr{Deep Deterministic Policy Gradient (DDPG)}، \lr{Proximal Policy Optimization (PPO)}، \lr{Soft Actor-Critic (SAC)} و \lr{Twin Delayed DDPG (TD3)}. تحلیل‌ها در دو رژیم تک‌عاملی و چندعاملیِ بازی مجموع‌صفر انجام شده و بر سه محور اصلی متمرکز است: سنجش مقاومت در سناریوهای اختلال (شرایط اولیه تصادفی، اغتشاش عملگر، عدم‌تطابق مدل، مشاهده ناقص، نویز حسگر و تأخیر زمانی)، بررسی کیفیت مسیر و پروفایل فرمان پیشران، و گزارش شاخص‌های عددی شامل پاداش تجمعی، خطای مسیر، تلاش کنترلی و احتمال شکست.
%
%به‌منظور هدایت خواننده، ساختار فصل به‌صورت زیر است:
%- ابتدا سناریوهای ارزیابی مقاومت و نحوه پیاده‌سازی آن‌ها معرفی می‌شوند (بخش \ref{sec:robustness_evaluation}).
%- سپس نتایج الگوریتم‌های \lr{DDPG}، \lr{PPO}، \lr{SAC} و \lr{TD3} به‌صورت نظام‌مند ارائه می‌شود؛ در هر مورد، مسیر، فرمان پیشران و توزیع پاداش در سناریوهای مختلف نمایش می‌یابد (بخش‌های \ref{sec:ddpg_results}، \ref{sec:ppo_results}، \ref{sec:sac_results} و \ref{sec:td3_results}).
%- در پایان، مقایسه‌های جمع‌بندی برای نسخه‌های تک‌عاملی و چندعاملی (single-agent در برابر zero-sum multi-agent) ارائه می‌شود تا تصویر روشنی از عملکرد نسبی روش‌ها فراهم گردد (بخش‌های \ref{sec:std_results} و \ref{sec:zs_results}).




در این فصل، چارچوب ارزیابی و نتایج تجربی چهار الگوریتم شاخص یادگیری تقویتی برای کنترل فضاپیما در میدان گرانشی سه‌جسمی برای روش‌های  \lr{DDPG}، \lr{PPO}، \lr{SAC} و \lr{TD3}
ارائه می‌شود. تحلیل‌ها در دو قسمت انجام می‌گیرد: حالت تک‌عاملی استاندارد و حالت چندعاملیِ بازی مجموع‌صفر. تمرکز ارزیابی بر سه محور اصلی است: سنجش مقاومت در برابر آشفتگی‌های محیطی و سامانه‌ای (شرایط اولیه‌ی تصادفی، اغتشاش عملگر، عدم‌تطابق مدل، مشاهده‌ی ناقص، نویز حسگر و تأخیر زمانی)، ارزیابی کیفیت مسیر و پروفایل فرمان پیشران، و گزارش شاخص‌های کمی شامل پاداش تجمعی، خطای مسیر، تلاش کنترلی و احتمال شکست.

به‌منظور هدایت خواننده و تضمین بازتولیدپذیری، ابتدا پروتکل و سناریوهای ارزیابی مقاومت همراه با جزئیات پیاده‌سازی و پارامترگذاری در بخش \ref{sec:robustness_evaluation} معرفی می‌شود. سپس نتایج هر یک از الگوریتم‌ها به‌صورت نظام‌مند ارائه می‌گردد؛ بدین‌ترتیب که مسیر طی‌شده، فرمان‌های پیشران و توزیع پاداش در سناریوهای مختلف تحلیل می‌شود. نتایج \lr{DDPG} در بخش \ref{sec:ddpg_results}، نتایج \lr{PPO} در بخش \ref{sec:ppo_results}، نتایج \lr{SAC} در بخش \ref{sec:sac_results} و نتایج \lr{TD3} در بخش \ref{sec:td3_results} گزارش شده‌اند. در پایان، جمع‌بندی مقایسه‌ای برای نسخه‌های تک‌عاملی 
در بخش \ref{sec:std_results} 
و چندعاملیِ مجموع‌صفر
در بخش \ref{sec:zs_results}
 ارائه می‌شود تا تصویر روشنی از عملکرد نسبی روش‌ها فراهم گردد. در این مقایسه‌ها علاوه بر شاخص‌های عددی، مبادله‌های کارایی–پایداری و حساسیت نسبت به
 اغتشاش‌ها نیز مورد بحث قرار می‌گیرد.






\section{ارزیابی مقاومت الگوریتم‌ها}
\label{sec:robustness_evaluation}

در این بخش، مقاومت الگوریتم‌های یادگیری در برابر شرایط مختلف اختلال مورد بررسی قرار گرفته است. این ارزیابی شامل شش سناریوی چالش‌برانگیز می‌شود: (۱) شرایط اولیه تصادفی، (۲) اغتشاش در عملگرها، (۳) عدم تطابق مدل، (۴) مشاهده ناقص، (۵) نویز حسگر و (۶) تأخیر زمانی. هدف، بررسی توانایی الگوریتم‌ها در حفظ کارایی خود در شرایط غیرایده‌آل و نزدیک به واقعیت است.

\subsection{سناریوهای ارزیابی مقاومت}

به‌منظور ایجاز، مشخصات هر سناریو به‌صورت فشرده فهرست شده است:
\begin{enumerate}
  \item شرایط اولیه تصادفی: به هر مؤلفه حالت اولیه نویز گوسی با انحراف معیار $\sigma{=}0.1$ افزوده می‌شود:
  \[
  x_0 \leftarrow x_0 + \mathcal{N}(0,\;0.1^2)
  \]
  \item اغتشاش در عملگرها: نویز افزایشی روی ورودی‌ها و نویز کوچک روی سنسورها:
  \[
  u_t \leftarrow u_t + \mathcal{N}(0,\;0.05^2)
  \]
  \[
  y_t \leftarrow y_t + \mathcal{N}(0,\;0.02^2)
  \]
  \item عدم تطابق مدل: پارامترهای دینامیک در طول انتقال با نویز گوسی مختل می‌شوند:
  \[
  \theta \leftarrow \theta + \mathcal{N}(0,\;0.05^2)
  \]
  \item مشاهده ناقص: در هر گام، به‌صورت تصادفی $50\%$ از مؤلفه‌های مشاهده ماسک شده و مقدارشان صفر می‌شود:
  \[
  m_t^{(i)} \sim \mathrm{Bernoulli}(0.5), \quad
  y_t \leftarrow y_t \circ m_t
  \]
  \item نویز حسگر: نویز گوسی ضربی با $\sigma{=}0.05$ روی هر مؤلفه مشاهده اعمال می‌شود:
  \[
  y_t \leftarrow y_t \circ \bigl(1 + \mathcal{N}(0,\;0.05^2)\bigr)
  \]
  \item تأخیر زمانی: اعمال عامل با تأخیر $10$ گام زمانی اعمال می‌شود و روی عملِ تاخیردار نویز افزایشی افزوده می‌گردد:
  \[
  u_t^{\mathrm{applied}} \leftarrow u_{t-10} + \mathcal{N}(0,\;0.05^2)
  \]
\end{enumerate}

\input{Chapters/Results/ddpg}
\input{Chapters/Results/td3}
\input{Chapters/Results/sac}
\input{Chapters/Results/ppo}
\input{Chapters/Results/std} 
\input{Chapters/Results/zs}





























