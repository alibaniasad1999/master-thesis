%\section{اهداف و نوآوری}


\section{اهداف و نوآوری پژوهش}\label{sec:objectives}

برای پاسخ به مسألهٔ فوق، این رساله سه هدف اصلی و چهار نوآوری کلیدی را دنبال می‌کند.

\subsection*{اهداف}
\begin{enumerate}
	\item توسعهٔ چارچوب یادگیری تقویتی چندعاملی که در آن فضاپیما و طبیعت به‌عنوان دو عامل با اهداف متضاد مدل می‌شوند.
	\item طراحی تابع پاداش چندبُعدی به‌منظور موازنهٔ مصرف سوخت، پایداری موقعیت و نرمی فرمان.
	\item ارزیابی تجربی روی سناریوهای {{CRTBP}} و مقایسه با کنترلگرهای کلاسیک (MPC, $H_\infty$) و تک‌عاملی (DDPG, TD3).
\end{enumerate}

\subsection*{نوآوری‌ها}
\begin{itemize}
	\item \textbf{آموزش متمرکز–اجرا توزیع‌شده (CTDE)}: استفاده از تخمین‌گر مشترک ارزش برای تقابل عامل–حریف و اعمال سیاست غیرمتمرکز در زمان اجرا.
	\item \textbf{تقریب سیاست مقاوم با شبکهٔ دوگان Q}: به‌کارگیری TD3 دوبخشی برای کاهش گرادیان بایاس ناشی از تابع ارزش بیش‌براورد.
	\item \textbf{شکل‌دهی پاداش مبتنی بر انرژی مدار}: معرفی مولفهٔ انرژی ژاکوبی در پاداش به‌منظور تضمین نزدیکی به سطح انرژی نامی~$C_{J^\ast}$.
	\item \textbf{سامانهٔ شبیه‌سازی متن‌باز}: پیاده‌سازی محیط CRTBP در قالب افزونهٔ {OpenAI Gym} با امکان تزریق عدم‌قطعیت و انتشار کد در GitHub.
\end{itemize}