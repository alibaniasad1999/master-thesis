%\section{
%    یادگیری تقویتی
%    }
%

%--------------------------------------------------------------------
\section{یادگیری تقویتی}\label{sec:rl}

یادگیری تقویتی (RL) شاخه‌ای از یادگیری ماشین است که در آن یک «عامل» از طریق تعامل پیاپی با «محیط» می‌آموزد چه توالی اقدام‌هایی $a_t\in\mathcal{A}$ را انتخاب کند تا بازده تجمعی آینده را بیشینه کند. یک فرایند تصمیم‌گیری مارکوف (MDP) به‌صورت $\langle\mathcal{S},\mathcal{A},p,r,\gamma\rangle$ تعریف می‌شود که در آن
\begin{itemize}
	\item $\mathcal{S}$: مجموعهٔ حالات؛
	\item $p(s'|s,a)$: دینامیک انتقال؛
	\item $r(s,a)$: پاداش آنی؛
	\item $\gamma\in[0,1)$: ضریب تنزیل.
\end{itemize}
\noindent
سیاست (policy) $\pi(a|s)$ احتمال انتخاب اقدام $a$ در حالت $s$ را بیان می‌کند. هدف بیشینه‌سازی بازده $G_t=\sum_{k=0}^{\infty}\gamma^k r\_{t+k}$ است. روش‌های RL معمولاً در دو دستهٔ \textit{ارزش‌محور} (مانند Q-learning و DQN) و \textit{سیاست‌محور} (مانند REINFORCE) جای می‌گیرند؛ ترکیب این دو به چارچوب \textit{Actor–Critic} منتهی می‌شود که در آن یک بازیگر (Actor) سیاست را به‌روزرسانی می‌کند و یک منتقد (Critic) ارزش یا Q را برآورد می‌نماید.

در حضور فضاهای پیوسته‌ی حالت/اقدام، الگوریتم‌های گرادیان سیاست عمیق مانند DDPG، TD3، SAC و PPO با تکیه بر شبکه‌های عصبی به‌عنوان تقریب‌گر توابع، کارایی بالایی نشان داده‌اند. در این پژوهش، خانوادهٔ Actor–Critic پایهٔ توسعهٔ کنترلگر پیشنهادشده است.
