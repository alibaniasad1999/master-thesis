%\section{
%    یادگیری تقویتی
%    }
%

%--------------------------------------------------------------------
\section{یادگیری تقویتی}\label{sec:rl}

یادگیری تقویتی\LTRfootnote{Reinforcement Learning (RL)}
 شاخه‌ای از یادگیریِ ماشین است که در آن توالیِ اقدام‌ها $\boldsymbol{a}_t\in\mathcal{A}$ به‌گونه‌ای انتخاب می‌شود که بازدهِ تجمعیِ آینده بیشینه شود. یک فرایندِ تصمیم‌گیریِ مارکوف\LTRfootnote{Markov Decision Process (MDP)}
  به‌صورت $\langle\mathcal{S},\mathcal{A},p,r,\gamma\rangle$ تعریف می‌شود که در آن:
\begin{itemize}
	\item $\mathcal{S}$:
	 مجموعه‌ی حالات،
	\item $p(\boldsymbol{s}'|\boldsymbol{s},\boldsymbol{a})$:
	 دینامیکِ انتقال،
	\item $r(s,\boldsymbol{a})$:
	 پاداشِ آنی،
	\item $\gamma\in[0,1)$:
	 ضریبِ تنزیل.
\end{itemize}
\noindent
سیاست\LTRfootnote{Policy}
 $\pi(a|s)$ به‌عنوان احتمالِ انتخابِ اقدامِ $\boldsymbol{a}$ در وضعیتِ $\boldsymbol{s}$ بیان می‌شود. هدف، بیشینه‌سازیِ برگشت\LTRfootnote{Return}
 است:
 \begin{equation}
 	G_t=\sum_{k=0}^{\infty}\gamma^k r_{t+k}
 \end{equation}
 
 روش‌های \lr{RL} معمولاً در دو دسته‌ی {ارزش‌محور} (مانند \lr{Q-learning} و \lr{DQN}) و {سیاست‌محور} (مانند \lr{Reinforce}) جای می‌گیرند؛ ترکیبِ این دو به چارچوبِ \lr{Actor–Critic} منتهی می‌شود که در آن، یک بازیگر (\lr{Actor}) سیاست را به‌روزرسانی می‌کند و یک منتقد (\lr{Critic}) ارزش یا \lr{Q} برآورد می‌شود
  \cite{SuttonBarto2018}.

در حضورِ فضاهای پیوسته‌ی حالت–عمل، الگوریتم‌های  \lr{DDPG}، \lr{TD3}، \lr{SAC} و \lr{PPO} با تکیه بر شبکه‌های عصبی به‌عنوان تقریب‌گر توابع، کاراییِ بالایی نشان داده‌اند. در این پژوهش، خانواده‌ی \lr{Actor–Critic} به‌عنوان پایه‌ی توسعه‌ی کنترل‌کننده پیشنهاد شده‌ است و در ادامه، به نسخه‌ی چندعاملیِ آن در بخش~\ref{sec:marl} پیوند داده می‌شود.
