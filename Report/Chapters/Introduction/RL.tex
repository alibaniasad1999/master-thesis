%\section{
%    یادگیری تقویتی
%    }
%

%--------------------------------------------------------------------
\section{یادگیری تقویتی}\label{sec:rl}

یادگیری تقویتی\LTRfootnote{Reinforcement Learning (RL)}
 شاخه‌ای از یادگیری ماشین است که در آن یک عامل از طریق تعامل پیاپی با محیط می‌آموزد چه توالی اقدام‌هایی $a_t\in\mathcal{A}$ را انتخاب کند تا بازده تجمعی آینده را بیشینه کند. یک فرایند تصمیم‌گیری مارکوف\LTRfootnote{Markov Decision Process (MDP)}
  به‌صورت $\langle\mathcal{S},\mathcal{A},p,r,\gamma\rangle$ تعریف می‌شود که در آن
\begin{itemize}
	\item $\mathcal{S}$:
	 مجموعهٔ حالات،
	\item $p(s'|s,a)$:
	 دینامیک انتقال،
	\item $r(s,a)$:
	 پاداش آنی،
	\item $\gamma\in[0,1)$:
	 ضریب تنزیل.
\end{itemize}
\noindent
سیاست\LTRfootnote{Policy}
 $\pi(a|s)$ احتمال انتخاب اقدام $\boldsymbol{a}$ در وضعیت $\boldsymbol{s}$ را بیان می‌کند. هدف بیشینه‌سازی برگشت\LTRfootnote{Return}
  
 \begin{equation}
 	G_t=\sum_{k=0}^{\infty}\gamma^k r_{t+k}
 \end{equation}
  است. روش‌های \lr{RL} معمولاً در دو دسته‌ی {ارزش‌محور} (مانند \lr{Q-learning} و \lr{DQN}) و {سیاست‌محور} (مانند \lr{REINFORCE}) جای می‌گیرند؛ ترکیب این دو به چارچوب \lr{Actor–Critic} منتهی می‌شود که در آن یک بازیکن (\lr{Actor}) سیاست را به‌روزرسانی می‌کند و یک منتقد (\lr{Critic}) ارزش یا \lr{Q} را برآورد می‌نماید
  \cite{SuttonBarto2018}.

در حضور فضاهای پیوسته‌ی حالت-عمل، الگوریتم‌های گرادیان سیاست عمیق مانند \lr{DDPG}، \lr{TD3}، \lr{SAC} و \lr{PPO} با تکیه بر شبکه‌های عصبی به‌عنوان تقریب‌گر توابع، کارایی بالایی نشان داده‌اند. در این پژوهش، خانواده‌ی \lr{Actor–Critic} پایه‌ی توسعه‌ی کنترل‌کننده پیشنهاد شده‌است.



