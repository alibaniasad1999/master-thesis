\section{محتوای گزارش}
در آغازِ این گزارش، در **فصل دو** مروری انتقادی بر پیشینهٔ پژوهش فراهم شده است؛ بدین‌گونه دستاوردهای مأموریت‌های بین‌مداری و کوشش‌های پیشینِ یادگیری تقویتی در قالب متنی تحلیلی عرضه شده و خلأهای موجود برجسته شده‌اند. پس از این بستر تاریخی، در **فصل سه** به یادگیری تقویتی پرداخته شده است؛ در آن‌جا مفاهیمی چون حالت، عمل، سیاست و معادلات بلمن توضیح داده شده‌اند و چهار الگوریتم شاخص—DDPG، TD3، SAC و PPO—با ساختار و شبه‌کد خلاصه‌شان معرفی شده‌اند تا شالودهٔ نظری کار مستحکم شود. ادامهٔ روایت در **فصل چهار** به یادگیری تقویتیِ چندعاملی اختصاص یافته است؛ جایی که تعامل و رقابتِ عامل‌ها در قالب مفاهیم نظریهٔ بازی‌ها تشریح شده و پیوند آن با روش‌های مدرن برجسته شده است. سپس در **فصل پنج** محیط آزمایشی بر پایهٔ مسألهٔ سه‌جسمی محدودِ دایره‌ای مدل‌سازی شده است؛ معادلات حرکت و نقاط لاگرانژ معرفی شده‌اند و ساده‌سازی‌های لازم برای شبیه‌سازی عددی اعمال شده است. در **فصل شش**، عامل‌ها طراحی شده‌اند؛ فضای حالت و عمل مشخص شده است، تابع پاداش شکل داده شده است و فرایند آموزش به کمک الگوریتم‌های منتخب اجرا شده است. گام بعدی در **فصل هفت** برداشته شده است؛ جایی که چارچوب «سخت‌افزار در حلقه» پیاده‌سازی شده و عامل‌های آموزش‌دیده در زمان واقعی آزموده شده‌اند. سرانجام، در **فصل هشت** ارزیابی جامع صورت گرفته است؛ نرخ همگرایی، پایداری مدارها و مقایسه با معیارهای مرجع تحلیل شده‌اند و چشم‌انداز پژوهش‌های آینده ترسیم شده است—تا بدین‌سان روایت پژوهش، از مرور گذشته تا آزمون عملی و تحلیل نتایج، به شکلی پیوسته و در قالبی منفعل اما داستانی پیش برده شده باشد.
