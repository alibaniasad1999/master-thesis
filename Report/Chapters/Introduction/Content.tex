\section{ساختار گزارش}\label{sec:chapter_content}
%در فصل \ref{ch:literature} مروری انتقادی بر کارهای مرتبط در هدایت پیشران‌کم و یادگیری تقویتی تک‌عاملی و چندعاملی ارائه می‌شود. فصل \ref{ch:rl} به مبانی یادگیری تقویتی اختصاص دارد و الگوریتم‌های \lr{DDPG}، \lr{TD3}، \lr{SAC} و \lr{PPO} در بخش‌های \ref{sec:DDPG} تا \ref{sec:PPO} مرور می‌شوند. در فصل \ref{ch:marl} چارچوب یادگیری تقویتی چندعاملی تشریح می‌شود و پیوند آن با بازی‌های جمع‌صفر و تعادل نش در بخش
%\ref{sec:marl_games}
%بیان می‌گردد.
%  در فصل \ref{ch:model} مدل‌سازی محیط آزمایش بر پایه‌ی \lr{CRTBP} ارائه می‌شود.
%   در فصل \ref{ch:simulation} طراحی عامل‌ها، فضای حالت، عمل، تابع پاداش و جزئیات آموزش توضیح داده می‌شود.  سرانجام، در فصل \ref{ch:results} نتایج، مقایسه با معیارهای مرجع و مسیرهای آینده‌ی پژوهش جمع‌بندی می‌شود.
در فصل \ref{ch:literature} مروری انتقادی بر کارهای مرتبط در هدایت پیشران‌کم و یادگیری تقویتی تک‌عاملی و چندعاملی ارائه می‌شود.
در فصل \ref{ch:model} مدل‌سازی محیط آزمایش بر پایه‌ی \lr{CRTBP} ارائه می‌گردد.
فصل \ref{ch:rl} به مبانی یادگیری تقویتی اختصاص دارد و الگوریتم‌های \lr{DDPG}، \lr{TD3}، \lr{SAC} و \lr{PPO} در بخش‌های \ref{sec:DDPG} تا \ref{sec:PPO} مرور می‌شوند.
در فصل \ref{ch:simulation} طراحی شبیه‌سازی شامل تعریف عامل‌ها، فضای حالت، عمل و تابع پاداش توضیح داده می‌شود.
در فصل \ref{ch:marl} چارچوب یادگیری تقویتی چندعاملی تشریح شده و پیوند آن با بازی‌های جمع‌صفر و تعادل نش در بخش \ref{sec:marl_games} بیان می‌شود.
در فصل \ref{ch:results} نتایج و مقایسه با معیارهای مرجع ارائه می‌شود.
در نهایت، فصل \ref{chap:conclusion} جمع‌بندی دستاوردها و پیشنهادهای پژوهش‌های آینده را ارائه می‌دهد.
