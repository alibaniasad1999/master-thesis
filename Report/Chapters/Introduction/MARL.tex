%\section{
%    یادگیری تقویتی چند عاملی
%    }
    
    \section{یادگیری تقویتی چندعاملی}\label{sec:marl}

    در یادگیری تقویتی چندعاملی\LTRfootnote{Multi-Agent Reinforcement Learning (MARL)}،
     فضای تصمیم‌گیری به‌صورت یک بازی مارکفی\LTRfootnote{Markov Games (MG)}
      با مجموعه‌ی عامل‌ها
      {$\mathcal{N}=\{1,\dots,N\}$}
       مدل شده که در آن هر عامل با سیاست {$\pi_i$} به‌دنبال بیشینه‌سازی بازده تجمعی خود است. در سناریوهای رقابتیِ دونفره‌ی جمع‌صفر\LTRfootnote{Zero-Sum}، مفهوم تعادل نش\LTRfootnote{Nash Equilibrium} به‌عنوان معیار پایداری سیاست‌ها در نظر گرفته می‌شود.

%    رویکرد آموزشِ متمرکز، اجرایِ توزیع‌شده\LTRfootnote{Centralized Training with Decentralized Execution (CTDE)}
%    با جدا کردن مرحله‌ی آموزش که در آن اطلاعات خصوصیِ همه‌ی عامل‌ها برای منتقدها در دسترس است که در آن هر عامل صرفاً بر مشاهده‌ی محلی اتکا می‌کند تعادل میان کارایی، مقیاس‌پذیری و هزینه‌ی ارتباطی برقرار می‌کند.
%
%    در این پایان‌نامه، یک صورت‌بندیِ دو‌عاملیِ جمع‌صفر اتخاذ می‌شود که در آن {عامل کنترل} سیاست هدایت را می‌آموزد و {عامل مزاحم} اغتشاشات یا نامعینی‌ها را مدل می‌کند تا سیاستی مقاوم حاصل شود. معماری منتخب از منتقد متمرکز مبتنی بر \lr{TD3} (با برآورد دوسویه‌ی $Q$ برای کاهش تورشِ بیش‌براورد) و بازیگران مستقلِ کم‌حجم برای استقرار روی سخت‌افزار بهره می‌برد؛ تابع پاداش مصالحه‌ی سوخت/انحراف/قیود را منعکس می‌کند.

%    \subsection*{چارچوب CTDE}

%    رویکرد آموزش متمرکز، اجرا توزیع‌شده\LTRfootnote{Centralized Training with Decentralized Execution (CTDE)}
%     \cite{lowe2020multiagentactorcriticmixedcooperativecompetitive}
%     با جداکردن مرحله‌ی آموزش که در آن اطلاعات خصوصی همه‌ی عامل‌ها در دسترس است از اجرا که در آن هر عامل صرفاً بر مشاهده‌ی محلی اتکا می‌کند، تعادل بین کارایی و مقیاس‌پذیری را برقرار می‌سازد. این معماری مخصوصاً در حضور تعامل‌های ضعیف عامل‌ها مفید است، زیرا هزینه‌ی ارتباطی در زمان اجرا را حذف می‌کند.

%    \subsection*{الگوریتم‌های شاخص}
%    \begin{itemize}
%    	\item \textbf{MADDPG}: تعمیم DDPG به محیط‌های چندعاملی با منتقد متمرکز و بازیگران مستقل.
%    	\item \textbf{QMIX}: تجمیع خطی منفرد-همواره-منفی از ارزش‌های فردی جهت تضمین افزایش مونوتون.
%    	\item \textbf{MASAC}: نسخه‌ی چندعاملی SAC با انتروپی نرم که توازن اکتشاف/بهره‌برداری را حفظ می‌کند.
%    \end{itemize}
%
%    در این پژوهش، ترکیبی از TD3 و {Robust–Adversarial} انشعاب‌یافته به‌کار گرفته می‌شود تا مدل اغتشاش‌محورِ حریف، امکان یادگیری سیاست مقاوم را فراهم کند. روش پیشنهادی از مزیت تخمین دوسویه‌ی Q برای کاهش واریانس گرادیان و محدودسازی خطای انتشاری بهره می‌برد.
%
%    \vspace{1em}
%    در ادامه‌ی فصل، پس از مرور کارهای مرتبط (فصل~\ref{chap:related})، به تشریح مدل ریاضی CRTBP (فصل~\ref{chap:modeling}) و سپس جزئیات چارچوب پیشنهادی (فصل~\ref{chap:framework}) پرداخته می‌شود.



رویکرد آموزشِ متمرکز، اجرایِ توزیع‌شده\LTRfootnote{Centralized Training with Decentralized Execution (CTDE)}
با جدا کردن مرحله‌ی آموزش که در آن اطلاعات خصوصیِ همه‌ی عامل‌ها برای منتقدها در دسترس است و مرحله‌ی اجرا در آن هر عامل صرفاً بر مشاهده‌ی محلی اتکا می‌کند که باعث تعادل میان کارایی، مقیاس‌پذیری و هزینه‌ی ارتباطی برقرار شده‌است.

در این پایان‌نامه، یک صورت‌بندیِ دو‌عاملیِ جمع‌صفر اتخاذ شده‌است که در آن سیاست هدایت توسط {عاملِ کنترل} آموخته می‌شود و اغتشاشات یا نامعینی‌ها توسط {عاملِ مزاحم} مدل‌سازی می‌شوند تا سیاستی مقاوم حاصل شود.

\begin{itemize}
	\item \lr{DDPG}: الگوریتم مبتنی بر گرادیان سیاستِ قطعی برای فضاهای کنشِ پیوسته،
	\item \lr{TD3}: نسخه‌ی بهبودیافته‌ی DDPG با برآورد دوسویه‌ی $Q$
	برای کاهش تورشِ بیش‌براورد\LTRfootnote{Overestimation Bias}،
	\item \lr{PPO}: الگوریتم سیاست احتمالیِ پایدار با قیود نسبت احتمال و بهبود تدریجی سیاست،
	\item \lr{SAC}: الگوریتم حداکثرسازی آنتروپی که تعادل میان بهره‌برداری و اکتشاف به‌طور ذاتی برقرار می‌شود.
\end{itemize}

تابع پاداشِ طراحی‌شده، مصالحه‌ی سوخت، انحراف و قیود منعکس می‌شود و مبنایی برای ارزیابیِ کیفیتِ سیاست‌های مختلف فراهم می‌گردد.
