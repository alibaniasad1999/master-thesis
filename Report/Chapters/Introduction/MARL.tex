%\section{
%    یادگیری تقویتی چند عاملی
%    }
    
    \section{یادگیری تقویتی چندعاملی}\label{sec:marl}
    
    در یادگیری تقویتی چندعاملی ({MARL})، فضای تصمیم‌گیری به‌صورت یک بازی مارکفی ({MG}) با مجموعهٔ عامل‌ها {$\mathcal{N}=\{1,\dots,N\}$} مدل می‌شود که در آن هر عامل سیاست {$\pi\_i$} مختص خود را با هدف بیشینه‌سازی بازده تجمعی کسب می‌کند. در سناریوهای رقابتیِ دونفره، این چارچوب به بازی‌های \textsc{Zero-Sum} یا عمومی گسترش یافته و مفهوم تعادل نش (Nash Equilibrium) به‌عنوان معیار پایداری استراتژیک مطرح می‌شود.
    
    \subsection*{چارچوب CTDE}
    رویکرد {«آموزش متمرکز، اجرا توزیع‌شده»} \cite{lowe2017multi} با جداکردن مرحلهٔ آموزش (که در آن اطلاعات خصوصی همهٔ عامل‌ها در دسترس است) از اجرا (که در آن هر عامل صرفاً روی مشاهدهٔ محلی اتکا می‌کند)، تعادل بین کارایی و مقیاس‌پذیری را برقرار می‌سازد. این معماری مخصوصاً در حضور تعامل‌های ضعیف مفید است، زیرا هزینهٔ ارتباطی در زمان اجرا را حذف می‌کند.
    
    \subsection*{الگوریتم‌های شاخص}
    \begin{itemize}
    	\item \textbf{MADDPG}: تعمیم DDPG به محیط‌های چندعاملی با منتقد متمرکز و بازیگران مستقل.
    	\item \textbf{QMIX}: تجمیع خطی منفرد-همواره-منفی از ارزش‌های فردی جهت تضمین افزایش مونوتون.
    	\item \textbf{MASAC}: نسخهٔ چندعاملی SAC با انتروپی نرم که توازن اکتشاف/بهره‌برداری را حفظ می‌کند.
    \end{itemize}
    
    در این پژوهش، ترکیبی از TD3 و {Robust–Adversarial} انشعاب‌یافته به‌کار گرفته می‌شود تا مدل اغتشاش‌محورِ حریف، امکان یادگیری سیاست مقاوم را فراهم کند. روش پیشنهادی از مزیت تخمین دوسویهٔ Q برای کاهش واریانس گرادیان و محدودسازی خطای انتشاری بهره می‌برد.
    
    \vspace{1em}
    در ادامهٔ فصل، پس از مرور کارهای مرتبط (فصل~\ref{chap:related})، به تشریح مدل ریاضی CRTBP (فصل~\ref{chap:modeling}) و سپس جزئیات چارچوب پیشنهادی (فصل~\ref{chap:framework}) پرداخته می‌شود.
    