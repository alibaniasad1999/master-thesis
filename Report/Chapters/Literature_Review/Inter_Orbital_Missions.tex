\section{ماموریت‌های بین‌مداری}\label{sec:inter_orbital}
هدایت فضاپیماها معمولاً با استفاده از ایستگاه‌های زمینی
% هدایت، مسیریابی و کنترل\LTRfootnote{Guidance, Navigation and Control (GNC)}  
انجام می‌شود. با این حال، این تکنیک‌ها دارای محدودیت‌هایی از جمله حساسیت به قطع ارتباطات، تأخیرهای زمانی و محدودیت‌های منابع محاسباتی هستند. الگوریتم‌های یادگیری تقویتی و بازی‌های دیفرانسیلی می‌توانند برای بهبود قابلیت‌های هدایت فضاپیماها، از جمله مقاومت در برابر تغییرات محیطی، کاهش تأخیرهای ناشی از ارتباطات زمینی و افزایش کارایی محاسباتی، مورد استفاده قرار گیرند.


%در فرآیند طراحی مسیر، معمولاً یک مسیر بهینه و تاریخچه فرمان کنترلی طراحی می‌شود که با معیارهای ماموریت، مصرف سوخت و زمان پرواز، مطابقت داشته باشد. این روش قبل از پرواز انجام می‌شود و می‌تواند از استراتژی‌های متعددی برای هدایت بهینه با پیشران کم استفاده کند، از جمله تکنیک‌های بهینه‌سازی جهانی \cite{vavrina2017global} و برنامه‌نویسی غیرخطی \cite{ocampo2004finite}.
هدایت فضاپیماها معمولاً  پیش از پرواز انجام می‌شود. این روش‌ها می‌توانند از تکنیک‌های بهینه‌سازی فراگیر  \cite{vavrina2017global} یا برنامه‌نویسی غیرخطی برای تولید مسیرها و فرمان‌های کنترلی بهینه استفاده کنند. با این حال، این روش‌ها معمولاً حجم محاسباتی زیادی دارند و برای استفاده درون‌سفینه‌ای نامناسب هستند \cite{ocampo2004finite}.
%با این حال، توانایی سریع محاسبه مجدد مسیر مرجع و تاریخچه فرمان کنترلی در داخل فضاپیما در هنگام پرواز بسیار مهم است. با نگاه به هدایت درون‌سفینه‌ای از دیدگاه یادگیری ماشین، یک کنترل‌کننده شبکه عصبی حلقه بسته، امکان محاسبه سریع و خودکار تاریخچه کنترل را برای یک فضاپیما فراهم می‌کند. به علاوه، یادگیری تقویتی\LTRfootnote{Reinforcement Learning (RL)}
% یادگیری پیش از پرواز واقعی است و می‌تواند از سخت‌افزارهای سریع و قدرتمند و ارزان قیمت زمینی جهت یادگیری استفاده کند.
% هدایت فضاپیماها معمولاً با استفاده از روش‌های طراحی مسیر پیش‌پرواز انجام می‌شود. با این حال، این روش‌ها محاسباتی فشرده هستند و برای استفاده درون‌سفینه نامناسب هستند.
یادگیری ماشین می‌تواند برای بهبود قابلیت‌های هدایت فضاپیماها استفاده شود. کنترل‌کننده شبکه عصبی حلقه‌‌بسته می‌تواند برای محاسبه سریع و خودکار تاریخچه کنترل استفاده شود. یادگیری تقویتی نیز می‌تواند برای یادگیری رفتارهای هدایت بهینه استفاده شود.

روش‌های هدایت و بهینه‌سازی مسیر فضاپیماها به‌طور کلی به راه‌حل‌های اولیه مناسب نیاز دارند. در مسائل چند جسمی، طراحان مسیر اغلب حدس‌های اولیه کم‌هزینه‌ای برای انتقال‌ها با استفاده از نظریه سیستم‌های دینامیکی و منیفولدهای ثابت
\cite{2013AcAau, haapala2016framework}
ایجاد می‌کنند. 
%روش‌های مبتنی بر سیستم‌های دینامیکی در بسیاری از کاربردهای گذشته مفید بوده‌اند و در ترکیب با اصلاحات دیفرانسیل و یا تکنیک‌های بهینه‌سازی، برای بسیاری از کاربردها راه‌حل‌های بهینه تولید می‌کنند. با این حال، این رویکرد محاسباتی، فشرده است و اغلب به تعاملات انسان در حلقه نیاز دارد. به عنوان یک جایگزین، تکنیک‌های بهینه‌سازی جهانی ابتکاری مانند جستجوی حوضچه و الگوریتم‌های تکاملی، نیاز به راه‌حل‌های راه‌اندازی دقیق را کاهش می‌دهند 
%\cite{vavrina2017global}، اما پیچیدگی محاسباتی مربوطه آنها را برای استفاده درون‌سفینه ناممکن می‌سازد.

%روش‌های مبتنی بر سیستم‌های دینامیکی می‌توانند راه‌حل‌های بهینه تولید کنند، اما محاسباتی فشرده و نیازمند تعاملات انسان هستند. تکنیک‌های بهینه‌سازی جهانی می‌توانند نیاز به راه‌حل‌های راه‌اندازی دقیق \cite{vavrina2017global}
%را کاهش دهند، اما محاسباتی پیچیده هستند.



شبکه‌های عصبی قابلیت‌های منحصر به فردی برای انجام هدایت در فضاپیما دارند. به‌عنوان مثال، شبکه‌های عصبی می‌توانند به‌طور مستقیم از تخمین‌های وضعیت به دستورهای پیشران کنترلی که با محدودیت‌های مأموریت سازگار است، برسند. عملکرد مناسب هدایت شبکه‌های عصبی در مطالعاتی مانند فرود بر سیارات \cite{gaudet2020six}، عملیات نزدیکی به سیارات \cite{gaudet2020terminal} و کنترل فضاپیما با پیشران از دست‌رفته \cite{rubinsztejn2020neural} نشان داده شده‌است.
تازه‌ترین پیشرفت‌های تکنیک‌های یادگیری ماشین در مسائل خودکارسازی درونی به‌طور گسترده‌ای مورد مطالعه قرار گرفته‌اند؛ از پژوهش‌های اولیه تا توانایی‌های پیاده‌سازی.
به‌عنوان مثال، الگوریتم‌های یادگیری ماشین ابتدایی در فضاپیماهای مریخی‌نورد برای کمک به شناسایی ویژگی‌های زمین‌شناسی تعبیه شده‌اند. الگوریتم \lr{AEGIS} توانایی انتخاب خودکار هدف توسط یک دوربین در داخل فضاپیماهای \lr{Spirit}، \lr{Opportunity} و \lr{Curiosity} را دارد
\cite{estlin2012aegis}.
در کامپیوتر پرواز اصلی، فرآیند دقت افزایی\LTRfootnote{Refinement Process} نیاز به 94 تا 96 ثانیه دارد 
\cite{francis2017aegis},
که به طور قابل توجهی کمتر از زمان مورد نیاز برای ارسال تصاویر به زمین و انتظار برای انتخاب دستی توسط دانشمندان است.
برنامه‌های آینده برای کاربردهای یادگیری ماشین درون‌سفینه شامل توانایی‌های رباتیکی درون‌سفینه برای فضاپیمای \lr{Perseverance}
\cite{higa2019vision, rothrock2016spoc}
و شناسایی عیب برای 
\lr{Europa Clipper}
\cite{wagstaff2019enabling} می‌شود. 
الگوریتم‌های یادگیری ماشین پتانسیل انجام نقش مهمی در مأموریت‌های خودکار آینده را دارند.


علاوه بر رباتیک سیاره‌ای، پژوهش‌های مختلفی به استفاده از تکنیک‌های مختلف یادگیری ماشین در مسائل نجومی پرداخته‌اند. در طراحی مسیر عملکرد رگرسیون معمولاً مؤثرتر هست. به عنوان مثال، از یک شبکه عصبی\LTRfootnote{Neural Network}
  در بهینه‌سازی مسیرهای رانشگر کم‌پیشران استفاده شده‌است 
\cite{dachwald2004evolutionary}.
پژوهش‌های جدید شامل شناسایی انتقال‌های هتروکلینیک \cite{desmet2019identifying}، اصلاح مسیر رانشگر کم‌پیشران \cite{parrish2018lowthrust} و تجزیه و تحلیل مشکلات ازدست‌رفتن رانشگر \cite{rubinsztejn2020neural} می‌شود.

%تکنیک‌های یادگیری نظارتی می‌توانند نتایج مطلوبی تولید کنند، اما دارای محدودیت‌های قابل توجهی هستند. ابتدا، این رویکردها بر وجود دانش از پیش از فرآیند تصمیم‌گیری متکی هستند. کاربر با انتخاب نتایج مطلوب، فرض می‌کند که این دانش را دارد. این امر مستلزم دقیق بودن داده‌های تولید‌شده توسط کاربر برای نتایج مطلوب و همچنین وجود تکنیک‌های موجود برای حل مشکل کنونی و تولید داده است. در بخش‌هایی که چنین دانشی وجود ندارد، تکنیک‌های یادگیری نظارتی قابل استفاده نیستند.
تکنیک‌های یادگیری نظارتی می‌توانند نتایج مطلوبی تولید کنند؛ اما، دارای محدودیت‌های قابل توجهی هستند. یکی از این محدودیت‌ها این است که این رویکردها بر وجود دانش پیش از فرآیند تصمیم‌گیری متکی هستند. این امر مستلزم دقیق‌بودن داده‌های تولید‌شده توسط کاربر برای نتایج مطلوب و همچنین وجود تکنیک‌های موجود برای حل مشکل کنونی و تولید داده است.

%در سال‌های اخیر، \lr{RL} به اثبات مفید بودن خود در دستیابی به عملکرد بهترین حالت در دامنه‌هایی با ابهام محیطی قابل توجه رسیده است
%. هدایت فعال‌سازی‌شده توسط \lr{RL} به‌صورت گسترده‌ای بر اساس فاز پرواز دسته‌بندی می‌شوند. مسائل فرود \cite{furfaro2020adaptive, gaudet2020deep} 
%و عملیات در نزدیکی به اجسام کوچک
%\cite{gaudet2020terminal, gaudet2020six}
%از حوزه‌های مطالعاتی بهره‌برداری‌شده‌ای هستند. تحقیقات دیگر شامل مواجهه ،\cite{broida2019spacecraft} تداخل خارجی‌جوی
%\cite{gaudet2020reinforcement}
%، نگهداری ایستگاهی \cite{guzzetti2019reinforcement} و اجتناب از تشخیص \cite{reiter2020augmenting} هستند. مطالعاتی که فضاپیماهای رانشگر کم پیشرانرا در یک چارچوب دینامیکی چندبدنی با استفاده از \lr{RL} شامل طراحی انتقال با استفاده از
% \lr{Q-Learning}
% \cite{dasstuart2020rapid} و \lr{Proximal Policy Optimization }
%  \cite{miller2019lowthrust} و همچنین هدایت نزدیکی مدار
%  \cite{sullivan2020using}، شده‌اند.

در سال‌های اخیر، قابلیت یادگیری تقویتی\LTRfootnote{Reinforcement Learning (RL)}
 در دستیابی به عملکرد بهینه در بخش‌هایی با ابهام محیطی قابل توجه، به اثبات رسیده است  \cite{heess2017emergence, silver2017mastering}.
هدایت انجام‌شده توسط
یادگیری تقویتی
  را می‌توان به‌صورت گسترده بر اساس فاز پرواز دسته‌بندی کرد.
مسائل فرود \cite{furfaro2020adaptive, gaudet2020deep} 
و عملیات در نزدیکی اجسام کوچک
\cite{gaudet2020terminal, gaudet2020six}،
از حوزه‌های پژوهشی هستند که از یادگیری تقویتی استفاده می‌کنند.
تحقیقات دیگر شامل مواجهه تداخل خارجی جوی \cite{gaudet2020reinforcement}،
نگهداری ایستگاهی \cite{guzzetti2019reinforcement}  و هدایت به‌صورت جلوگیری از شناسایی \cite{reiter2020augmenting} است.
مطالعاتی که فضاپیماهای رانشگر کم‌پیشران را در یک چارچوب دینامیکی چندبدنی با استفاده از یادگیری تقویتی انجام شده است، شامل طراحی انتقال با استفاده از \lr{Q-learning}
\cite{dasstuart2020rapid}، \lr{Proximal Policy Optimization}
\cite{miller2019lowthrust}
و  هدایت نزدیکی مدار  \cite{sullivan2020using} است.