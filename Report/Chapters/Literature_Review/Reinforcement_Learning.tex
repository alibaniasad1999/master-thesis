\section{یادگیری تقویتی}


از نخستین صورت‌بندی‌های فرایند تصمیم‌گیری مارکُفی در یادگیری تقویتی، پژوهش بر آن بوده‌است که عامل بتواند با اجرای عمل‌ها و دریافت پاداش، سیاستی برای بیشینه‌سازی بازگشت بیاموزد. تبیین جامع این چارچوب و الگوریتم‌های بنیادین در ویرایش دوم کتاب سوتون و بارتو به‌مثابه مرجع کلاسیک این حوزه ارائه شده و همچنان مبنای بسیاری از آثار معاصر است \cite{SuttonBarto2018}. % 

\noindent
دهه‌‌ی ۱۹۹۰ ملادی شاهد شکل‌گیری روش‌هایی بر پایه‌ی ارزش\LTRfootnote{Value}
 نظیر \lr{Q-learning} و نخستین رویکردهای گرادیانِ سیاست بود؛ با وجود این، محدودیت توان محاسباتی و فقدان داده‌ی فراوان، سرعت رشد را کند می‌کرد. ورود شبکه‌های عصبی عمیق نقطه‌ی عطفی بود: مقاله‌ی معروف دیپ‌مایند\LTRfootnote{DeepMind}
  نشان داد که شبکه‌ی \lr{Q} عمیق (\lr{DQN}) می‌تواند صرفاً از پیکسل‌های بازی آتاری سیاستی نزدیک به انسان بیاموزد \cite{Mnih2015}. % 

\noindent
موفقیت \lr{DQN} نگاه‌ها را به‌سوی گرادیانِ سیاستِ مقیاس‌پذیر معطوف ساخت. بهینه‌سازی ناحیهٔ اطمینان\LTRfootnote{Trust Region Policy Optimization (TRPO)}
  تضمین بهبود یکنواخت سیاست را فراهم کرد \cite{Schulman2015TRPO}, و روش \lr{A3C} با موازی‌سازی بازیگران، سرعت یادگیری را چند برابر افزایش داد \cite{Mnih2016A3C}. % :contentReference[oaicite:2]{index=2}
کمی بعد، \lr{DDPG} اولین بار گرادیان سیاست قطعی را به فضاهای عمل پیوسته وارد کرد \cite{Lillicrap2015DDPG}. % 
سپس \lr{PPO} با ساده‌سازی قیود \lr{TRPO} و کاهش پارامترهای حساس، به انتخاب پیش‌فرض بسیاری از کاربردهای مهندسی بدل شد \cite{Schulman2017PPO}. % 

\noindent
با گسترش دامنه‌ی مسائل، پایداری و کاراییِ داده به چالش اصلی بدل گشت. \lr{TD3} نشان داد که کمینه‌کردن میان دو منتقد می‌تواند برآورد بیش‌از‌حد \lr{Q} را مهار کند \cite{Fujimoto2018TD3}, و \lr{SAC} با افزودن بند آنتروپی، هم‌زمان اکتشاف و بازده را بهبود داد \cite{Haarnoja2018SAC}. % :contentReference[oaicite:5]{index=5}

\noindent
%برای کاهش هزینهٔ تعامل، موج نوینی از یادگیری تقویتی مدل‌مبنا شکل گرفت. منابع نظام‌مند از این خط پژوهش را بررسی جامع مورلاند و همکاران عرضه می‌کند \cite{Moerland2020MBRLsurvey}، درحالی‌که عامل Dreamer نشان داد می‌توان سیاست را تنها با «تخیل نهفته» در مدل آموخته‌شده به‌روزرسانی کرد \cite{Hafner2019Dreamer}. % :contentReference[oaicite:6]{index=6}

\noindent
در محیط‌های پرخطر یا گران، جمع‌آوری داده‌ی برخط ناممکن است؛ ازاین‌رو \lr{RL} آفلاین مطرح شد. روش \lr{CQL} با برقراری کران محافظه‌کارانه بر \lr{Q-value} از گرایشِ خارج از توزیع جلوگیری می‌کند \cite{Kumar2020CQL}، و مرور اخیر پراودِنسیو و همکاران طبقه‌بندی جامعی از چالش‌های باز این حوزه ارائه داده است \cite{Prudencio2022OfflineSurvey}. % 

\noindent
هم‌زمان، دغدغه‌ی ایمنی و تبیین در سامانه‌های واقعی پررنگ شد. مرور سال ۲۰۲۲ نشان می‌دهد که ترکیب قیدهای سخت، توابع جریمه‌ی ریسک و شبیه‌سازی محیط‌های بدبینانه سه خط اصلی ایمنی در \lr{RL} هستند \cite{Garcia2022SafeSurvey}. سلسله‌مراتب نیز با هدف انتقال دانش و تسریع یادگیری مورد توجه قرار گرفت و یک مطالعهٔ جامع در 
\lr{ACM Computing Surveys}
 چهار چالش کشف زیرکار، یادگیری اشتراک‌پذیر، انتقال و مقیاس‌پذیری را برجسته می‌کند \cite{Ghazalpour2021HRLsurvey}. % 

\noindent
وقتی چند عامل به‌طور هم‌زمان یاد می‌گیرند، پویایی محیط از دید هر عامل غیرایستا می‌شود. مرور جامع ۲۰۲۴ نشان می‌دهد که چارچوب ناظر متمرکز ـ بازیگر توزیع‌شده\LTRfootnote{Centralized Training with Decentralized Execution (CTDE)}
 راهکاری موثر برای این چالش است و مباحثی چون تخصیص اعتبار جمعی و کشف تعادل را معرفی می‌کند \cite{Song2024MARLsurvey}. % 

\noindent
پیشرفت‌های یادشده در نهایت به دستاوردهای نمادینی چون \lr{AlphaGo} \cite{Silver2016AlphaGo} و \lr{AlphaStar} \cite{Vinyals2019AlphaStar} انجامیدند که در بازی‌های \lr{Go} و \lr{StarCraft II} از انسان پیشی گرفتند، و معماری توزیع‌شدهٔ \lr{IMPALA} نشان داد که چگونه می‌توان هزاران شبیه‌ساز را با به‌روزرسانی وزن‌های مهم ادغام کرد \cite{Espeholt2018IMPALA}. % 

\noindent
به‌رغم این جهش‌ها، سه شکاف اساسی پابرجا مانده است: ۱) تضمین ایمنی سخت‌گیرانه در سناریوهای نزدیک‌برخورد، ۲) کاهش وابستگی به داده‌ی پرهزینه یا نایاب از طریق روش‌های مدل‌مبنا و آفلاین، و ۳) مقیاس‌پذیری یادگیری چندعاملی برای سامانه‌های رباتیکی یا فضاپیمای چندگانه. پژوهش حاضر در پی آن است که با تلفیق یادگیری تقویتی مقاوم و چندعاملی در چارچوب سه‌جسمی مداری، به این خلأ پاسخ دهد.
