\section{یادگیری تقویتی}


از نخستین صورت‌بندی‌های فرایند تصمیم‌گیری مارکُفی، پژوهش در «یادگیری تقویتی» (RL) بر آن بوده‌است که عامل بتواند با اجرای کنش‌ها و دریافت پاداش، سیاستی برای بیشینه‌سازی بازده بیاموزد. تبیین جامع این چارچوب و الگوریتم‌های بنیادین در ویرایش دوم کتاب سوتون و بارتو به‌مثابه مرجع کلاسیک این حوزه ارائه شده و همچنان مبنای بسیاری از آثار معاصر است \cite{SuttonBarto2018}. % 

\noindent
دههٔ ۱۳۹۰ (۱۹۹۰s میلادی) شاهد شکل‌گیری روش‌های «پایه‌ارزش» نظیر Q-learning و نخستین رویکردهای گرادیانِ سیاست بود؛ با وجود این، محدودیت توان محاسباتی و فقدان دادهٔ حجیم، سرعت رشد را کند می‌کرد. ورود شبکه‌های عصبی عمیق در اوایل دههٔ ۱۳۹۰ خورشیدی (۲۰۱۰s) نقطهٔ عطفی بود: مقالهٔ معروف دیپ‌مایند نشان داد که شبکهٔ Q عمیق (DQN) می‌تواند صرفاً از پیکسل‌های بازی آتاری سیاستی نزدیک به انسان بیاموزد \cite{Mnih2015}. % 

\noindent
موفقیت DQN نگاه‌ها را به‌سوی گرادیانِ سیاستِ مقیاس‌پذیر معطوف ساخت. «بهینه‌سازی ناحیهٔ اطمینان» یا TRPO تضمین بهبود یکنواخت سیاست را فراهم کرد \cite{Schulman2015TRPO}, و روش A3C با موازی‌سازی بازیگران، سرعت یادگیری را چند برابر افزایش داد \cite{Mnih2016A3C}. % :contentReference[oaicite:2]{index=2}
کمی بعد، DDPG اولین بار گرادیان سیاست قطعی را به فضاهای عمل پیوسته وارد کرد \cite{Lillicrap2015DDPG}. % 
سپس PPO با ساده‌سازی قیود TRPO و کاهش پارامترهای حساس، به انتخاب پیش‌فرض بسیاری از کاربردهای مهندسی بدل شد \cite{Schulman2017PPO}. % 

\noindent
با گسترش دامنهٔ مسائل، پایداری و کاراییِ داده به چالش اصلی بدل گشت. TD3 نشان داد که «کمینه‌کردن» میان دو منتقد می‌تواند برآورد بیش‌از‌حد Q را مهار کند \cite{Fujimoto2018TD3}, و SAC با افزودن بند آنتروپی، هم‌زمان اکتشاف و بازده را بهبود داد \cite{Haarnoja2018SAC}. % :contentReference[oaicite:5]{index=5}

\noindent
برای کاهش هزینهٔ تعامل، موج نوینی از «یادگیری تقویتی مدل‌مبنا» شکل گرفت. مأخذی نظام‌مند از این خط پژوهش را بررسی جامع مورلاند و همکاران عرضه می‌کند \cite{Moerland2020MBRLsurvey}، درحالی‌که عامل Dreamer نشان داد می‌توان سیاست را تنها با «تخیل نهفته» در مدل آموخته‌شده به‌روزرسانی کرد \cite{Hafner2019Dreamer}. % :contentReference[oaicite:6]{index=6}

\noindent
در محیط‌های پرخطر یا گران، جمع‌آوری دادهٔ برخط ناممکن است؛ ازاین‌رو RL «آفلاین» مطرح شد. روش CQL با برقراری کران محافظه‌کارانه بر Q-value از گرایشِ خارج از توزیع جلوگیری می‌کند \cite{Kumar2020CQL}، و مرور اخیر پراودِنسیو و همکاران طبقه‌بندی جامعی از چالش‌های باز این حوزه ارائه داده است \cite{Prudencio2022OfflineSurvey}. % 

\noindent
هم‌زمان، دغدغهٔ ایمنی و تبیین در سامانه‌های واقعی پررنگ شد. مرور سال ۲۰۲۲ نشان می‌دهد که ترکیب قیدهای سخت، توابع جریمهٔ ریسک و شبیه‌سازی محیط‌های بدبینانه سه خط اصلی ایمنی در RL هستند \cite{Garcia2022SafeSurvey}. سلسله‌مراتب نیز با هدف انتقال دانش و تسریع یادگیری مورد توجه قرار گرفت و یک مطالعهٔ جامع در {\it ACM Computing Surveys} چهار چالش کشف زیرکار، یادگیری اشتراک‌پذیر، انتقال و مقیاس‌پذیری را برجسته می‌کند \cite{Ghazalpour2021HRLsurvey}. % 

\noindent
وقتی چند عامل به‌طور هم‌زمان یاد می‌گیرند، پویایی محیط از دید هر عامل غیرایستا می‌شود. مرور جامع ۲۰۲۴ نشان می‌دهد که چارچوب «ناظر متمرکز ـ بازیگر توزیع‌شده» (CTDE) راهکاری موثر برای این چالش است و مباحثی چون تخصیص اعتبار جمعی و کشف تعادل را معرفی می‌کند \cite{Song2024MARLsurvey}. % 

\noindent
پیشرفت‌های یادشده در نهایت به دستاوردهای نمادینی چون AlphaGo \cite{Silver2016AlphaGo} و AlphaStar \cite{Vinyals2019AlphaStar} انجامیدند که در بازی‌های Go و StarCraft II از انسان پیشی گرفتند، و معماری توزیع‌شدهٔ IMPALA نشان داد که چگونه می‌توان هزاران شبیه‌ساز را با به‌روزرسانی وزن‌های مهم ادغام کرد \cite{Espeholt2018IMPALA}. % 

\noindent
به‌رغم این جهش‌ها، سه شکاف اساسی پابرجا مانده است: ۱) تضمین ایمنی سخت‌گیرانه در سناریوهای نزدیک‌برخورد، ۲) کاهش وابستگی به دادهٔ پرهزینه یا نایاب از طریق روش‌های مدل‌مبنا و آفلاین، و ۳) مقیاس‌پذیری یادگیری چندعاملی برای سامانه‌های رباتیکی یا فضاپیمای چندگانه. پژوهش حاضر در پی آن است که با تلفیق یادگیری تقویتی مقاوم و چندعاملی در چارچوب سه‌جسمی مداری، به این خلأ پاسخ دهد.
