\chapter{یادگیری تقویتی چندعاملی}
کاربردهای پیچیده در یادگیری تقویتی نیازمند اضافه کردن چندین عامل\LTRfootnote{Multi-Agent} برای انجام همزمان وظایف مختلف هستند.
با این حال، افزایش تعداد عامل‌ها چالش‌هایی در مدیریت تعاملات میان آن‌ها به همراه دارد.
در این فصل، بر اساس مسئله بهینه‌سازی برای هر عامل، مفهوم تعادل\LTRfootnote{Equilibrium} معرفی شده تا رفتارهای توزیعی چندعاملی را تنظیم کند.
رابطه رقابت میان عامل‌ها در سناریوهای مختلف تحلیل شده و آن‌ها با الگوریتم‌های معمول یادگیری تقویتی چندعاملی ترکیب شده‌اند. بر اساس انواع تعاملات، یک چارچوب نظریه بازی برای مدل‌سازی عمومی در سناریوهای چندعاملی استفاده شده است. با تحلیل بهینه‌سازی و وضعیت تعادل برای هر بخش از چارچوب، سیاست بهینه یادگیری تقویتی چندعاملی برای هر عامل بررسی شده است. در این فصل ابتدا در بخش \ref{sec:marl_definitions} مفاهیم اولیه یادگیری تقویتی چندعاملی معرفی شده است.
 سپس در بخش \ref{sec:marl_games} انواع بازی‌ها و تعادل نش مورد بررسی قرار گرفته است.
الگوریتم‌های مختلف یادگیری تقویتی چندعاملی شامل
 \lr{MA-DDPG}
 در بخش
  \ref{sec:marl_maddpg}،
   \lr{MA-TD3}
    در بخش \ref{sec:MATD3}،
    \lr{MA-SAC} در بخش \ref{sec:MASAC} و \lr{MA-PPO} در بخش \ref{sec:MAPPO} معرفی و بررسی شده‌اند.
  \input{Chapters/MARL/def}
  % \input{Chapters/MARL/MARL_importance}
    \input{Chapters/MARL/games}
    \input{Chapters/MARL/nash}
    \input{Chapters/MARL/zero_sum}
    \input{Chapters/MARL/MADDPG}
    \input{Chapters/MARL/MATD3}
    \input{Chapters/MARL/MASAC}
     \input{Chapters/MARL/MAPPO}
%    \input{Chapters/MARL/safe}
%    \input{Chapters/MARL/algorithms}
%       \section{تنظیمات آزمایشی}
%    \section{نتایج عملکرد الگوریتم‌ها}
%    \section{تحلیل پایداری و همگرایی}
%    \section{مقایسه روش‌های تک‌عاملی و چندعاملی}
%    \section{ارزیابی مقاومت الگوریتم‌ها در برابر اختلالات}
%    \section{تحلیل آماری نتایج}
%    \section{بحث و تفسیر نتایج}
%    \section{مقایسه با معیارهای مرجع}
%    \section{جمع‌بندی و پیشنهادات آتی}
