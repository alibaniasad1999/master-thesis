\chapter{یادگیری تقویتی چندعاملی}\label{ch:marl}
کاربردهای پیچیده در یادگیری تقویتی نیازمند اضافه کردن چندین عامل\LTRfootnote{Multi-Agent} برای انجام همزمان وظایف مختلف هستند.
با این حال، افزایش تعداد عامل‌ها چالش‌هایی در مدیریت تعاملات میان آن‌ها به همراه دارد.
در این فصل، بر اساس مسئله بهینه‌سازی برای هر عامل، مفهوم تعادل نش\LTRfootnote{Nash Equilibrium} معرفی شده تا رفتارهای توزیعی چندعاملی را تنظیم کند.
رابطه رقابت میان عامل‌ها در سناریوهای مختلف تحلیل شده و آن‌ها با الگوریتم‌های معمول یادگیری تقویتی چندعاملی ترکیب شده‌اند. بر اساس انواع تعاملات، یک چارچوب نظریه بازی برای مدل‌سازی عمومی در سناریوهای چندعاملی استفاده شده است. با تحلیل بهینه‌سازی و وضعیت تعادل برای هر بخش از چارچوب، سیاست بهینه یادگیری تقویتی چندعاملی برای هر عامل بررسی شده است. در این فصل ابتدا در بخش \ref{sec:marl_definitions} مفاهیم اولیه‌ی یادگیری تقویتی چندعاملی معرفی می‌شوند،
 سپس در بخش \ref{sec:marl_games} انواع بازی‌ها و تعادل نش مورد بررسی قرار می‌گیرند.
الگوریتم‌های مختلف یادگیری تقویتی چندعاملی شامل
 \lr{MA-DDPG}
 در بخش
  \ref{sec:marl_maddpg}،
   \lr{MA-TD3}
    در بخش \ref{sec:MATD3}،
    \lr{MA-SAC} در بخش \ref{sec:MASAC} و \lr{MA-PPO} در بخش \ref{sec:MAPPO} معرفی و بررسی شده‌اند.
  \input{Chapters/MARL/def}
    \input{Chapters/MARL/games}
    \input{Chapters/MARL/nash}
    \input{Chapters/MARL/zero_sum}
    \input{Chapters/MARL/MADDPG}
    \input{Chapters/MARL/MATD3}
    \input{Chapters/MARL/MASAC}
     \input{Chapters/MARL/MAPPO}
