\subsection{تعادل نش}
تعادل نش\LTRfootnote{Nash Equilibrium}
 یکی از بنیادی‌ترین مفاهیم در نظریه‌ی بازی‌ها است که توسط جان نش در سال 1950 معرفی شد. این مفهوم به ترکیب\LTRfootnote{Profile}
  سیاست‌ها اشاره دارد که در آن هیچ بازیکنی نمی‌تواند با تغییر یک‌جانبه‌ی سیاست خود، سود بیشتری به دست آورد (در حالی‌که سیاست‌های سایر بازیکنان ثابت است).

\begin{itemize}
	\item \textbf{تعریف تعادل نش:}
	فرض کنید یک بازی با \( n \) بازیکن داریم. هر بازیکن \( i \) دارای مجموعه‌ی سیاست‌های \( \Pi_i \) و تابع مطلوبیت \( u_i: \Pi_1 \times \Pi_2 \times \cdots \times \Pi_n \rightarrow \mathbb{R} \) است. یک ترکیب سیاست \( \pi^* = (\pi_1^*, \pi_2^*, \ldots, \pi_n^*) \) تعادل نش نامیده می‌شود اگر برای هر بازیکن \( i \) و هر سیاست \( \pi_i \in \Pi_i \)
	 در وضعیت \(\boldsymbol{s}\)
	 داشته باشیم:
\begin{equation}
		u_i(\pi_i^*, \pi_{-i}^*, \boldsymbol{s}) \geq u_i(\pi_i, \pi_{-i}^*, \boldsymbol{s})
\end{equation}
	
	در اینجا، \( \pi_{-i}^* \) نشان‌دهنده‌ی سیاست‌های همه‌ی بازیکنان به جز بازیکن \( i \) است. در ادامه‌ی این پژوهش و به‌منظور به‌کارگیری چارچوب نظریه‌ی بازی در یادگیری تقویتی، مطلوبیت هر عامل به‌صورت برابر با تابع ارزش او در حالت \(\boldsymbol{s}\) در نظر گرفته می‌شود:
	\(u_i(\pi_i, \pi_{-i}, \boldsymbol{s}) = V_i^{\pi_i, \pi_{-i}}(\boldsymbol{s})\).
\end{itemize}

%\subsubsection{تعادل نش در بازی‌های مارکوفی و صورت‌بندی در MARL}
%\paragraph{نمادگذاری و تعاریف.}
%فرض کنید مجموعه‌ی حالات و کنش‌ها متناهی باشند. بازی مارکوفیِ \(n\)-نفره را به‌صورت
%\(\mathcal{G}=(\mathcal{S},\{\mathcal{A}_i\}_{i=1}^n, P, \{r_i\}_{i=1}^n, \gamma)\) با \(\gamma\in(0,1)\) در نظر بگیرید.
%مجموعه‌ی کنش‌های مشترک 
%\(\mathcal{A}\coloneqq \mathcal{A}_1\times\cdots\times \mathcal{A}_n\) 
%و کنش مشترک را با \(\mathbf{a}=(a_1,\ldots,a_n)\) نشان می‌دهیم.
%برای هر مجموعه‌ی متناهی \(X\)، $\Delta(X)$ مجموعه‌ی تمام توزیع‌های احتمالی روی $X$ است؛ 
%یعنی همه‌ی توابعی که به هر عضو $X$ عددی نامنفی نسبت می‌دهند 
%به‌طوری‌که جمع آن‌ها برابر ۱ باشد.
%
%سیاست ایستای مختلطِ عامل \(i\) نگاشتی است \(\pi_i:\mathcal{S}\to \Delta(\mathcal{A}_i)\) و \(\Pi_i\) مجموعه‌ی کلیه‌ی این سیاست‌ها است.
%همچنین \(\Pi\coloneqq \Pi_1\times\cdots\times \Pi_n\) و \(\pi=(\pi_1,\ldots,\pi_n)\in\Pi\) یک ترکیب سیاست است.
%با \(\pi_{-i}\) ترکیب همه‌ی عوامل به‌جز \(i\) را نشان می‌دهیم.
%پاسخ-بهترینِ عامل \(i\) یک ترسیم چندارزشی \(\mathrm{BR}_i:\Pi_{-i}\rightrightarrows \Pi_i\) است که به‌صورت زیر تعریف می‌شود:
%\begin{equation}
%\mathrm{BR}_i(\pi_{-i}) \;\in\; \arg\max_{\pi_i\in\Pi_i}\; V_i^{(\pi_i,\pi_{-i})}.
%\end{equation}
%
%اپراتور بلمن برای سیاستِ ثابت \(\pi\) و عامل \(i\) به‌شکل
%\begin{equation}
%(\mathcal{T}_i^{\pi}V)(s) \;=\; \mathbb{E}_{\mathbf{a}\sim \pi(\cdot|s)}\!\big[r_i(s,\mathbf{a})+\gamma\,\mathbb{E}_{s'\sim P(\cdot|s,\mathbf{a})}V(s')\big]
%\end{equation}
%تعریف می‌شود که روی نرمِ \(\ell_\infty\) انقباضی است و بنابراین \(V_i^{\pi}\) تنها نقطه‌ی ثابت آن است.
%یک \(\varepsilon\)-تعادل نش ترکیبی است \(\pi^\varepsilon\) که برای هر \(i\) و هر \(s\):
%\begin{equation}
%V_i^{(\pi_i^\varepsilon,\pi_{-i}^\varepsilon)}(s)\;\ge\; \sup_{\pi_i\in\Pi_i}V_i^{(\pi_i,\pi_{-i}^\varepsilon)}(s) - \varepsilon.
%\end{equation}
%
%یک بازی مارکوفیِ \(n\)-نفره را به‌صورت
%\(\mathcal{G}=(\mathcal{S},\{\mathcal{A}_i\}_{i=1}^n, P, \{r_i\}_{i=1}^n, \gamma)\)
%در نظر بگیرید؛ که در آن \(\mathcal{S}\) مجموعه‌ی حالات، \(\mathcal{A}_i\) مجموعه‌ی کنش‌های عامل \(i\)،
%\(P(s'|s,a_1,\ldots,a_n)\) دینامیک سیستم، \(r_i(s,a_1,\ldots,a_n)\) تابع پاداش، و \(\gamma\in(0,1)\) ضریب تنزیل است.
%سیاستِ ایستای مختلطِ عامل \(i\) نگاشتی است از حالات به توزیع روی کنش‌ها: \(\pi_i(\cdot|s)\in \Delta(\mathcal{A}_i)\).
%به‌ازای سیاست مشترک \(\pi=(\pi_1,\ldots,\pi_n)\)، توابع ارزش و ارزش-کنش به صورت زیر تعریف می‌شوند:
%\begin{align}
%V_i^{\pi}(s) &\;=\; \mathbb{E}_{\pi,P}\Big[\sum_{t=0}^{\infty}\gamma^t\, r_i(S_t,A_{1,t},\ldots,A_{n,t})\;\big|\;S_0=s\Big], \\
%Q_i^{\pi}(s,a_1,\ldots,a_n) &\;=\; r_i(s,\mathbf{a})+\gamma\,\mathbb{E}_{s'\sim P(\cdot|s,\mathbf{a})}\big[V_i^{\pi}(s')\big].
%\end{align}
%
%\paragraph{تعریف تعادل نشِ ایستا در MARL.}
%سیاستِ \(\pi^*=(\pi_1^*,\ldots,\pi_n^*)\) یک تعادل نشِ ایستا نامیده می‌شود هرگاه برای همه‌ی عوامل \(i\) و همه‌ی حالات \(s\):
%\begin{equation}
%V_i^{(\pi_i^*,\pi_{-i}^*)}(s)\;\ge\; V_i^{(\pi_i,\pi_{-i}^*)}(s)\quad \forall\, \pi_i \in \Pi_i,
%\end{equation}
%یا معادل آن، با عملگر بهترین پاسخ:
%\begin{equation}
%\pi_i^* \;\in\; \mathrm{BR}_i(\pi_{-i}^*) \quad \text{که}\quad
%\mathrm{BR}_i(\pi_{-i})(s)\in \arg\max_{\pi_i(\cdot|s)\in\Delta(\mathcal{A}_i)} \mathbb{E}_{a_{-i}\sim \pi_{-i}(\cdot|s)}\!\Big[\sum_{a_i}\pi_i(a_i|s)\,Q_i^{(\pi_i,\pi_{-i})}(s,a_i,a_{-i})\Big].
%\end{equation}
%
%\paragraph{وجود تعادل.}
%برای بازی‌های دو-نفره‌ی مجموع‌صفرِ تنزیلی، وجود مقدار و سیاست‌های ایستای بهینه تضمین می‌شود \LTRfootnote{Shapley, 1953}.
%برای بازی‌های کلیِ تنزیلی، وجود تعادل نشِ ایستا (با اختلاط) تضمین می‌شود \LTRfootnote{Fink, 1964}.
%
%\subsubsection{ویژگی‌ها و صورت‌بندی‌های بلمن/شابلی}
%
%\paragraph{حالتِ دو-نفره مجموع‌صفر:} 
%بگذارید \(Q(s)\in \mathbb{R}^{|\mathcal{A}_1|\times|\mathcal{A}_2|}\) ماتریسی باشد با درایه‌های
%\(Q(s)_{a_1,a_2} = r(s,a_1,a_2)+\gamma\,\mathbb{E}_{s'\sim P(\cdot|s,a_1,a_2)}[V(s')]\).
%اپراتورِ شابلی به صورت زیر تعریف می‌شود:
%\begin{equation}
%(\mathcal{T}V)(s) \;=\; \max_{\sigma\in\Delta(\mathcal{A}_1)}\;\min_{\tau\in\Delta(\mathcal{A}_2)}\;\sum_{a_1,a_2}\sigma(a_1)\tau(a_2)\,Q_V(s)_{a_1,a_2},
%\end{equation}
%که \(Q_V(s)_{a_1,a_2}=r(s,a_1,a_2)+\gamma\,\mathbb{E}_{s'\sim P(\cdot|s,a_1,a_2)}[V(s')]\).
%بردار \(V^*\) تنها نقطه‌ی ثابتِ \(\mathcal{T}\) بوده و سیاست‌های تعادلی از زین-نقطه‌ی بازی ماتریسیِ هر حالت به‌دست می‌آیند.
%
%\paragraph{حالتِ کلی:} 
%در تعادل \(\pi^*\)، روابط سازگاری زیر برقرار است:
%\begin{align}
%Q_i^{\pi^*}(s,\mathbf{a}) &= r_i(s,\mathbf{a})+\gamma\,\mathbb{E}_{s'\sim P(\cdot|s,\mathbf{a})}\big[V_i^{\pi^*}(s')\big],\\
%V_i^{\pi^*}(s) &= \mathbb{E}_{\mathbf{a}\sim \pi^*(\cdot|s)}\big[Q_i^{\pi^*}(s,\mathbf{a})\big],\\
%\pi_i^*(a_i|s)>0 \;\Rightarrow\;
%a_i &\in \arg\max_{a'_i}\;\mathbb{E}_{a_{-i}\sim \pi_{-i}^*(\cdot|s)}\big[Q_i^{\pi^*}(s,a'_i,a_{-i})\big]\quad (\text{شرطِ پشتیبان}).
%\end{align}
%
%\subsubsection{روش‌های دستیابی به تعادل نش در MARL}
%\paragraph{۱) برنامه‌ریزی مدل‌محور.}
%\begin{itemize}
%    \item \textbf{دو-نفره مجموع-صفر (شابلی-تکرارِ مقدار):} برای هر \(k\),
%    \begin{align}
%    Q_{k}(s,\mathbf{a}) &\leftarrow r(s,\mathbf{a})+\gamma\,\mathbb{E}_{s'\sim P(\cdot|s,\mathbf{a})}[V_k(s')],\\
%    V_{k+1}(s) &\leftarrow \max_{\sigma\in\Delta(\mathcal{A}_1)}\min_{\tau\in\Delta(\mathcal{A}_2)}\; \sigma^\top Q_k(s)\,\tau.
%    \end{align}
%    در هر حالت یک بازی ماتریسی حل می‌شود (مثلاً با استفاده از برنامه‌ریزی خطی).
%
%    \item \textbf{کلیِ جمع-غیرصفر (تکرار ارزش نش):} مشابه روش بالا، اما در هر حالت با ماتریس‌های \(Q_{i,k}(s)\) یک تعادل نشِ مرحله‌ای محاسبه و سپس
%    \begin{equation}
%    V_{i,k+1}(s) \leftarrow \mathbb{E}_{\mathbf{a}\sim \pi_k(\cdot|s)}[Q_{i,k}(s,\mathbf{a})].
%    \end{equation}
%    همگرایی این روش تنها تحت فروضی مانند یکتایی تعادل مرحله‌ای تضمین‌پذیر است.
%\end{itemize}
%
%\paragraph{۲) یادگیری مدل‌نا‌آگاه (جدول‌نگار).}
%\begin{itemize}
%    \item \textbf{Minimax-Q (دو-نفره مجموع-صفر):}
%    \begin{align}
%    Q_{t+1}(s,a_1,a_2) &\leftarrow (1-\alpha_t)Q_t(s,a_1,a_2) + \alpha_t\big[r(s,a_1,a_2) + \gamma\,V_t(s')\big],\\
%    V_t(s') &= \max_{\sigma\in\Delta(\mathcal{A}_1)}\min_{\tau\in\Delta(\mathcal{A}_2)}\sigma^\top Q_t(s')\,\tau.
%    \end{align}
%    اعمال با راهبرد تعادلیِ بازیِ \(Q_t(s)\) انتخاب می‌شوند تا همگرایی به سیاست بهینه تضمین شود.
%
%    \item \textbf{Nash-Q (جمع-غیرصفر):}
%    \begin{align}
%    Q_{i,t+1}(s,\mathbf{a}) &\leftarrow (1-\alpha_t)Q_{i,t}(s,\mathbf{a}) + \alpha_t\big[r_i(s,\mathbf{a}) + \gamma\,V_{i,t}(s')\big],\\
%    V_{i,t}(s) &= \mathbb{E}_{\mathbf{a}\sim \pi_t(\cdot|s)}[Q_{i,t}(s,\mathbf{a})],
%    \end{align}
%    که \(\pi_t(\cdot|s)\) یک تعادل نشِ بازی مرحله‌ایِ القاشده توسط \(Q_t(s)\) است. همگرایی این روش محدود و متکی به فروض یکتایی تعادل و اکتشاف کافی محیط است.
%\end{itemize}
%
%\paragraph{۳) گرادیان‌سیاست/اکتور-کریتیک.}
%با سیاست‌های قابل‌تفاضل \(\pi_\theta\) و استفاده از آنتروپی یا منظم‌سازی مناسب:
%\begin{itemize}
%    \item \textbf{گرادیان‌بازیِ همزمان:} بر روی تابع هدف \(J_i(\theta)=\mathbb{E}[\sum_t \gamma^t r_i]\) با روش‌های پایدارسازی مانند extra-gradient یا OGDA به‌ویژه در بازی‌های دو-نفره مجموع-صفر به نقطه‌ی زین همگرا می‌شود.
%    \item در بازی‌های پتانسیلی یا تک‌سویه‌یکنواخت، همگرایی به تعادل نش تقویت می‌گردد.
%    \item استفاده از کریتیکِ مرکزی برای برآورد گرادیان‌ها در این روش‌ها متداول است و کارایی الگوریتم را افزایش می‌دهد.
%\end{itemize}
%
%\paragraph{۴) پویایی‌های بی‌حسرت.}
%\begin{itemize}
%    \item \textbf{در بازی‌های دو-نفره مجموع-صفر:} الگوریتم‌های بی‌حسرت (مانند mirror descent یا optimistic descent) در میانگین زمان به تعادل مینیمکس همگرا می‌شوند.
%    \item \textbf{در بازی‌های جمع-غیرصفر:} این الگوریتم‌ها معمولاً به تعادل همبسته‌ی خشن (coarse correlated equilibrium) می‌رسند و برای رسیدن به تعادل نش نیاز به اعمال قیود یا منظم‌سازی خاص (مثلاً پاسخ-بهترین لگیتی با دمای رو به صفر) و روش‌های هوشمندانه انتخاب تعادل است.
%\end{itemize}
%
%\paragraph{۴) پویایی‌های بی‌حسرت.}
%\begin{itemize}
%    \item \textbf{در بازی‌های دو-نفره مجموع-صفر:} الگوریتم‌های بی‌حسرت (مانند mirror descent یا optimistic descent) در میانگین زمان به تعادل مینیمکس همگرا می‌شوند.
%    
%    \item \textbf{در بازی‌های جمع-غیرصفر:} این الگوریتم‌ها معمولاً به تعادل همبسته‌ی خشن (coarse correlated equilibrium) می‌رسند و برای رسیدن به تعادل نش نیاز به اعمال قیود یا منظم‌سازی خاص (مثلاً پاسخ-بهترین لگیتی با دمای رو به صفر) و روش‌های هوشمندانه انتخاب تعادل است.
%\end{itemize}
%
%\paragraph{نکات عملی.}
%\begin{itemize}
%    \item \textbf{انتخاب تعادل:} در حضور چندین تعادل نش، استفاده از منظم‌سازی (مانند آنتروپی یا روش لگیستیک) و استراتژی گرمادادن و سپس سرد کردن تدریجی (annealing) به انتخاب پایدارترین تعادل کمک می‌کند.
%    
%    \item \textbf{اکتشاف و تقریب تابع:} تنظیم دقیق نرخ‌های یادگیری، اطمینان از پوشش کافی فضای حالت، و برآورد دقیق توابع \(Q\) و \(V\) برای دستیابی به نتایج قابل اعتماد حیاتی هستند.
%    
%    \item \textbf{مدل‌سازی رقیب:} استفاده از روش‌های برون‌خط و برخط مدل‌سازی رقیب (opponent modeling) می‌تواند پایداری و کارایی الگوریتم‌های یادگیری را به طور قابل توجهی بهبود بخشد.
%\end{itemize}



