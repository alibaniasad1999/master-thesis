%\subsection{بازی مجموع صفر}
%
%بازی‌های مجموع صفر\LTRfootnote{Zero-Sum Games}
% دسته‌ای از بازی‌ها هستند که در آن‌ها تابع ارزش یک بازیکن دقیقاً برابر با ضرر بازیکن دیگر است. به عبارت دیگر، مجموع ارزشهای همه بازیکنان در هر مرحله صفر است.
%
%
%
%\begin{itemize}
%	\item تعریف بازی مجموع صفر:
%	در یک بازی دو نفره، اگر تابع ارزش بازیکن اول (\( V_1^{(\pi_1 ,\pi_2)}(s)
%	\)) و بازیکن دوم (\( V_2^{(\pi_1 ,\pi_2)}(s)
%	\)) به‌گونه‌ای باشد که برای هر مجموعه سیاست
%	\( (\pi_1, \pi_2) \) 
%به صورت زیر باشد را یک بازی مجموع صفر نامیده می‌شود.
%	\begin{equation}\label{eq:game_v}
%		V_1^{(\pi_1 ,\pi_2)}(s) + V_2^{(\pi_1 ,\pi_2)}(s) = 0 \to V_1^{(\pi_1 ,\pi_2)}(s) = -V_2^{(\pi_1 ,\pi_2)}(s)
%%		V_1^{(\pi_1 ,\pi_2)}(s) =- V_2^{(\pi_1, \pi_2)}(s)
%	\end{equation}
%	\item سیاست بهینه در بازی مجموع صفر:
%	در بازی‌های مجموع صفر، سیاست بهینه هر بازیکن، انتخابی است که  تابع ارزش خود را در برابر بهترین پاسخ حریف به حداکثر برساند. این سیاست اغلب به تعادل نش منجر می‌شود. سیاست بهینه دو بازیکن در بازی مجموع صفر با تابع  ارزش معادله
%	\eqref{eq:game_v}
%	به صورت زیر است.
%	
%	\begin{align}
%		V_1^*(s) = \max_{\pi_1} \min_{\pi_2} V_1^{(\pi_1 ,\pi_2)}(s) \\
%		V_2^*(s) = \max_{\pi_2} \min_{\pi_1} V_2^{(\pi_1 ,\pi_2)}(s)
%	\end{align}
%	
%	
%\end{itemize}


















\subsection{بازی مجموع صفر}

بازی‌های مجموع صفر\LTRfootnote{Zero-Sum Games}
دسته‌ای از بازی‌ها هستند که در آن‌ها تابع ارزش یک بازیکن دقیقاً برابر با ضرر بازیکن دیگر است؛ ازاین‌رو، مجموع ارزش‌های همه‌ی بازیکنان در هر وضعیت صفر خواهد بود.

\begin{itemize}
	%------------------------------------
	\item \textbf{تعریف بازی مجموع صفر:}
	
	در یک بازی دو نفره، اگر تابع ارزشِ حالت (value) بازیکن اوّل 
	\(V_1^{(\pi_1 ,\pi_2)}(s)\)
	و بازیکن دوم 
	\(V_2^{(\pi_1 ,\pi_2)}(s)\)
	برای هر مجموعه سیاست 
	\((\pi_1,\pi_2)\)
	به‌گونه‌ای باشند که:
	\begin{equation}\label{eq:game_v}
		V_1^{(\pi_1 ,\pi_2)}(s) + V_2^{(\pi_1 ,\pi_2)}(s) = 0 
		\;\;\Longrightarrow\;\;
		V_1^{(\pi_1 ,\pi_2)}(s) = -\,V_2^{(\pi_1 ,\pi_2)}(s),
	\end{equation}
	آنگاه آن بازی را {بازی مجموع صفر} می‌نامیم.
	
	به‌طور مشابه، اگر تابع ارزش–عمل برای دو بازیکن را با
	\(Q_1^{(\pi_1,\pi_2)}(s,a_1,a_2)\)
	و
	\(Q_2^{(\pi_1,\pi_2)}(s,a_1,a_2)\)
	نشان دهیم، باید برقرار باشد:
	\begin{equation}\label{eq:game_q}
		Q_1^{(\pi_1,\pi_2)}(s,a_1,a_2) + 
		Q_2^{(\pi_1,\pi_2)}(s,a_1,a_2) = 0
		\;\;\Longrightarrow\;\;
		Q_1^{(\pi_1,\pi_2)}(s,a_1,a_2) = -\,Q_2^{(\pi_1,\pi_2)}(s,a_1,a_2).
	\end{equation}
	%------------------------------------
	
	\item \textbf{سیاست بهینه در بازی مجموع صفر:}
	
	در این بازی‌ها، هر بازیکن سیاستی را برمی‌گزیند که تابع ارزش خود را
	در برابر بهترین پاسخِ حریف بیشینه کند؛ این انتخاب در نهایت به
	تعادل نش منجر می‌شود.
	
	به‌صورت تابع ارزشِ حالت:
	\begin{align}
		V_1^*(s) &= \max_{\pi_1}\,\min_{\pi_2} \;
		V_1^{(\pi_1 ,\pi_2)}(s), \\
		V_2^*(s) &= \max_{\pi_2}\,\min_{\pi_1} \;
		V_2^{(\pi_1 ,\pi_2)}(s).
	\end{align}
	
	و به‌صورت تابع ارزش–عمل:
	\begin{align}
		Q_1^*(s,a_1,a_2) &= \max_{\pi_1}\,\min_{\pi_2} \;
		Q_1^{(\pi_1 ,\pi_2)}(s,a_1,a_2), \\
		Q_2^*(s,a_1,a_2) &= \max_{\pi_2}\,\min_{\pi_1} \;
		Q_2^{(\pi_1 ,\pi_2)}(s,a_1,a_2).
	\end{align}
	\item \textbf{تابع پاداش:}
	تابع پاداش
	در بازی‌های دوسویه مجموع‌صفر باید به‌گونه‌ای طراحی شود که پاداش لحظه‌ای دو عامل در هر گام جمعاً صفر باشد. در ادامه ساختار پاداش عاملِ ۱ مشابه قالب تک‌عاملی تعریف می‌شود و پاداش عاملِ ۲ به‌صورت منفی آن اخذ می‌گردد.
	
	\begin{itemize}
		\item \textbf{پاداش نهایی برای دستیابی به هدفِ عامل ۱}: در صورت رسیدن به هدف عامل ۱، شبیه‌سازی پایان یافته و پاداش بزرگ مثبت به او داده می‌شود.
		\item \textbf{جریمه نهایی برای دور شدنِ عامل ۱}: اگر عامل ۱ از محدوده مجاز خود خارج شود، شبیه‌سازی خاتمه یافته و جریمه بزرگ منفی اعمال می‌گردد.
		\item \textbf{جریمه برای مصرف سوختِ عامل ۱}: استفاده بیش‌از‌حد از پیشرانه برای عامل ۱ با جریمه همراه است.
		\item \textbf{جریمه برای انحراف از مسیر مرجعِ عامل ۱}: انحراف از مسیر مرجع عامل ۱ باعث دریافت جریمه متناسب می‌شود.
	\end{itemize}
	
	تابع پاداش عامل ۱ به‌صورت زیر تعریف می‌شود:
	\[
	r_1(s, a_1, a_2) = r_{\text{thrust},1}(a_1) + r_{\text{thrust},1}(a_2) + r_{\text{reference},1}(s) + r_{\text{terminal},1}(s)
	\]
	
	که در آن مؤلفه‌ها عبارتند از:
	\begin{align}
		r_{\text{thrust},1}(a_1) &= -k_1 \cdot |a_1| \\
		r_{\text{thrust},1}(a_2) &= -k_2 \cdot |a_2| \\
		r_{\text{reference},1}(s) &= -k_3 \cdot d_1\!\big(s, s_{\text{ref},1}\big) \\
		r_{\text{terminal},1}(s) &=
		\begin{cases}
			+R_{\text{goal},1} & \text{if}~ s \in S_{\text{goal},1} \\
			-R_{\text{fail},1} & \text{if}~ d_1\!\big(s, s_{\text{ref},1}\big) > \epsilon_1 \\
			0 & \text{otherwise}
		\end{cases}
	\end{align}
	
	برای تضمین خاصیت مجموع‌صفر، پاداشِ عامل ۲ را در هر گام به‌صورت زیر تعریف می‌کنیم:
	\[
	r_2(s, a_1, a_2) = -\,r_1(s, a_1, a_2),
	\]
	بنابراین با افق و ضریب تنزیل یکسان، روابط \eqref{eq:game_v} و \eqref{eq:game_q} نیز برقرار خواهند بود.
	
	در این رابطه:
	\begin{enumerate}
		\item \(R_{\text{goal},1}\): پاداش بزرگ مثبت برای دستیابی عامل ۱ به هدف.
		\item \(R_{\text{fail},1}\): جریمه بزرگ منفی برای خروج عامل ۱ از محدوده مجاز.
		\item \(d_1(s, s')\): فاصله مرتبط با عامل ۱ اقلیدسی بین دو وضعیت.
	\end{enumerate}
	
	ضرایب \(k_1, k_2, k_3\) برای تنظیم تعادل بین جریمهٔ پیشرانهٔ عامل ۱، جریمهٔ پیشرانهٔ عامل ۲، و جریمهٔ انحراف از مسیر مرجع استفاده می‌شوند. به‌دلیل تعریف \(r_2=-r_1\)، جمع پاداش‌ها در هر گام صفر بوده و مقدار بازی یکتا و با تعادل نش در راهبردهای مختلط سازگار است.
	
	
	
\end{itemize}

% نکته‌ی وجود تعادل در بازی‌های متناهیِ مجموع‌صفر:
بر پایه‌ی قضیه‌ی کمینه‌بیشینه‌ی فون‌نویمان، در بازی‌های دوسویه‌ی مجموع‌صفرِ متناهی داریم:
\[
\max_{\pi_1}\min_{\pi_2} V_1^{(\pi_1,\pi_2)}(s)
=
\min_{\pi_2}\max_{\pi_1} V_1^{(\pi_1,\pi_2)}(s),
\]
که وجود تعادل نش در راهبردهای مختلط و یکتایی مقدار بازی را تضمین می‌کند.

\subsection{چالش‌های استایی در  یادگیری تقویتی تک‌عاملی}
%\subsubsection{چالش غیراستایی در محیط‌های چندعاملی و راهکار MA-DDPG}

در محیط‌های چندعاملی، رفتار سایر عامل‌ها هم‌زمان با فرایند یادگیری در حال تغییر است؛ بنابراین توزیع انتقال $p(s'|s,a)$ از دید هر عامل ثابت باقی نمی‌ماند. در نتیجه، محیط از منظر عامل غیراستا تلقی می‌شود، زیرا یکی از فرض‌های بنیادی در روش‌های یادگیری تقویتی تک‌عاملی مانند \lr{DDPG}، {ثبات دینامیک محیط در طول یادگیری} است. نقض این فرض باعث ناپایداری در برآورد تابع ارزش، انحراف در به‌روزرسانی گرادیان سیاست و در نهایت همگرایی ضعیف یا شکست فرایند یادگیری می‌گردد.

برای مقابله با این مسئله، چارچوب
 \lr{MA-DDPG}
از رویکرد \lr{Centralized Training with Decentralized Execution (CTDE)} بهره می‌گیرد. در مرحله آموزش، هر عامل علاوه بر مشاهدات محلی خود، به اطلاعات حالت و اقدامات سایر عامل‌ها نیز دسترسی دارد و منتقد هر عامل  تابع ارزش را  به‌درستی تخمین می‌زند.
این ساختار به منتقد اجازه می‌دهد {تغییرات سیاست سایر عامل‌ها را در مدل ارزش منعکس کرده و نوسانات ناشی از غیراستایی محیط را کاهش دهد}. بدین ترتیب، تابع ارزش در طول یادگیری نسبت به تغییر رفتار عامل‌های دیگر حساسیت کمتری خواهد داشت.

در مرحله اجرا، هر عامل تنها بر اساس مشاهدات محلی خود عمل می‌کند و سیاست‌ها به‌صورت غیرمتمرکز اعمال می‌شوند. این تفکیک آموزش/اجرا موجب می‌شود الگوریتم ضمن بهره‌گیری از مزیت اطلاعات کامل در آموزش، همچنان برای کاربرد در محیط‌های واقعی و قابل‌استقرار روی سامانه‌های درون‌برد مناسب باقی بماند.

مزیت اصلی \lr{CTDE} در \lr{MA-DDPG} را می‌توان به صورت زیر خلاصه کرد:
\begin{itemize}
	\item کاهش \textbf{غیراستایی ادراک شده} از طریق یادگیری تابع ارزش با اطلاعات جامع محیط؛
	\item بهبود پایداری و همگرایی به دلیل کاهش خطای \lr{Non-Stationary Target Shift}؛
	\item امکان اجرای سبک و مستقل عامل‌ها بدون نیاز به اشتراک اطلاعات در زمان اجرا.
\end{itemize}

شواهد تجربی پژوهش حاضر نیز نشان می‌دهد که نسخه \lr{MA-DDPG} نسبت به \lr{DDPG} تک‌عاملی در سناریوهای دارای اغتشاشات تصادفی و عدم‌قطعیت‌های مدل، از پایداری و همگرایی به‌مراتب بهتری برخوردار است.


در محیط‌های چند­عاملی، سیاست هر عامل مدام در حال تغییر است، که باعث می‌شود محیط از دید هر عامل غیرایستا\LTRfootnote{Non-stationary} شود. این مسئله چالش بزرگی برای الگوریتم‌های یادگیری تقویتی تک‌عاملی مانند \lr{DDPG} ایجاد می‌کند، زیرا فرض ایستایی محیط را نقض می‌کند.

\lr{MA-DDPG} با استفاده از رویکرد آموزش متمرکز، اجرای غیرمتمرکز\LTRfootnote{Centralized Training, Decentralized Execution} این مشکل را حل می‌کند. در این رویکرد، هر عامل در زمان آموزش به اطلاعات کامل محیط دسترسی دارد، اما در زمان اجرا تنها از مشاهدات محلی خود استفاده می‌کند.

\subsection{ضرورت استفاده از یادگیری تقویتی چندعاملی}

در نگاه اول به‌نظر می‌رسد که می‌توان مسئله هدایت فضاپیما را در چارچوب یادگیری تقویتی تک‌عاملی مدل نمود و تمامی عدم‌قطعیت‌ها و اغتشاشات محیطی را صرفاً به‌صورت نویز در دینامیک سیستم در نظر گرفت. با این حال، ماهیت دینامیک سه‌جسمی و حساسیت بالای مسیرهای ناپایدار در حوالی نقاط لاگرانژ، باعث می‌شود که تعامل میان فضاپیما و محیط، از منظر کنترلی بیش‌تر شبیه یک {بازی دیفرانسیلی خصمانه} باشد تا یک محیط ایستا با نویز تصادفی ساده. به‌عبارت دیگر، مزاحمت‌های محیطی، خطای تراستر، تأخیر حسگر و عدم‌تطابق مدل، همگی در جهت تضعیف پایداری و افزایش انحراف مسیر عمل می‌کنند و می‌توان آن‌ها را به‌صورت یک عامل حریف مدل نمود که در پی بدتر کردن کارکرد سامانه است.

در روش‌های کلاسیک یادگیری تقویتی تک‌عاملی نظیر \lr{DDPG}، اغتشاشات معمولاً به‌صورت بخشی از نویز فرایند در نظر گرفته می‌شوند و عامل تنها با هدف بیشینه‌سازی بازده خود آموزش می‌بیند. این رویکرد اگرچه در شرایط بهینه عملکرد مناسبی دارد، اما تضمین قوی در برابر {بدترین‌حالت عدم‌قطعیت‌ها} ارائه نمی‌کند و سیاست به‌دست‌آمده ممکن است نسبت به سناریوهای شدید اغتشاش و خطای مدل بسیار حساس باشد. در مقابل، صورت‌بندی مسئله به‌عنوان یک بازی مجموع‌صفر فضاپیما–مزاحم و استفاده از چارچوب \lr{Multi-Agent RL} این امکان را فراهم می‌سازد که:
\begin{itemize}
	\item عامل کنترل‌کننده، سیاست خود را نه تنها برای یک مدل بهینه، بلکه در برابر {حریفی که فعالانه به‌دنبال بدتر کردن عملکرد است}، بهینه کند؛
	\item مفهوم {بهینگی مقاوم} (\lr{Robust Optimality}) به‌صورت درونی در فرایند یادگیری لحاظ شود و سیاست هدایت به‌گونه‌ای شکل گیرد که در سناریوهای بدترین‌حالت نیز پایداری مسیر و محدودیت‌های سوخت را حفظ نماید؛
	\item به‌کمک آموزش متمرکز و اجرای غیرمتمرکز (\lr{CTDE})، تابع ارزش با استفاده از اطلاعات کامل محیط و رفتار حریف تقریب زده شود و بدین ترتیب حساسیت به غیراستایی ناشی از تغییر سیاست‌ها کاهش یابد.
\end{itemize}

بدین ترتیب، استفاده از \lr{MARL} در این پژوهش صرفاً انتخابی تزئینی یا پیچیده‌سازی بی‌دلیل مسئله نیست، بلکه مستقیماً از {ماهیت بازی‌گونه و خصمانه‌ی تعامل فضاپیما با محیط} در دینامیک سه‌جسمی ناشی می‌شود. نتایج عددی ارائه‌شده در فصل~\ref{ch:results} نیز نشان می‌دهند که نسخه‌های چندعاملی پیشنهادشده، در مقایسه با نسخه‌های تک‌عاملی متناظر، در سناریوهای دارای تأخیر حسگر، نویز عملگر و عدم‌تطابق مدل، انحراف مسیر کمتر و مصرف سوخت بهینه‌تری را به‌همراه دارند. این تفاوت عملکرد، ضرورت استفاده از چارچوب یادگیری تقویتی چندعاملی را برای دستیابی به هدایت مقاوم و قابل‌اتکا در محیط‌های ناپایدارِ سه‌جسمی توجیه می‌کند.


%\subsection{تابع پاداش} \label{subsec:marl_reward}
