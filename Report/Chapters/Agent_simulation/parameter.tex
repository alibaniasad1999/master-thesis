\section{شبیه‌سازی عامل}\label{sec:agent_sim}

در این زیربخش، فرآیند شبیه‌سازی و آموزش عامل با استفاده از الگوریتم‌های یادگیری تقویتی پیشرفته ارائه شده است. تمرکز بر طراحی شبکه‌ها، منطق انتخاب الگوریتم‌ها، فراپارامترهای کلیدی و ملاحظات پایداری در حین آموزش است تا تکرارپذیری و دقت نتایج تضمین شود.

\subsection{پارامترهای یادگیری و منطق انتخاب الگوریتم‌ها}
الگوریتم‌های \lr{DDPG}، \lr{TD3}، \lr{SAC} و \lr{PPO} به دلیل کارایی در فضاهای کنش پیوسته و عملکرد پایدار در محیط‌های پیچیده انتخاب شده‌اند. به‌طور خلاصه:
\begin{itemize}
  \item \lr{DDPG}: سیاست قطعی با شبکه‌های هدف و میانگین پلیاک؛ مناسب محیط‌های پیوسته با هزینه محاسباتی پایین‌تر، اما حساس به نویز.
  \item \lr{TD3}: بهبود \lr{DDPG} با دو \lr{Critic}، هموارسازی سیاست هدف و به‌روزرسانی تأخیری سیاست؛ کاهش بیش‌براوردی \lr{Q} و پایداری بیشتر.
  \item \lr{SAC}: سیاست تصادفی بیشینه‌ساز آنتروپی با دمای \(\alpha\)؛ کاوش مؤثرتر و همگرایی پایدارتر در محیط‌های نویزی.
  \item \lr{PPO}: روش مبتنی بر سیاست با برش نسبت احتمال؛ به‌روزرسانی‌های ایمن و پیاده‌سازی ساده با کارایی تجربی بالا.
\end{itemize}
این الگوریتم‌ها به دلیل توانایی در مدیریت فضاهای پیوسته و عملکرد مؤثر در محیط‌های پیچیده انتخاب شده‌اند.

در شکل‌های \ref{fig:actor_nn} و \ref{fig:critic_nn} ساختار شبکه‌های \lr{Actor} و \lr{Critic} آورده شده است.
\input{Chapters/Agent_simulation/agent_nn}
\input{Chapters/Agent_simulation/critic_nn}






% جدول فشرده با کاهش فاصله‌ی عمودی و چهار ستون




% جدول چهارستونه فشرده با افزودن شبکه‌ی بازیگر–منتقد و تابع فعال‌سازی
\begin{table}[H]
	\centering
	% \footnotesize % کوچک‌تر کردن فونت در صورت نیاز
	\setlength{\tabcolsep}{8pt} % کاهش فاصله‌ی افقی میان ستون‌ها
	\renewcommand{\arraystretch}{0.95} % کاهش فاصله‌ی عمودی سطرها
	\begin{RTL}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{نام پارامتر} & \textbf{مقدار} & \textbf{نام پارامتر} & \textbf{مقدار} \\
			\hline
			گام در هر دوره یادگیری & $30\,000$ & تعداد دوره‌های یادگیری & $100$ \\
			اندازه‌ی مخزنِ تجربه & $10^{6}$ &	ضریب تنزیل \((\gamma)\)& $0.99$ \\
			ضریب میانگین پلیاک & $0.995$ & نرخِ یادگیریِ سیاست & $10^{-3}$ \\
			نرخِ یادگیریِ \lr{Q} & $10^{-3}$ & اندازه‌ی دسته & $1024$ \\
			گام‌ شروعِ استفاده از سیاست & $5\,000$ & گام شروعِ به‌روزرسانی& $1\,000$ \\
			فاصله‌ی به‌روزرسانی & $2\,000$ & نویز عمل & $0.1$ \\
			حداکثر طولِ رخداد & $6\,000$ & دستگاه & \lr{Cuda} \\
			اندازه شبکه‌ی \lr{Actor}
			& \((2^5, 2^5) \)  & تابع فعال‌سازی  \lr{Actor} & \lr{ReLU} \\
			اندازه شبکه‌ی \lr{Critic}
			& \( (2^5, 2^5) \)  & تابع فعال‌سازی  \lr{Critic} & \lr{ReLU} \\
			\hline
		\end{tabular}
	\end{RTL}
	\caption{جدول پارامترها و مقادیر پیش‌فرض الگوریتم \lr{DDPG}
		\cite{SpinningUp2018}}
\end{table}









\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{8pt}
	\renewcommand{\arraystretch}{0.95}
	\begin{RTL}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{نام پارامتر} & \textbf{مقدار} & \textbf{نام پارامتر} & \textbf{مقدار} \\
			\hline
			گام در هر دوره یادگیری & $30\,000$ & تعداد دوره‌های یادگیری & $100$ \\
			اندازه‌ی مخزنِ تجربه & $10^{6}$ &	ضریب تنزیل \((\gamma)\)& $0.99$ \\
			ضریب میانگین پلیاک & $0.995$ & نرخِ یادگیریِ سیاست & $10^{-3}$ \\
			نرخِ یادگیریِ \lr{Q} & $10^{-3}$ & اندازه‌ی دسته & $1024$ \\
			گام‌ شروعِ استفاده از سیاست & $5\,000$ & گام شروعِ به‌روزرسانی & $1\,000$ \\
			فاصله‌ی به‌روزرسانی & $2\,000$ & نویز عمل & $0.1$ \\
			نویز هدف & $0.2$ & برش نویز & $0.5$ \\
			تأخیر در به‌روزرسانی سیاست & $2$ &	حداکثر طولِ رخداد & $30\,000$  \\
			اندازه شبکه‌ی \lr{Actor} & \( (2^5, 2^5) \) & تابع فعال‌سازی \lr{Actor} & \lr{ReLU} \\
			اندازه شبکه‌ی \lr{Critic} & \( (2^5, 2^5) \) & تابع فعال‌سازی \lr{Critic} & \lr{ReLU} \\
			\hline
		\end{tabular}
	\end{RTL}
	\caption{جدول پارامترها و مقادیر پیش‌فرض الگوریتم \lr{TD3}
		\cite{SpinningUp2018}}
\end{table}



\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{8pt}
	\renewcommand{\arraystretch}{0.95}
	\begin{RTL}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{نام پارامتر} & \textbf{مقدار} & \textbf{نام پارامتر} & \textbf{مقدار} \\
			\hline
			گام در هر دوره یادگیری & $30\,000$ & تعداد دوره‌های یادگیری & $100$ \\
			اندازه‌ی مخزنِ تجربه & $10^{6}$ &	ضریب تنزیل \((\gamma)\)& $0.99$ \\
			ضریب میانگین پلیاک & $0.995$ & نرخِ یادگیری & $10^{-3}$ \\
			نرخ دمای آلفا & $0.2$ & اندازه‌ی دسته & $1024$ \\
			گام‌ شروعِ استفاده از سیاست & $5\,000$ & گام شروعِ به‌روزرسانی & $1\,000$ \\
			تعداد به‌روزرسانی در هر مرحله & $10$ & فاصله‌ی به‌روزرسانی & $2\,000$ \\
			تعداد اپیزودهای آزمون & $10$ & حداکثر طولِ رخداد & $30\,000$ \\
			اندازه شبکه‌ی \lr{Actor} & \( (2^5, 2^5) \) & تابع فعال‌سازی \lr{Actor} & \lr{ReLU} \\
			اندازه شبکه‌ی \lr{Critic} & \( (2^5, 2^5) \) & تابع فعال‌سازی \lr{Critic} & \lr{ReLU} \\
			\hline
		\end{tabular}
	\end{RTL}
	\caption{جدول پارامترها و مقادیر پیش‌فرض الگوریتم \lr{SAC} 
		\cite{SpinningUp2018}}
\end{table}



\begin{table}[H]
	\centering
	\setlength{\tabcolsep}{8pt}
	\renewcommand{\arraystretch}{0.95}
	\begin{RTL}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{نام پارامتر} & \textbf{مقدار} & \textbf{نام پارامتر} & \textbf{مقدار} \\
			\hline
			گام در هر دوره یادگیری & $30\,000$ & تعداد دوره‌های یادگیری & $100$ \\
			ضریب تنزیل \((\gamma)\) & $0.99$ & ضریب برش \(\text{clip ratio}\) & $0.2$ \\
			نرخِ یادگیریِ سیاست & $\!3\times\!10^{-4}$ & نرخِ یادگیریِ تابع ارزش & $10^{-3}$ \\
			تعداد تکرار آموزش سیاست & $80$ & تعداد تکرار آموزش ارزش & $80$ \\
			%				لامبدا برای GAE & $0.97$ & حداکثر طولِ رخداد & $1\,000$ \\
			اندازه شبکه‌ی \lr{Actor} & \( (2^5, 2^5) \) & تابع فعال‌سازی \lr{Actor} & \lr{ReLU} \\
			اندازه شبکه‌ی \lr{Critic} & \( (2^5, 2^5) \) & تابع فعال‌سازی \lr{Critic} & \lr{ReLU} \\
			\hline
		\end{tabular}
	\end{RTL}
	\caption{جدول پارامترها و مقادیر پیش‌فرض الگوریتم \lr{PPO}
		\cite{SpinningUp2018}}
\end{table}










\subsection{فرآیند آموزش}

رویه آموزش با \lr{PyTorch} و اجرای \lr{Cuda} به‌صورت زیر انجام شده است:
\begin{enumerate}
  \item گردآوری تجربه‌ی اولیه با سیاست تصادفی تا رسیدن به گام شروع به‌روزرسانی برای پرشدن اولیه‌ی مخزن تجربه.
  \item حلقه‌ی یادگیری: در هر گام، اجرای کنش، ذخیره‌ی چهار‌تایی‌ها \((s,a,r,s')\) (و در صورت نیاز \(d\) برای پایان اپیزود) در مخزن تجربه با ظرفیت \(10^6\).
  \item نمونه‌گیری دسته داده  و به‌روزرسانی \lr{Critic}ها با هدف‌های حاوی شبکه‌های هدف و میانگین پلیاک؛ در \lr{TD3}
   استفاده از دو شبکه \lr{Q} مستقل و هدف‌های کمینه‌شده.
  \item به‌روزرسانی \lr{Actor}: در \lr{DDPG}/\lr{TD3} بیشینه‌سازی \(\mathbb{E}_s[Q(s,\pi_\theta(s))]\) و در \lr{SAC} بیشینه‌سازی بازگشت انتروپی‌دار؛ در \lr{PPO} به‌روزرسانی برش‌خورده با نسبت احتمال.
  \item تکنیک‌های پایداری: \lr{Target networks} با پلیاک، \lr{reward/observation normalization}، هموارسازی هدف \lr{TD3}، \lr{gradient clipping} در صورت نیاز، و بذردهی ثابت برای تکرارپذیری.
  \item ارزیابی دوره‌ای: اجرای چند اپیزود آزمون بدون نویز کنش و ثبت بازگشت، نرخ موفقیت و واریانس.
\end{enumerate}

برای جلوگیری از بیش‌برازش و همگرایی زودرس، از نویز کاوش کنش و هموارسازی سیاست هدف (در \lr{TD3}) استفاده شده است. معیار توقف زمانی فعال می‌شود که نرخ موفقیت آزمون در چند پنجره‌ی پیاپی از 90٪ عبور کند و واریانس بازگشت کاهش یابد.

\subsubsection*{بهینه‌سازی و پس‌انتشار گرادیان}
محاسبه‌ی گرادیان‌ها با \lr{autograd} انجام شده است. به‌روزرسانی پارامترها با \lr{Adam}  
\cite{kingma2017adammethodstochasticoptimization}
بوده است که در عمل نسبت به گرادیان نزولی ساده پایدارتر است:
\begin{align}
  g_t &= \nabla_{\!w} L_t,\quad
  m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t,\quad
  v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \nonumber\\
  \hat{m}_t &= \frac{m_t}{1-\beta_1^t},\quad
  \hat{v}_t = \frac{v_t}{1-\beta_2^t},\quad
  w_{t+1} = w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}
که در آن \(\eta\) نرخ یادگیری، \(\beta_1,\beta_2\) ضرایب مومنتوم (\(0.9,\,0.999\)) و \(\epsilon\) برای پایدارسازی عددی است. به‌صورت مفهومی، زنجیره گرادیان نیز برقرار است:
\begin{equation}
  \nabla_{\!w} L = \frac{\partial L}{\partial y}\,\frac{\partial y}{\partial w}
\end{equation}

%\paragraph{معیارهای ارزیابی.}
%\begin{itemize}
%  \item بازگشت تجمعی میانگین و صدک‌ها در اپیزودهای آزمون بدون نویز.
%  \item نرخ موفقیت سناریوی مأموریت‌-محور (تعریف دامنه‌محور کار)، با پنجره‌ی میانگین‌گیری متحرک.
%  \item پایداری سیاست: واریانس بازگشت و تعداد نقض قیود (در صورت وجود قیود).
%\end{itemize}





%\subsubsection*{بهینه‌سازی و پس‌انتشار گرادیان}
%محاسبه‌ی گرادیان‌ها با استفاده از \lr{autograd} صورت می‌گیرد که به‌صورت خودکار قاعده‌ی زنجیره‌ای مشتق‌گیری را اعمال می‌کند. به‌روزرسانی پارامترها بر پایه‌ی الگوریتم \lr{Adam} انجام شده است که یک روش تطبیقی و پایدارتر نسبت به گرادیان نزولی ساده به‌شمار می‌رود. فرمول‌های به‌روزرسانی در Adam به‌صورت زیر است:
%\begin{align}
%	g_t &= \nabla_{\!w} L_t,\quad
%	m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t,\quad
%	v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \nonumber\\
%	\hat{m}_t &= \frac{m_t}{1-\beta_1^t},\quad
%	\hat{v}_t = \frac{v_t}{1-\beta_2^t},\quad
%	w_{t+1} = w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
%\end{align}

در این رابطه:
\begin{itemize}
	\item $L_t$: مقدار تابع هزینه (\lr{Loss}) در گام زمانی $t$.
	\item $w_t$: بردار وزن‌ها یا پارامترهای مدل در گام $t$.
	\item $g_t = \nabla_{\!w} L_t$: گرادیان تابع هزینه نسبت به پارامترها در زمان $t$.
	\item $m_t$: میانگین نمایی گرادیان‌ها (مومنتوم مرتبه اول) که حافظه‌ای از جهت گرادیان‌ها ایجاد می‌کند.
	\item $v_t$: میانگین نمایی مربعات گرادیان‌ها (مومنتوم مرتبه دوم) که بزرگی تغییرات گرادیان را ثبت می‌کند.
	\item $\hat{m}_t,\,\hat{v}_t$: نسخه‌های اصلاح‌شده‌ی بایاس برای $m_t$ و $v_t$ به‌منظور پایدارسازی در مراحل اولیه.
	\item $\eta$: نرخ یادگیری (\lr{Learning Rate}) که اندازه‌ی گام به‌روزرسانی وزن‌ها را مشخص می‌کند.
	\item $\beta_1,\,\beta_2$: ضرایب کاهش (\lr{Decay Rates}) برای میانگین‌گیری نمایی؛ مقادیر معمول آن‌ها به‌ترتیب $0.9$ و $0.999$ است.
	\item $\epsilon$: یک مقدار بسیار کوچک (معمولاً $10^{-8}$) برای جلوگیری از تقسیم بر صفر و افزایش پایداری عددی.
\end{itemize}

الگوریتم \lr{Adam} به این صورت عمل می‌کند که همزمان از میانگین مرتبه‌ی اول ($m_t$) برای جهت حرکت و از میانگین مرتبه‌ی دوم ($v_t$) برای تنظیم نرخ یادگیری هر پارامتر استفاده می‌کند. در نتیجه هم از نوسانات شدید جلوگیری می‌شود و هم فرآیند همگرایی سرعت می‌گیرد.  

از دیدگاه محاسبه‌ی گرادیان، زنجیره‌ی مشتق‌گیری (قاعده‌ی زنجیره‌ای) نیز برقرار است:
\begin{equation}
	\nabla_{\!w} L = \frac{\partial L}{\partial y}\,\frac{\partial y}{\partial w}
\end{equation}
که در آن $y$ خروجی لایه یا شبکه است. این فرمول مبنای پس‌انتشار خطا (\lr{Backpropagation}) در شبکه‌های عصبی محسوب می‌شود و باعث می‌گردد که گرادیان تابع هزینه نسبت به تمامی پارامترها به‌صورت کارآمد محاسبه شود.

