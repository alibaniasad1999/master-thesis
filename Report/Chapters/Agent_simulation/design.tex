\section{طراحی عامل}\label{sec:agent_design}

در این زیربخش، معماری عامل هوشمند کنترل‌کننده فضاپیما در محیط سه‌جسمی شرح داده شده‌است. این معماری شامل تعریف فضای حالت، عمل و تابع پاداش است.


\subsection{ فضای حالت}

فضای حالت\LTRfootnote{State Space}
در این پژوهش به‌گونه‌ای طراحی شده‌است که وضعیت دینامیکی فضاپیما را نسبت به یک مسیر و سرعت مرجع مشخص می‌کند. این فضا شامل اختلاف‌های موقعیت و سرعت از مسیر و سرعت مرجع  است و به‌صورت زیر تعریف شده‌است:
\[
S = \{ \delta x, \delta y, \delta \dot{x}, \delta \dot{y} \}
\]

که در آن:
\begin{itemize}
	\item \( \delta x, \delta y \): اختلاف موقعیت فضاپیما نسبت به مسیر مرجع در محورهای \( x, y \) .
	\item \( \delta \dot{x}, \delta \dot{y} \): اختلاف سرعت فضاپیما نسبت به سرعت مرجع در محورهای \( x, y \) .
\end{itemize}

هر یک از این متغیرها به‌طور مستقل وضعیت فضاپیما را در یک جهت خاص توصیف می‌کنند و امکان تحلیل دقیق انحرافات را فراهم می‌سازند.
%\subsection{دلیل انتخاب اختلاف‌ها به‌عنوان متغیرهای فضای حالت}
استفاده از اختلاف‌های موقعیت و سرعت به جای مقادیر مطلق، به دلایل زیر انجام شده‌است:
\begin{itemize}
	\item \textbf{تمرکز بر انحرافات}: هدف اصلی سیستم کنترلی، کاهش انحرافات از مسیر و سرعت مطلوب است. با استفاده از اختلاف‌ها، کنترلر می‌تواند به طور مستقیم بر این انحرافات اثر بگذارد و نیازی به محاسبه مقادیر مطلق موقعیت و سرعت ندارد.
	%    \item \textbf{سادگی در طراحی کنترلر}: تعریف فضای حالت بر اساس اختلاف‌ها، معادلات دینامیکی را ساده‌تر می‌کند و طراحی کنترلرهای خطی یا غیرخطی را تسهیل می‌نماید.
	%    \item \textbf{بهینه‌سازی محاسبات}: از آنجایی که مسیر و سرعت مرجع معمولاً ثابت یا از پیش تعیین‌شده هستند، محاسبه اختلاف‌ها به جای مقادیر مطلق، حجم محاسبات را کاهش می‌دهد و دقت شبیه‌سازی را افزایش می‌دهد.
	\item \textbf{سازگاری با یادگیری تقویتی}: در الگوریتم‌های یادگیری تقویتی، فضاهای حالت مبتنی بر اختلاف معمولاً دامنه محدودتری دارند که فرآیند یادگیری را سریع‌تر و پایدارتر می‌کند.
\end{itemize}


\subsection{فضای عمل }



فضای عمل\LTRfootnote{Action Space} 
فضاپیما با پیشران‌کم
مجموعه‌ای از عمل‌های پیوسته است که فضاپیما می‌تواند در محیط شبیه‌سازی انجام دهد. این فضا به‌گونه‌ای طراحی شده که امکان اعمال نیرو در جهت‌های مشخص و با مقادیر متناسب با توان واقعی فضاپیماها فراهم شود. به‌طور خاص، فضای اقدام شامل موارد زیر است:

\begin{itemize}
	\item \textbf{نیروی اعمال‌شده در جهت \( x \)}: این متغیر پیوسته، مقدار نیرویی را که در جهت محور \( x \) به فضاپیما وارد می‌شود، تعیین می‌کند. دامنه این نیرو بر اساس توان پیشرانه‌های موجود در فضاپیماهای واقعی انتخاب شده‌است. به عبارت دیگر، اگر حداکثر نیروی قابل اعمال در جهت \( x \) برابر با \( f_{x,\max} \) باشد، این متغیر می‌تواند مقادیری در بازه \( [-f_{x,\max}, f_{x,\max}] \) داشته باشد.
	
	\item \textbf{نیروی اعمال‌شده در جهت \( y \)}: این متغیر پیوسته، مقدار نیرویی را که در جهت محور \( y \) به فضاپیما وارد می‌شود، مشخص می‌کند. مشابه جهت \( x \)، دامنه این نیرو نیز بر اساس توان پیشرانه‌های موجود تعیین شده و می‌تواند در بازه \( [-f_{y,\max}, f_{y,\max}] \) قرار گیرد.
\end{itemize}

انتخاب این نیروها بر اساس ویژگی‌های واقعی فضاپیماها، به‌ویژه توان و محدودیت‌های پیشرانه‌های آن‌ها، صورت گرفته است. این امر اطمینان می‌دهد که شبیه‌سازی تا حد ممکن به شرایط واقعی نزدیک باشد و نتایج به‌دست‌آمده قابلیت تعمیم به کاربردهای عملی را داشته باشند. همچنین، تعریف فضای اقدام به‌صورت پیوسته، امکان کنترل دقیق و انعطاف‌پذیر بر حرکت فضاپیما را فراهم می‌کند، که برای دستیابی به اهداف کنترلی در محیط‌های دینامیکی پیچیده ضروری است.
به‌طور خلاصه، فضای اقدام به‌صورت زیر تعریف می‌شود:
\[
a = \{ f_x, f_y \mid f_x \in [-f_{x,\max}, f_{x,\max}], \, f_y \in [-f_{y,\max}, f_{y,\max}] \}
\]

% --- افزودنی: جدول ماهواره‌های مشابه و ارتباط با بازه‌ی فضای عمل ---
\subsubsection*{انطباق بازه‌ی فضای عمل با داده‌های واقعی}
برای هم‌تراز کردن شبیه‌سازی با سخت‌افزارهای واقعی، از بیشینه‌ی نیروی بی‌بُعدِ پیشران‌ها استفاده می‌شود. جدول زیر نمونه‌هایی از فضاپیماهای مجهز به پیشران‌های یونی/الکتریکی را نشان می‌دهد که مبنای انتخاب بازه‌ی نیروی عمل قرار گرفته شده‌اند. با توجه به برداری‌بودن عمل
$\boldsymbol{a}=[
f_x ~  f_y
]$،
کران‌ها را به دو صورت اعمال شده‌است:
\[
|a| \le f_{\text{max, nondim}}
\quad \text{یا} \quad
f_{x,\max}=f_{y,\max}=f_{\text{max, nondim}}.
\]
با استناد به جدول
\ref{tab:camparison}،
مقدار نمونه‌ی \(4{\times}10^{-2}\) شبیه‌سازی شده با \lr{Psyche} هم‌مرتبه و کمتر از \lr{DS1} است که باعث شده‌است بازه‌ی عمل را در چارچوب پیشران‌های کم‌تراست واقع‌گرایانه نگه داشته شود.


\vspace{1cm}
\begin{table*}[h!]
	\centering
	\begin{RTL}
		\caption{قابلیت‌های بی‌بعد پیشران‌کم‌تراستِ {فضاپیماهای} مختلف در سامانه‌ی زمین–ماه \cite{lafarge}.}
		\label{tab:camparison}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			{\textbf{نام اختصار}} & {\textbf{نام فضاپیما}} & \lr{\textbf{$f_{\text{max, nondim}}$}} & \lr{\textbf{$M_{3,0}$ (kg)}} & \lr{\textbf{$F_{\text{max}}$ (mN‌)}} \\ \hline
			\lr{DS1} & \lr{Deep Space 1} & $6.940 \cdot 10^{-2}$ & $486.3$ & $92.0$ \\ \hline
			\lr{Psyche} & \lr{Psyche} & $4.158 \cdot 10^{-2}$ & $2464$ & $279.3$ \\ \hline
			\lr{Dawn} & \lr{Dawn}  & $2.741 \cdot 10^{-2}$ & $1217.8$ & $91.0$ \\ \hline
			\lr{LIC} & \lr{Lunar IceCube} & $3.276 \cdot 10^{-2}$ & $14$ & $1.25$ \\ \hline
			\lr{H1} & \lr{Hayabusa 1} & $1.640 \cdot 10^{-2}$ & $510$ & $22.8$ \\ \hline
			\lr{H2} & \lr{Hayabusa 2} & $1.628 \cdot 10^{-2}$ & $608.6$ & $27.0$ \\ \hline
			\lr{s/c} & فضاپیمای نمونه & $4 \cdot 10^{-2}$ & $-$ & $-$ \\ \hline
		\end{tabular}
	\end{RTL}
\end{table*}


% --- پایان افزودنی ---





\subsection{تابع پاداش} \label{subsec:rl_reward}
تابع پاداش\LTRfootnote{Reward Function}
به‌منظور هدایت رفتار عامل طراحی شده و شامل سه بخش اصلی در طول شبیه‌سازی و یک پاداش نهایی در هنگام پایان است:
\begin{itemize}
	\item \textbf{پاداش نهایی برای دستیابی به هدف}: در صورت وارد شدن وضعیت سامانه به مجموعه‌ی هدف، شبیه‌سازی بلافاصله خاتمه یافته و یک پاداش بزرگ مثبت به عامل تخصیص داده می‌شود تا ارزش‌یابی فرایند همگرایی را تقویت کند.
	\item \textbf{جریمه نهایی برای دور شدن بیش‌از‌حد}: اگر وضعیت سامانه از حدود ایمنی تعریف‌شده فراتر رود، اپیزود متوقف شده و یک جریمه بزرگ منفی به عامل تعلق می‌گیرد تا ناپایداری یا گریز از ناحیه‌ی قابل‌قبول را منعکس کند.
	\item \textbf{جریمه برای مصرف سوخت}: در طول مسیر، استفاده بیش‌از‌حد از پیشرانه با جریمه همراه است.
	\item \textbf{جریمه برای انحراف از مسیر مرجع}: در طول مسیر، انحراف از مسیر مرجع باعث دریافت جریمه متناسب می‌شود.
\end{itemize}

این دو مؤلفه‌ی ترمینال با هدف تضمین همگرایی سیاست کنترلی به سمت مدار مطلوب طراحی شده‌اند. پاداش مثبت پایان، طیف مسیرهای موفق را برجسته کرده و عامل را به سمت سیاست‌های پایدار هدایت می‌کند؛ در مقابل، جریمه‌ی منفی پایان با ایجاد گرادیان تنبیهی، از انتخاب راهبردهایی که منجر به خروج خطرناک از ناحیه‌ی عملیاتی می‌شوند جلوگیری می‌نماید.

تابع پاداش به‌صورت زیر تعریف می‌شود:
\[
r(s, a) = r_{\text{thrust}}(a) + r_{\text{reference}}(s) + r_{\text{terminal}}(s)
\]

که در آن مؤلفه‌ها عبارتند از:
\begin{align}
	r_{\text{thrust}}(a) &= -k_1 \cdot |a| \\
	r_{\text{reference}}(s) &= -k_2 \cdot d(s, s_{\text{reference}}) \\
	r_{\text{terminal}}(s) &= 
	\begin{cases}
		+R_{\text{goal}} & \text{if} ~ s \in S_{\text{goal}} \\
		-R_{\text{fail}} & \text{if} ~ d(s, s_{\text{reference}}) > \epsilon \\
		0 & \text{otherwise}
	\end{cases}
\end{align}

در این رابطه:  
\begin{enumerate}
	\item \(R_{\text{goal}}\): یک پاداش بزرگ مثبت برای دستیابی به هدف است.
	\item \(R_{\text{fail}}\): یک جریمه بزرگ منفی برای خروج از محدوده مجاز است که رفتارهای ناپایدار را به‌شدت سرکوب می‌کند.
	\item \(d(s, s')\): فاصله بین دو وضعیت بوده و به‌صورت فاصله اقلیدسی محاسبه می‌شود.
\end{enumerate}


ضرایب \(k_1, k_2\) برای تنظیم تعادل بین بهینه‌سازی مصرف سوخت و حفظ نزدیکی به مسیر مرجع استفاده می‌شوند. انتخاب مناسب مقادیر این ضرایب نقش کلیدی در سرعت همگرایی و پایداری الگوریتم یادگیری تقویتی دارد.







