\chapter{شبیه‌سازی عامل درمحیط سه جسمی}

%\input{Chapters/Agent_simulation/agent_nn}
	

	
		در این فصل، فرآیند شبیه‌سازی عامل هوشمند کنترل‌کننده فضاپیما در محیط دینامیکی سه جسمی بررسی شده است.
		در بخش \ref{sec:agent_design}
		به
		طراحی و 
		در بخش 
		\ref{sec:agent_sim}
	به	شبیه‌سازی عامل هدایت‌کننده مبتنی بر یادگیری تقویتی است
		پرداخته شده است. این عامل طراحی و شبیه‌سازی شده باید توانایی این را داشته باشد 
		 که  فضاپیما را به‌طور مؤثر به سمت اهداف تعیین‌شده هدایت کند، در حالی که محدودیت‌هایی نظیر مصرف سوخت و وجود اغتشاش دارد.
	
	
	\section{طراحی عامل}\label{sec:agent_design}
	
	در این زیربخش، معماری عامل هوشمند کنترل‌کننده فضاپیما در محیط سه‌جسمی شرح داده شده است. این معماری شامل تعریف فضای حالت، عمل و تابع پاداش است.

	
	\subsection{ فضای حالت}

فضای حالت\LTRfootnote{State Space}
در این پژوهش به‌گونه‌ای طراحی شده است که وضعیت دینامیکی فضاپیما را نسبت به یک مسیر و سرعت مرجع مشخص می‌کند. این فضا شامل اختلاف‌های موقعیت و سرعت از مسیر و سرعت مرجع  است و به‌صورت زیر تعریف می‌شود:
\[
S = \{ \delta x, \delta y, \delta \dot{x}, \delta \dot{y} \}
\]

که در آن:
\begin{itemize}
    \item \( \delta x, \delta y \): اختلاف موقعیت فضاپیما نسبت به مسیر مرجع در محورهای \( x, y \) .
    \item \( \delta \dot{x}, \delta \dot{y} \): اختلاف سرعت فضاپیما نسبت به سرعت مرجع در محورهای \( x, y \) .
\end{itemize}

هر یک از این متغیرها به‌طور مستقل وضعیت فضاپیما را در یک جهت خاص توصیف می‌کنند و امکان تحلیل دقیق انحرافات را فراهم می‌سازند.
%\subsection{دلیل انتخاب اختلاف‌ها به‌عنوان متغیرهای فضای حالت}
استفاده از اختلاف‌های موقعیت و سرعت به جای مقادیر مطلق، به دلایل زیر انجام شده است:
\begin{itemize}
    \item \textbf{تمرکز بر انحرافات}: هدف اصلی سیستم کنترلی، کاهش انحرافات از مسیر و سرعت مطلوب است. با استفاده از اختلاف‌ها، کنترلر می‌تواند به طور مستقیم بر این انحرافات اثر بگذارد و نیازی به محاسبه مقادیر مطلق موقعیت و سرعت ندارد.
%    \item \textbf{سادگی در طراحی کنترلر}: تعریف فضای حالت بر اساس اختلاف‌ها، معادلات دینامیکی را ساده‌تر می‌کند و طراحی کنترلرهای خطی یا غیرخطی را تسهیل می‌نماید.
%    \item \textbf{بهینه‌سازی محاسبات}: از آنجایی که مسیر و سرعت مرجع معمولاً ثابت یا از پیش تعیین‌شده هستند، محاسبه اختلاف‌ها به جای مقادیر مطلق، حجم محاسبات را کاهش می‌دهد و دقت شبیه‌سازی را افزایش می‌دهد.
    \item \textbf{سازگاری با یادگیری تقویتی}: در الگوریتم‌های یادگیری تقویتی، فضاهای حالت مبتنی بر اختلاف معمولاً دامنه محدودتری دارند که فرآیند یادگیری را سریع‌تر و پایدارتر می‌کند.
\end{itemize}

	
	\subsection{فضای عمل }
	

	
		فضای عمل\LTRfootnote{Action Space} 
		فضاپیما با پیشران‌کم
		مجموعه‌ای از عمل‌های پیوسته است که فضاپیما می‌تواند در محیط شبیه‌سازی انجام دهد. این فضا به‌گونه‌ای طراحی شده که امکان اعمال نیرو در جهت‌های مشخص و با مقادیر متناسب با توان واقعی فضاپیماها فراهم شود. به‌طور خاص، فضای اقدام شامل موارد زیر است:
	
	\begin{itemize}
		\item \textbf{نیروی اعمال‌شده در جهت \( x \)}: این متغیر پیوسته، مقدار نیرویی را که در جهت محور \( x \) به فضاپیما وارد می‌شود، تعیین می‌کند. دامنه این نیرو بر اساس توان پیشرانه‌های موجود در فضاپیماهای واقعی انتخاب شده است. به عبارت دیگر، اگر حداکثر نیروی قابل اعمال در جهت \( x \) برابر با \( f_{x,\max} \) باشد، این متغیر می‌تواند مقادیری در بازه \( [-f_{x,\max}, f_{x,\max}] \) داشته باشد.
		
		\item \textbf{نیروی اعمال‌شده در جهت \( y \)}: این متغیر پیوسته، مقدار نیرویی را که در جهت محور \( y \) به فضاپیما وارد می‌شود، مشخص می‌کند. مشابه جهت \( x \)، دامنه این نیرو نیز بر اساس توان پیشرانه‌های موجود تعیین شده و می‌تواند در بازه \( [-f_{y,\max}, f_{y,\max}] \) قرار گیرد.
	\end{itemize}
	
	انتخاب این نیروها بر اساس ویژگی‌های واقعی فضاپیماها، به‌ویژه توان و محدودیت‌های پیشرانه‌های آن‌ها، صورت گرفته است. این امر اطمینان می‌دهد که شبیه‌سازی تا حد ممکن به شرایط واقعی نزدیک باشد و نتایج به‌دست‌آمده قابلیت تعمیم به کاربردهای عملی را داشته باشند. همچنین، تعریف فضای اقدام به‌صورت پیوسته، امکان کنترل دقیق و انعطاف‌پذیر بر حرکت فضاپیما را فراهم می‌کند، که برای دستیابی به اهداف کنترلی در محیط‌های دینامیکی پیچیده ضروری است.
	به‌طور خلاصه، فضای اقدام به‌صورت زیر تعریف می‌شود:
	\[
	a = \{ f_x, f_y \mid f_x \in [-f_{x,\max}, f_{x,\max}], \, f_y \in [-f_{y,\max}, f_{y,\max}] \}
	\]
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\subsection{تابع پاداش }
	تابع پاداش\LTRfootnote{Reward Function}
 به‌منظور هدایت رفتار عامل طراحی شده و شامل دو مؤلفه اصلی است:
	\begin{itemize}
		\item \textbf{پاداش برای دستیابی به هدف}: تشویق عامل برای نزدیک شدن به مدار هدف.
		\item \textbf{جریمه برای مصرف سوخت}: تنبیه برای استفاده بیش از حد از پیشرانه.
		\item \textbf{جریمه برای انحراف از مسیر مرجع}: تنبیه برای خروج از مسیر مرجع.
	\end{itemize}
	تابع پاداش به‌صورت زیر تعریف می‌شود:
	\[
	r(s, a) = r_{\text{target}}(s) + r_{\text{thrust}}(a)  + r_{\text{divergence}}(s)
	\]
	که در آن مؤلفه‌های تابع پاداش به‌صورت زیر تعریف شده‌اند:
	\begin{align}
	r_{\text{target}}(s) &= -k_1 \cdot d(s, s_{\text{target}}) \\
	r_{\text{thrust}}(a) &= -k_2 \cdot \|a\| \\	
	r_{\text{divergence}}(s) &= \begin{cases}
	-k_3 & \text{if} ~ d(s, s_{\text{reference}}) > \epsilon \\
	0 & \text{otherwise}
	\end{cases}
	\end{align}
تابع \( d(s, s') \) فاصله بین دو وضعیت \( s \) و \( s' \) را نشان می‌دهد که معمولاً به‌صورت فاصله اقلیدسی محاسبه می‌شود.
ضرایب \( k_1, k_2, k_3 \) از طریق آزمایش و خطا تنظیم شده‌اند تا تعادل مناسبی بین دستیابی به هدف، بهینه‌سازی مصرف سوخت، و حفظ مسیر مرجع برقرار شود. علاوه بر این، این ضرایب تأثیر مستقیمی بر پایداری و فرآیند یادگیری عامل دارند. به‌عنوان مثال، انتخاب مقادیر بیش از حد بزرگ برای \( k_1 \) ممکن است باعث شود عامل به‌سرعت به سمت هدف حرکت کند اما پایداری مسیر را از دست بدهد، در حالی که مقادیر بزرگ \( k_3 \) می‌تواند عامل را بیش از حد محافظه‌کار کرده و فرآیند یادگیری را کند نماید. تنظیم دقیق این ضرایب، نه‌تنها عملکرد عامل را بهینه می‌کند، بلکه پایداری عددی و سرعت همگرایی الگوریتم یادگیری تقویتی را نیز تضمین می‌نماید.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\section{شبیه‌سازی عامل}\label{sec:agent_sim}
	
	در این زیربخش، فرآیند شبیه‌سازی و آموزش عامل با استفاده از الگوریتم‌های یادگیری تقویتی پیشرفته شرح داده می‌شود. الگوریتم‌های مورد استفاده، مراحل آموزش، و نتایج حاصل از شبیه‌سازی ارائه می‌گردند.
	
	\subsection{پارامترهای یادگیری الگوریتم‌های مورد استفاده}
	برای آموزش عامل، الگوریتم‌های زیر به‌کار گرفته شده‌اند:
	
	\begin{table}[h]
		\centering
		\caption{ویژگی‌های الگوریتم‌های مورد استفاده در شبیه‌سازی}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}{\textbf{الگوریتم}} & \multicolumn{2}{c|}{\textbf{شبکه \lr{Actor}}} & \multicolumn{2}{c|}{\textbf{شبکه \lr{Critic}}} & \multirow{2}{*}{\textbf{تعداد پارامترها}} \\
			\cline{2-5}
			& \textbf{لایه‌ها} & \textbf{نودها} & \textbf{لایه‌ها} & \textbf{نودها} & \\
			\hline
			\lr{DDPG} & \(3\) & \( (2^8, 2^7, 2^6) \) & \(3\) & \( (2^8, 2^5) \) & \(150 \times 10^3\) \\
			\hline
			\lr{PPO} & \(2\) & \( (2^7, 2^6) \) & \(2\) & \( (2^7, 2^6) \) & \(50 \times 10^3\) \\
			\hline
			\lr{SAC} & \(3\) & \( (2^8, 2^7, 2^6) \) & \(3\) & \( (2^8, 2^7, 2^6) \) & \(160 \times 10^3\) \\
			\hline
			\lr{TD3} & \(3\) & \( (2^8, 2^7, 2^6) \) & \(4\) & \( (2^8, 2^7, 2^7, 2^6) \) & \(200 \times 10^3\) \\
			\hline
		\end{tabular}
	\end{table}
	این الگوریتم‌ها به دلیل توانایی در مدیریت فضاهای پیوسته و عملکرد مؤثر در محیط‌های پیچیده انتخاب شده‌اند.
	در شکل‌های
	\ref{fig:actor_nn}
	و
	\ref{fig:critic_nn}
	ساختار شبیه‌سازی شده شبکه عصبی عامل و نقاد آورده شده است.
	\input{Chapters/Agent_simulation/agent_nn}
	\input{Chapters/Agent_simulation/critic_nn}
	
	
	
	
	
	
	% جدول فشرده با کاهش فاصله‌ی عمودی و چهار ستون
	
	
	
	
	% جدول چهارستونه فشرده با افزودن شبکه‌ی بازیگر–منتقد و تابع فعال‌سازی
	\begin{table}[H]
		\centering
		% \footnotesize % کوچک‌تر کردن فونت در صورت نیاز
		\setlength{\tabcolsep}{8pt} % کاهش فاصله‌ی افقی میان ستون‌ها
		\renewcommand{\arraystretch}{0.95} % کاهش فاصله‌ی عمودی سطرها
		\begin{RTL}
			\begin{tabular}{|c|c|c|c|}
				\hline
				\textbf{نام پارامتر} & \textbf{مقدار} & \textbf{نام پارامتر} & \textbf{مقدار} \\
				\hline
				گام در هر دوره یادگیری & $30\,000$ & تعداد دوره‌های یادگیری & $100$ \\
				اندازه‌ی مخزنِ تجربه & $10^{6}$ & ضریب تنزیل & $0.99$ \\
				ضریب میانگین پلیاک & $0.995$ & نرخِ یادگیریِ سیاست & $10^{-3}$ \\
				نرخِ یادگیریِ \lr{Q} & $10^{-3}$ & اندازه‌ی دسته & $1024$ \\
				گام‌ شروعِ استفاده از سیاست & $5\,000$ & گام شروعِ به‌روزرسانی& $1\,000$ \\
				فاصله‌ی به‌روزرسانی & $2\,000$ & نویز عمل & $0.1$ \\
				حداکثر طولِ رخداد & $6\,000$ & دستگاه & \lr{Cuda} \\
				شبکه‌ی \lr{Actor}
				 & \((2^5, 2^5) \)  & تابع فعال‌سازی  \lr{Actor} & \lr{ReLU} \\
				 	شبکه‌ی \lr{Critic}
				 & \( (2^5, 2^5) \)  & تابع فعال‌سازی  \lr{Critic} & \lr{ReLU} \\
				\hline
			\end{tabular}
		\end{RTL}
		\caption{جدول پارامترها و مقادیر پیش‌فرض الگوریتم \lr{DDPG}
		\cite{SpinningUp2018}}
	\end{table}
	
	
	
	

\subsection{فرآیند آموزش}

آموزش عامل به‌صورت کلی در چند مرحله انجام شده است. ابتدا، کاوش اولیه در محیط با استفاده از یک سیاست تصادفی صورت گرفته و تجربه‌های اولیه جمع‌آوری شده‌اند. سپس، شبکه‌های عصبی الگوریتم‌ها با بهره‌گیری از این تجربه‌ها به‌روزرسانی شده‌اند. در نهایت، پارامترهای کلیدی مانند نرخ یادگیری و اندازه بافر تجربه تنظیم شده‌اند تا پایداری فرآیند تضمین شود.

برای پیاده‌سازی این فرآیند، از چارچوب \lr{PyTorch} استفاده شده است. همچنین، به‌منظور جلوگیری از بیش‌برازش، تکنیک \lr{Noise Exploration} به‌کار گرفته شده است. آموزش تا زمانی ادامه یافته که موفقیت عامل در بیش از 90 درصد موارد به‌دست آمده باشد. در این راستا، برای بهینه‌سازی پارامترهای شبکه‌های عصبی، از روش \lr{Backpropagation} استفاده شده است. این روش بر اساس گرادیان تابع خطا نسبت به پارامترها عمل می‌کند که به‌صورت زیر بیان می‌شود:
































\begin{equation}
	\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w}
\end{equation}
%که در آن:
که در آن \( L \) تابع خطا، \( w \) وزن‌های شبکه، و \( y \) خروجی شبکه عصبی است. 
%\begin{itemize}
%	\item \( L \): تابع خطا (Loss Function) است که معمولاً به‌صورت \( L = \mathbb{E}[(r - \hat{r})^2] \) تعریف می‌شود.
%	\item \( w \): پارامترهای شبکه (وزن‌ها).
%	\item \( y \): خروجی شبکه عصبی.
%\end{itemize}
به‌روزرسانی وزن‌ها با استفاده از روش گرادیان نزولی انجام شده است:
\begin{equation}
	w_{t+1} = w_t - \eta \cdot \frac{\partial L}{\partial w}
\end{equation}
که \( \eta \) نرخ یادگیری است و به‌عنوان یک پارامتر کلیدی تنظیم شده است.




