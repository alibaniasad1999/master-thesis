% -------------------------------------------------------
%  Conclusion
% -------------------------------------------------------

\chapter{نتیجه‌گیری و پیشنهادها}
\label{chap:conclusion}

\noindent
در این پایان‌نامه، مسأله‌ی هدایتِ مقاومِ فضاپیماهای کم‌پیشران در دینامیکِ چندجسمیِ مدلِ \lr{CRTBP} زمین–ماه به‌صورت یک {بازی دیفرانسیلی مجموع‌صفر} میان عامل هدایت و عامل مزاحم صورت‌بندی شد و با الگوی {آموزش متمرکز–اجرای توزیع‌شده} دنبال گردید. چهار الگوریتم پیوسته‌ی \lr{DDPG}، \lr{TD3}، \lr{SAC} و \lr{PPO} به نسخه‌های چندعاملیِ مجموع‌صفر تعمیم داده شدند، اجزای بازیگر–منتقد و سازوکارهای پایداری آموزش تشریح گردید. ارزیابی گسترده زیر عدم‌قطعیت‌های واقع‌گرایانه شرایط اولیه‌ی تصادفی، اغتشاش عملگر، نویز حسگر، تأخیر زمانی و عدم‌تطابق مدل نشان داد نسخه‌های مجموع‌صفر به‌صورت پایدار از همتایان تک‌عاملی پیشی می‌گیرند؛ به‌ویژه \lr{MA-TD3} بهترین سازش میان دقت مسیر، مصرف سوخت و پایداری را فراهم کرد.

\section*{جمع‌بندی دستاوردها}
\begin{itemize}
  \item ارائه‌ی صورت‌بندی بازی دیفرانسیلی مجموع‌صفر برای هدایت کم‌پیشران در \lr{CRTBP} با آموزش متمرکز و اجرای توزیع‌شده.
  \item تعمیم چهار الگوریتم پرکاربرد \lr{RL} به نسخه‌های چندعاملی مجموع‌صفر و تبیین دقیق معماری بازیگر–منتقد و پایدارسازی آموزش.
  \item طراحی حریفِ یادگیر برای تنوع‌بخشی نظام‌مند به عدم‌قطعیت‌ها و ارتقای تاب‌آوری سیاست در سناریوهای دشوار.
  \item پروتکل ارزیابی چندمعیاره با شاخص‌های دقت مسیر، پاداش 
  و پایداری، و نشان‌دادن برتری
   منسجم نسخه‌های مجموع‌صفر.
%  \item پیاده‌سازی سبک‌وزن و قابل‌اجرا به‌صورت بلادرنگ با کوانتیزاسیون \lr{INT8} با افت دقت ناچیز.
\end{itemize}

%\section*{محدودیت‌ها}
%\begin{itemize}
%  \item تحلیل در چارچوب \lr{CRTBP} انجام شد؛ تعمیم کامل به میدان‌های چندجسمیِ مبتنی بر اپمریس یا اثرات غیرمداری (تابش خورشیدی، سایه، محدودیت توان/حرارت) مستلزم توسعه‌ی مدل است.
%  \item سناریوهای عدم‌قطعیت عمدتاً به‌صورت مستقل آزموده شدند؛ ارزیابی هم‌زمانِ ترکیبات شدیدتر می‌تواند تصویر محافظه‌کارانه‌تری ارائه دهد.
%  \item حساسیت به طراحی و مقیاس‌بندی پاداش و تنظیمات \lr{RL}؛ نیاز به مطالعه‌ی نظام‌مند حساسیت و ابلیشن برای جداسازی عوامل اثرگذار.
%  \item هزینه‌ی آموزش و نمونه‌کارایی: بودجه‌ی محاسباتی بالا و نوسان عملکرد نسبت به بذرهای تصادفی می‌تواند بازتولید را دشوار کند.
%  \item تضمین‌های نظری محدود درباره‌ی ایمنی و بهینگی در حضور تأخیر و عدم‌تطابق مدل؛ نیاز به چارچوب‌های رسمی‌تر.
%\end{itemize}

\section*{پیشنهادهایی برای کارهای آینده}
\begin{itemize}
	\item تعمیم چارچوب به مسئله \lr{$N$-body} و درنظرگرفتن اغتشاشات غیرگرانشی؛ استفاده از یادگیری مرحله‌ای (\lr{curriculum}) متناسب با پیچیدگی دینامیکی.
	\item بررسی \lr{Risk-Sensitive RL}، اعمال قیود ایمنی به‌صورت \lr{chance constraints} و به‌کارگیری \lr{Control Barrier Functions}
	 برای فراهم‌کردن تضمین.
	\item توسعه راهبردهای ترکیبی یادگیری تقویتی و کنترل مبتنی بر مدل (مانند \lr{MPC}/\lr{iLQR}) برای بهبود ایمنی و تفسیرپذیری.
	\item آموزش خصمانه مبتنی بر توزیع (جمعیت مزاحم‌ها) و طراحی \lr{curriculum/adversary shaping} برای پوشش بهتر نواحی عدم‌قطعیت.
	\item استقرار روی سامانه‌های تعبیه‌شده‌ی کم‌مصرف و مقایسه‌ی \lr{ONNX Runtime}، \lr{TVM} و \lr{TensorRT} در معماری‌های گوناگون؛ بهینه‌سازی \lr{latency/throughput} و سنجه‌ی \lr{energy–delay}.
	\item انجام تحلیل حساسیت نسبت به تابع پاداش، معماری، نویز حسگر و تأخیر؛ مستندسازی دقیق برای ارتقای بازتولیدپذیری.
\end{itemize}
       

\vspace{0.5em}
در مجموع، نتایج این پژوهش نشان داد که رویکرد بازی‌محور چندعاملی در یادگیری تقویتی می‌تواند هدایت تطبیقی و مقاوم را بدون اتکای شدید به مدل‌های دقیق فراهم کند و مسیر روشنی برای گذار به کاربردهای عملی و سناریوهای پیچیده‌تر می‌گشاید.

