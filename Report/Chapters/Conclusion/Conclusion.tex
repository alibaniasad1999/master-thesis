% -------------------------------------------------------
%  Conclusion
% -------------------------------------------------------

\chapter{نتیجه‌گیری و پیشنهادها}
\label{chap:conclusion}

\noindent
در این پایان‌نامه، چارچوبی برای هدایت مقاوم فضاپیماهای کم‌پیشران در محیط‌های پویای چندجسمی (مدل \lr{CRTBP} زمین–ماه) ارائه شد که مسأله را به‌صورت یک \emph{بازی دیفرانسیلی مجموع‌صفر} میان عامل هدایت و عامل مزاحم صورت‌بندی می‌کند. چهار الگوریتمِ پیوسته‌ی \lr{DDPG}، \lr{TD3}، \lr{SAC} و \lr{PPO} به نسخه‌های چندعاملی مجموع‌صفر گسترش یافتند و جریان آموزش و ساختار بازیگر–منتقد به‌دقت تبیین شد. ارزیابی گسترده زیر عدم‌قطعیت‌های واقع‌گرایانه—از جمله شرایط اولیه‌ی تصادفی، اغتشاش عملگر، نویز حسگر، تأخیر زمانی و عدم‌تطابق مدل—نشان داد نسخه‌های مجموع‌صفر به‌طور منسجم از همتایان تک‌عاملی پیشی می‌گیرند؛ به‌ویژه \lr{MATD3} بهترین موازنه‌ی دقت مسیر، مصرف سوخت و پایداری را ارائه کرد.

\section*{جمع‌بندی دستاوردها}
\begin{itemize}
  \item ارائه‌ی یک چارچوب بازی دیفرانسیلی مجموع‌صفر برای هدایت کم‌پیشران در \lr{CRTBP} با آموزش متمرکز–اجرای توزیع‌شده.
  \item تعمیم چهار الگوریتم پرکاربرد \lr{RL} به نسخه‌های چندعاملی و مقایسه‌ی نظام‌مندِ کارایی و مقاومت در سناریوهای دشوار.
  \item پیاده‌سازی و بهینه‌سازی استقرار زمان‌واقعی بر \lr{ROS\,2} با تبدیل \lr{PyTorch→ONNX} و کوانتیزاسیون \lr{INT8} که به \SI{5.8}{\milli\second} تأخیر استنتاج و \SI{9.2}{\mega\byte} حافظه انجامید.
  \item تایید امکان‌پذیری حلقه‌ی کنترل \SI{100}{\hertz} روی سخت‌افزار کلاس پرواز بدون نقض مهلت زمانی در بستر \lr{HIL}.
\end{itemize}

\section*{محدودیت‌ها}
\begin{itemize}
  \item تحلیل در چارچوب \lr{CRTBP} انجام شد؛ تعمیم کامل به میدان‌های چندجسمی کلی‌تر یا اثرات غیرمداری (مانند تابش خورشیدی) مستلزم توسعه‌ی مدل است.
  \item سناریوهای عدم‌قطعیت به‌صورت مستقل تست شدند؛ ارزیابی همزمانِ ترکیبات شدیدتر می‌تواند تصویر محافظه‌کارانه‌تری ارائه دهد.
  \item گرچه کوانتیزاسیون \lr{INT8} افت دقت ناچیزی داشت، ارزیابی طولانی‌مدت پایداری عددی زیر \lr{drift} سخت‌افزار توصیه می‌شود.
\end{itemize}

\section*{پیشنهاد برای کارهای آینده}
\begin{itemize}
  \item تعمیم چارچوب به \lr{Ephemeris-based N-body} و لحاظ اغتشاشات غیرگرانشی؛ استفاده از \lr{curriculum} مبتنی بر پیچیدگی دینامیک.
  \item \lr{Risk-sensitive RL} و محدودیت‌های ایمنی به‌صورت \lr{chance constraints} برای تضمین‌های رسمی‌تر.
  \item یادگیری همزمان سیاست و \lr{state estimator} (یادگیری انتها-به-انتها) با سنسورهای واقعی و \lr{domain randomization}.
  \item استقرار روی سامانه‌های تعبیه‌شده‌ی کم‌مصرف‌تر و مقایسه‌ی \lr{ONNX Runtime} با \lr{TVM} و \lr{TensorRT} در معماری‌های گوناگون.
\end{itemize}

\vspace{0.5em}
در مجموع، نتایج این پژوهش نشان داد که رویکردِ بازی‌محورِ چندعاملی در یادگیری تقویتی می‌تواند هدایت تطبیقی و مقاوم را بدون اتکای شدید به مدل‌های دقیق فراهم کند و آمادگی لازم برای گذار به آزمایش‌های میدانی و پیاده‌سازی عملی را دارد.

