
% -------------------------------------------------------
%  English Abstract
% -------------------------------------------------------


\pagestyle{empty}

\begin{latin}
	
	\begin{center}
		\textbf{Abstract}
	\end{center}
	\baselineskip=.8\baselineskip
	\noindent
	
%	In this study, a quadcopter stand with three degrees of freedom was controlled using game theory-based control. The first player tracks a desired input, and the second player creates a disturbance in the tracking of the first player to cause an error in the tracking. The move is chosen using the Nash equilibrium, which presupposes that the other player made the worst move.. In addition to being resistant to input interruptions, this method may also be resilient to modeling system uncertainty. This method evaluated the performance through simulation in the Simulink environment and implementation on a three-degree-of-freedom stand.
This thesis proposes a robust guidance framework for low‑thrust spacecraft operating in multi‑body dynamical environments modeled by the Earth–Moon circular restricted three‑body problem (CRTBP). The guidance task is cast as a zero‑sum differential game between a controller agent (spacecraft) and an adversary agent (environmental disturbances), implemented under a centralized‑training/ decentralized‑execution paradigm. Four continuous‑control reinforcement‑learning algorithms—DDPG, TD3, SAC, and PPO—are extended to their multi‑agent zero‑sum counterparts (MA‑DDPG, MATD3, MASAC, MAPPO); their actor–critic network structures and training pipelines are detailed.

The policies are trained and evaluated on transfers to the Earth–Moon  lyapunov orbit under five uncertainty scenarios: random initial states, actuator perturbations, sensor noise, communication delays, and model mismatch. Zero‑sum variants consistently outperform their single‑agent baselines, with MATD3 delivering the best trade‑off between trajectory accuracy and propellant consumption while maintaining stability in the harshest conditions.

For real‑time deployment, the learned networks are quantized to INT8 and exported to ONNX for execution on a ROS 2 hardware‑in‑the‑loop platform. Inference latency is reduced to 5.8 ms and memory footprint to 9.2 MB—improvements of 47 \% and 53 \% over the FP32 models—while sustaining a 100 Hz control loop with no deadline misses.

The results demonstrate that the proposed multi‑agent, game‑theoretic reinforcement‑learning framework enables adaptive and robust low‑thrust guidance in unstable three‑body regions without reliance on precise dynamics models, and is ready for hardware‑in‑the‑loop implementation.

	
	\bigskip\noindent\textbf{Keywords}:
%	Quadcopter, Differential Game, Game Theory, Nash Equilibrium, Three Degree of Freedom Stand, Model Base Design, Linear Quadratic Regulator
	Deep Reinforcement Learning; Differential Game; Multi‑Agent; Low‑Thrust Guidance; Three‑Body Problem; Robustness.
\end{latin}

\newpage