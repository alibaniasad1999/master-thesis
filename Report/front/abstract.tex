% -------------------------------------------------------
%  Abstract
% -------------------------------------------------------


\pagestyle{plain}

\begin{وسط‌چین}
\مهم{چکیده}
\end{وسط‌چین}

\بدون‌تورفتگی 
در این پژوهش، یک چارچوب هدایت مقاوم فضاپیماهای کم‌پیشران در محیط‌های دینامیکی چندجسمی (مدل \lr{CRTBP} زمین–ماه) ارائه شده است. مسئله به‌صورت بازی دیفرانسیلی مجموع‌صفر بین عامل هدایت (فضاپیما) و عامل مزاحم (طبیعت) فرموله شده و با رویکرد آموزش متمرکز–اجرا توزیع‌شده پیاده‌سازی شده است. چهار الگوریتم یادگیری تقویتی پیوسته \lr{DDPG}، \lr{TD3}، \lr{SAC} و \lr{PPO} به نسخه‌های چندعاملی مجموع‌صفر تعمیم یافته‌اند (\lr{MA‑DDPG}، \lr{MATD3}، \lr{MASAC} و \lr{MAPPO})؛ جریان آموزش آن‌ها و ساختار شبکه‌ها در قالب ارزش–سیاست مشترک تشریح شده است.

الگوریتم‌ها در سناریوهای عدم قطعیت مختلف—شرایط اولیه تصادفی، اغتشاش عملگر، نویز حسگر، تأخیر زمانی و عدم تطابق مدل—روی مسیر مدار لیاپانوف زمین–ماه ارزیابی شدند. نتایج نشان می‌دهد نسخه‌های مجموع‌صفر در تمامی معیارها بر نسخه‌های تک‌عاملی برتری دارند؛ به ویژه \lr{MATD3} با حفظ پایداری، کمترین انحراف مسیر و مصرف سوخت متعادل را در سخت‌ترین سناریوها ارائه کرد.

به منظور امکان استقرار عملی، سیاست‌های آموخته‌شده روی سکوی سخت‌افزاری \lr{ROS 2} با کوانتیزاسیون \lr{INT8} و تبدیل به \lr{ONNX} پیاده‌سازی شد. این بهینه‌سازی، زمان استنتاج را به ۵٫۸ میلی‌ثانیه و حافظه را به ۹٫۲ مگابایت رساند که به ترتیب ۴۷ درصد و ۵۳ درصد بهبود نسبت به مدل \lr{FP32} داشت، در حالی‌که چرخه کنترل ۱۰۰ هرتز بدون نقض زمانی حفظ شد.

چارچوب پیشنهادی نشان داد که یادگیری تقویتی چندعاملی مبتنی بر بازی دیفرانسیلی می‌تواند بدون اتکا به مدل دقیق، هدایت تطبیقی و مقاوم فضاپیماهای کم‌پیشران را در نواحی ناپایدار سه‌جسمی تضمین کرده و برای پیاده‌سازی روی سخت‌افزار در حلقه آماده باشد.


\پرش‌بلند
\بدون‌تورفتگی \مهم{کلیدواژه‌ها}: 
 یادگیری تقویتی عمیق؛ بازی دیفرانسیلی؛ چندعاملی؛ هدایت کم‌پیشران؛ مسئله سه‌جسمی؛ مقاومت.
\صفحه‌جدید
