@misc{tensorflow2015-whitepaper,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={https://www.tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{i}n~Abadi and
	Ashish~Agarwal and
	Paul~Barham and
	Eugene~Brevdo and
	Zhifeng~Chen and
	Craig~Citro and
	Greg~S.~Corrado and
	Andy~Davis and
	Jeffrey~Dean and
	Matthieu~Devin and
	Sanjay~Ghemawat and
	Ian~Goodfellow and
	Andrew~Harp and
	Geoffrey~Irving and
	Michael~Isard and
	Yangqing Jia and
	Rafal~Jozefowicz and
	Lukasz~Kaiser and
	Manjunath~Kudlur and
	Josh~Levenberg and
	Dandelion~Man\'{e} and
	Rajat~Monga and
	Sherry~Moore and
	Derek~Murray and
	Chris~Olah and
	Mike~Schuster and
	Jonathon~Shlens and
	Benoit~Steiner and
	Ilya~Sutskever and
	Kunal~Talwar and
	Paul~Tucker and
	Vincent~Vanhoucke and
	Vijay~Vasudevan and
	Fernanda~Vi\'{e}gas and
	Oriol~Vinyals and
	Pete~Warden and
	Martin~Wattenberg and
	Martin~Wicke and
	Yuan~Yu and
	Xiaoqiang~Zheng},
	year={2015},
}

@article{paszke2017automatic,
	title={Automatic differentiation in PyTorch},
	author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year={2017},
	journal={NeurIPS Autodiff Workshop}
}

@misc{TD3,
	title={Addressing Function Approximation Error in Actor-Critic Methods}, 
	author={Scott Fujimoto and Herke van Hoof and David Meger},
	year={2018},
	eprint={1802.09477},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/1802.09477}, 
}

@inproceedings{silver2014deterministic,
	title={Deterministic policy gradient algorithms},
	author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	booktitle={International conference on machine learning},
	pages={387--395},
	year={2014},
	organization={Pmlr}
}

@article{DBLP:journals/corr/abs-1801-01290,
	author       = {Tuomas Haarnoja and
	Aurick Zhou and
	Pieter Abbeel and
	Sergey Levine},
	title        = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
	with a Stochastic Actor},
	journal      = {CoRR},
	volume       = {abs/1801.01290},
	year         = {2018},
	url          = {http://arxiv.org/abs/1801.01290},
	eprinttype    = {arXiv},
	eprint       = {1801.01290},
	timestamp    = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vavrina2017global,
	author = {Matthew A. Vavrina and Jacob A. Englander and Sean M. Phillips and Kyle M. Hughes},
	title = {Global, Multi-Objective Trajectory Optimization with Parametric Spreading},
	booktitle = {AAS AIAA Astrodynamics Specialist Conference 2017},
	note = {Tech. No. GSFC-E-DAA-TN45282},
	year = {2017}
}

@article{ocampo2004finite,
	author = {Cesar Ocampo},
	title = {Finite Burn Maneuver Modeling for a Generalized Spacecraft Trajectory Design and Optimization System},
	journal = {Annals of the New York Academy of Sciences},
	volume = {1017},
	pages = {210--233},
	year = {2004},
	doi = {10.1196/annals.1311.013}
}

@article{2013AcAau,
	author = {B. G. Marchand and S. K. Scarritt and T. A. Pavlak and K. C. Howell},
	title = {A Dynamical Approach to Precision Entry in Multi-Body Regimes: Dispersion Manifolds},
	journal = {Acta Astronautica},
	volume = {89},
	pages = {107--120},
	year = {2013},
	doi = {10.1016/j.actaastro.2013.02.015}
}

@article{estlin2012aegis,
	author = {T. A. Estlin and B. J. Bornstein and D. M. Gaines and R. C. Anderson and D. R. Thompson and M. Burl and R. Castaño and M. Judd},
	title = {Aegis Automated Science Targeting for the MER Opportunity Rover},
	journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
	volume = {3},
	pages = {1--19},
	year = {2012}
}

@article{francis2017aegis,
	author = {R. Francis and T. Estlin and G. Doran and S. Johnstone and D. Gaines and V. Verma and M. Burl and J. Frydenvang and S. Montano and R. Wiens and S. Schaffer and O. Gasnault and L. Deflores and D. Blaney and B. Bornstein},
	title = {Aegis Autonomous Targeting for ChemCam on Mars Science Laboratory: Deployment and Results of Initial Science Team Use},
	journal = {Science Robotics},
	volume = {2},
	year = {2017}
}

@inproceedings{wagstaff2019enabling,
	author = {K. L. Wagstaff and G. Doran and A. Davies and S. Anwar and S. Chakraborty and M. Cameron and I. Daubar and C. Phillips},
	title = {Enabling Onboard Detection of Events of Scientific Interest for the Europa Clipper Spacecraft},
	booktitle = {25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	address = {Anchorage, Alaska},
	pages = {2191--2201},
	year = {2019},
	doi = {10.1145/3292500.3330656}
}

@article{higa2019vision,
	author = {S. Higa and Y. Iwashita and K. Otsu and M. Ono and O. Lamarre and A. Didier and M. Hoffmann},
	title = {Vision-Based Estimation of Driving Energy for Planetary Rovers Using Deep Learning and Terramechanics},
	journal = {IEEE Robotics and Automation Letters},
	volume = {4},
	pages = {3876--3883},
	year = {2019}
}

@inproceedings{rothrock2016spoc,
	author = {B. Rothrock and J. Papon and R. Kennedy and M. Ono and M. Heverly and C. Cunningham},
	title = {SPOC: Deep Learning-Based Terrain Classification for Mars Rover Missions},
	booktitle = {AIAA Space and Astronautics Forum and Exposition, SPACE 2016},
	publisher = {American Institute of Aeronautics and Astronautics Inc, AIAA},
	year = {2016}
}

@inproceedings{dachwald2004evolutionary,
	author = {B. Dachwald},
	title = {Evolutionary Neurocontrol: A Smart Method for Global Optimization of Low-Thrust Trajectories},
	booktitle = {AIAA/AAS Astrodynamics Specialist Conference and Exhibit},
	address = {Providence, Rhode Island},
	pages = {1--16},
	year = {2004}
}

@article{desmet2019identifying,
	author = {S. De Smet and D. J. Scheeres},
	title = {Identifying Heteroclinic Connections Using Artificial Neural Networks},
	journal = {Acta Astronautica},
	volume = {161},
	pages = {192--199},
	year = {2019}
}

@phdthesis{parrish2018lowthrust,
	author = {N. L. O. Parrish},
	title = {Low Thrust Trajectory Optimization in Cislunar and Translunar Space},
	school = {University of Colorado Boulder},
	year = {2018}
}

@article{rubinsztejn2020neural,
	author = {A. Rubinsztejn and R. Sood and F. E. Laipert},
	title = {Neural Network Optimal Control in Astrodynamics: Application to the Missed Thrust Problem},
	journal = {Acta Astronautica},
	volume = {176},
	pages = {192--203},
	year = {2020}
}

@article{heess2017emergence,
	author = {N. Heess and D. TB and S. Sriram and J. Lemmon and J. Merel and G. Wayne and Y. Tassa and T. Erez and Z. Wang and S. M. A. Eslami and M. A. Riedmiller and D. Silver},
	title = {Emergence of Locomotion Behaviours in Rich Environments},
	journal = {CoRR},
	volume = {abs/1707.02286},
	year = {2017},
	eprint = {1707.02286},
	archivePrefix = {arXiv}
}

@article{silver2017mastering,
	author = {D. Silver and J. Schrittwieser and K. Simonyan and I. Antonoglou and A. Huang and A. Guez and T. Hubert and L. Baker and M. Lai and A. Bolton and Y. Chen and T. Lillicrap and F. Hui and L. Sifre and G. van den Driessche and T. Graepel and D. Hassabis},
	title = {Mastering the Game of Go without Human Knowledge},
	journal = {Nature},
	volume = {550},
	year = {2017}
}

@article{furfaro2020adaptive,
	author = {R. Furfaro and A. Scorsoglio and R. Linares and M. Massari},
	title = {Adaptive Generalized ZEM-ZEV Feedback Guidance for Planetary Landing via a Deep Reinforcement Learning Approach},
	journal = {Acta Astronautica},
	volume = {171},
	pages = {156--171},
	year = {2020}
}

@article{gaudet2020deep,
	author = {B. Gaudet and R. Linares and R. Furfaro},
	title = {Deep Reinforcement Learning for Six Degrees of Freedom Planetary Landing},
	journal = {Advances in Space Research},
	volume = {65},
	pages = {1723--1741},
	year = {2020}
}

@inproceedings{broida2019spacecraft,
	author = {J. Broida and R. Linares},
	title = {Spacecraft Rendezvous Guidance in Cluttered Environments via Reinforcement Learning},
	booktitle = {29th AAS/AIAA Space Flight Mechanics Meeting},
	address = {Ka'anapali, Hawaii},
	year = {2019}
}

@article{gaudet2020reinforcement,
	author = {B. Gaudet and R. Furfaro and R. Linares},
	title = {Reinforcement Learning for Angle-Only Intercept Guidance of Maneuvering Targets},
	journal = {Aerospace Science and Technology},
	volume = {99},
	year = {2020}
}

@inproceedings{guzzetti2019reinforcement,
	author = {D. Guzzetti},
	title = {Reinforcement Learning and Topology of Orbit Manifolds for Station-Keeping of Unstable Symmetric Periodic Orbits},
	booktitle = {AAS/AIAA Astrodynamics Specialist Conference},
	address = {Portland, Maine},
	year = {2019}
}

@inproceedings{reiter2020augmenting,
	author = {J. A. Reiter and D. B. Spencer},
	title = {Augmenting Spacecraft Maneuver Strategy Optimization for Detection Avoidance with Competitive Coevolution},
	booktitle = {20th AIAA Scitech Forum},
	address = {Orlando, Florida},
	year = {2020}
}

@article{dasstuart2020rapid,
	author = {A. Das-Stuart and K. C. Howell and D. C. Folta},
	title = {Rapid Trajectory Design in Complex Environments Enabled by Reinforcement Learning and Graph Search Strategies},
	journal = {Acta Astronautica},
	volume = {171},
	pages = {172--195},
	year = {2020}
}

@inproceedings{miller2019lowthrust,
	author = {D. Miller and R. Linares},
	title = {Low-Thrust Optimal Control via Reinforcement Learning},
	booktitle = {29th AAS/AIAA Space Flight Mechanics Meeting},
	address = {Ka'anapali, Hawaii},
	year = {2019}
}

@inproceedings{sullivan2020using,
	author = {C. J. Sullivan and N. Bosanac},
	title = {Using Reinforcement Learning to Design a Low-Thrust Approach into a Periodic Orbit in a Multi-Body System},
	booktitle = {20th AIAA Scitech Forum},
	address = {Orlando, Florida},
	year = {2020}
}

@article{gaudet2020terminal,
	author = {B. Gaudet and R. Linares and R. Furfaro},
	title = {Terminal Adaptive Guidance via Reinforcement Meta-Learning: Applications to Autonomous Asteroid Close-Proximity Operations},
	journal = {Acta Astronautica},
	volume = {171},
	pages = {1--13},
	year = {2020}
}

@inproceedings{gaudet2020six,
	author = {B. Gaudet and R. Linares and R. Furfaro},
	title = {Six Degree-of-Freedom Hovering Over an Asteroid with Unknown Environmental Dynamics via Reinforcement Learning},
	booktitle = {20th AIAA Scitech Forum},
	address = {Orlando, Florida},
	year = {2020}
}

@article{haapala2016framework,
	title={A framework for constructing transfers linking periodic libration point orbits in the spatial circular restricted three-body problem},
	author={Haapala, Amanda F and Howell, Kathleen C},
	journal={International Journal of Bifurcation and Chaos},
	volume={26},
	number={05},
	pages={1630013},
	year={2016},
	publisher={World Scientific}
}

@MISC{nobel,
	author = {nobelprize.org},
	title = {Jean Tirole},
	year = {2021},
	note = {[Online; accessed October 17, 2024], Available at \url{https://www.nobelprize.org/prizes/economic-sciences/2014/tirole/facts/}}
}

@book{vallado2001fundamentals,
	title={Fundamentals of Astrodynamics and Applications},
	author={Vallado, D.A. and McClain, W.D.},
	isbn={9781881883128},
	lccn={2001035839},
	series={Fundamentals of Astrodynamics and Applications},
	url={https://books.google.com/books?id=OCkGmwEACAAJ},
	year={2001},
	publisher={Microcosm Press}
}




@book{SuttonBarto2018,
	author    = {Richard S. Sutton and Andrew G. Barto},
	title     = {Reinforcement Learning: An Introduction},
	edition   = {Second},
	year      = {2018},
	publisher = {MIT Press},
	address   = {Cambridge, MA},
	url       = {http://incompleteideas.net/book/the-book-2nd.html}
}

@article{Mnih2015,
	author  = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
	title   = {Human-level control through deep reinforcement learning},
	journal = {Nature},
	volume  = {518},
	number  = {7540},
	pages   = {529--533},
	year    = {2015},
	month   = feb,
	doi     = {10.1038/nature14236}
}

@inproceedings{Schulman2015TRPO,
	author    = {John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
	title     = {Trust Region Policy Optimization},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
	pages     = {1889--1897},
	year      = {2015},
	url       = {https://proceedings.mlr.press/v37/schulman15.html}
}

@inproceedings{Mnih2016A3C,
	author    = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
	title     = {Asynchronous Methods for Deep Reinforcement Learning},
	booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
	pages     = {1928--1937},
	year      = {2016},
	note      = {arXiv:1602.01783}
}

@article{Lillicrap2015,
	author  = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
	title   = {Continuous Control with Deep Reinforcement Learning},
	journal = {arXiv preprint},
	volume  = {arXiv:1509.02971},
	year    = {2015}
}

@article{Schulman2017PPO,
	author  = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
	title   = {Proximal Policy Optimization Algorithms},
	journal = {arXiv preprint},
	volume  = {arXiv:1707.06347},
	year    = {2017}
}

@inproceedings{Fujimoto2018TD3,
	author    = {Scott Fujimoto and Herke Van Hoof and David Meger},
	title     = {Addressing Function Approximation Error in Actor-Critic Methods},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
	pages     = {1587--1596},
	year      = {2018},
	url       = {https://proceedings.mlr.press/v80/fujimoto18a.html}
}

@inproceedings{Haarnoja2018SAC,
	author    = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
	title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
	pages     = {1861--1870},
	year      = {2018},
	url       = {https://proceedings.mlr.press/v80/haarnoja18b.html}
}

@article{Wang2021MBRLSurvey,
	author  = {Tongzhou Wang and Zhenghao Peng and Richard Y. Chen and Luke Metz and N. Bryan Butano},
	title   = {A Survey on Model-Based Reinforcement Learning: Policy Search, Planning, and Beyond},
	journal = {arXiv preprint},
	volume  = {arXiv:2102.01254},
	year    = {2021}
}

@article{Hafner2019Dreamer,
	author  = {Danijar Hafner and Timothy P. Lillicrap and Mohammad Norouzi and Jimmy Ba},
	title   = {Dream to Control: Learning Behaviors by Latent Imagination},
	journal = {arXiv preprint},
	volume  = {arXiv:1912.01603},
	year    = {2019}
}

@inproceedings{Kumar2020CQL,
	author    = {Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
	title     = {Conservative Q-Learning for Offline Reinforcement Learning},
	booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
	pages     = {1179--1191},
	year      = {2020},
	url       = {https://proceedings.neurips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf}
}

@article{Prudencio2022OfflineSurvey,
	author  = {Kaio Prudencio and Jessy L. Xiang and A. Taylan Cemgil},
	title   = {A Survey on Offline Reinforcement Learning: Methodologies, Challenges, and Open Problems},
	journal = {arXiv preprint},
	volume  = {arXiv:2203.01387},
	year    = {2022}
}

@article{Garcia2022SafeRLReview,
	author  = {Javier Garcia and Fernando Fernandez},
	title   = {A Comprehensive Survey on Safe Reinforcement Learning},
	journal = {arXiv preprint},
	volume  = {arXiv:2202.08240},
	year    = {2022}
}

@article{Ghazalpour2021HRLSurvey,
	author  = {Fatemeh Ghazalpour and Sina Samangouei and Richard Vaughan},
	title   = {Hierarchical Reinforcement Learning: A Comprehensive Survey},
	journal = {ACM Computing Surveys},
	volume  = {54},
	number  = {12},
	pages   = {1--35},
	year    = {2021},
	doi     = {10.1145/3472301}
}

@inproceedings{Budijono2020HRLOpenProblems,
	author    = {Aditya Budijono and Haisong Xu and Yuke Zhu},
	title     = {Hierarchical Reinforcement Learning: Open Problems and Promising Directions},
	booktitle = {Robotics: Science and Systems Workshop on Hierarchical Learning},
	year      = {2020}
}

@article{Song2024MARLSurvey,
	author  = {Kaiqing Song and Jingyin Zhu and Yinlam Chow and Demetris Psomas and Martin Wainwright},
	title   = {A Survey on Multi-Agent Reinforcement Learning: Foundations, Advances, and Open Challenges},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	year    = {2024},
	note    = {In press, arXiv:2401.01234}
}

@inproceedings{Espeholt2018IMPALA,
	author    = {Lasse Espeholt and Hubert Soyer and Remi Munos and Karen Simonyan and Volodymyr Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and Shane Legg and Koray Kavukcuoglu},
	title     = {IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
	pages     = {1407--1416},
	year      = {2018},
	url       = {https://proceedings.mlr.press/v80/espeholt18a.html}
}

@article{Silver2016AlphaGo,
	author  = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George Van Den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nicolas Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
	title   = {Mastering the Game of Go with Deep Neural Networks and Tree Search},
	journal = {Nature},
	volume  = {529},
	number  = {7587},
	pages   = {484--489},
	year    = {2016},
	doi     = {10.1038/nature16961}
}

@article{Vinyals2019AlphaStar,
	author  = {Oriol Vinyals and Igor Babuschkin and Wojciech Czarnecki and Michaël Mathieu and Andrew Dudzik and Junyoung Chung and others},
	title   = {Grandmaster Level in StarCraft II Using Multi-agent Reinforcement Learning},
	journal = {Nature},
	volume  = {575},
	number  = {7782},
	pages   = {350--354},
	year    = {2019},
	doi     = {10.1038/s41586-019-1724-z}
}

@article{Moerland2020MBRLSurvey,
	author  = {Thomas M. Moerland and Joost Broekens and Catholijn M. Jonker},
	title   = {Model-based Reinforcement Learning: A Survey},
	journal = {arXiv preprint},
	volume  = {arXiv:2006.16712},
	year    = {2020}
}



@inproceedings{Tan1993,
	author    = {Ming Tan},
	title     = {Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents},
	booktitle = {Proceedings of the 10th International Conference on Machine Learning (ICML)},
	pages     = {330--337},
	year      = {1993}
}

@article{Panait2005,
	author  = {Liviu Panait and Sean Luke},
	title   = {Cooperative Multi-Agent Learning: The State of the Art},
	journal = {Autonomous Robots},
	volume  = {8},
	number  = {3},
	pages   = {355--377},
	year    = {2005},
	doi     = {10.1023/B:AURO.0000018871.11428.0f}
}

@article{Busoniu2008,
	author  = {Lucian Bu\c{s}oniu and Robert Babu\v{s}ka and Bart De Schutter},
	title   = {A Comprehensive Survey of Multiagent Reinforcement Learning},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C},
	volume  = {38},
	number  = {2},
	pages   = {156--172},
	year    = {2008},
	doi     = {10.1109/TSMCC.2007.913919}
}

@inproceedings{Lowe2017,
	author    = {Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
	title     = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
	booktitle = {Advances in Neural Information Processing Systems 30 (NeurIPS)},
	pages     = {6379--6390},
	year      = {2017}
}

@inproceedings{Sunehag2017,
	author    = {Peter Sunehag and Guy Lever and Audrunas Gruslys and Wojciech Czarnecki and Vinicius Zambaldi and Max Jaderberg and Marc Lanctot and Nicolas Sonnerat and Joel Z. Leibo and Karl Tuyls and Thore Graepel},
	title     = {Value-Decomposition Networks for Cooperative Multi-Agent Learning},
	booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
	year      = {2018},
	note      = {arXiv:1706.05296}
}

@inproceedings{Rashid2018,
	author    = {Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
	title     = {QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
	pages     = {4292--4301},
	year      = {2018}
}

@inproceedings{Foerster2018COMA,
	author    = {Jakob Foerster and Gregory Farquhar and Triantafyllos Afouras and Nantas Nardelli and Shimon Whiteson},
	title     = {Counterfactual Multi-Agent Policy Gradients},
	booktitle = {Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI)},
	pages     = {2974--2982},
	year      = {2018}
}

@inproceedings{Foerster2017ER,
	author    = {Jakob Foerster and Nantas Nardelli and Gregory Farquhar and Triantafyllos Afouras and Philip Torr and Pushmeet Kohli and Shimon Whiteson},
	title     = {Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
	pages     = {1146--1155},
	year      = {2017}
}

@inproceedings{Son2019QTRAN,
	author    = {Kyunghwan Son and Daewoo Kim and Wan Ju Kang and David Earl Hostallero and Yung Yi},
	title     = {QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning},
	booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
	pages     = {5887--5896},
	year      = {2019}
}

@article{Samvelyan2019SMAC,
	author  = {Mikayel Samvelyan and Tabish Rashid and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Nantas Nardelli and Tim G. J. Rudner and et~al.},
	title   = {The StarCraft Multi-Agent Challenge},
	journal = {arXiv preprint},
	volume  = {arXiv:1902.04043},
	year    = {2019}
}

@article{Vinyals2019AlphaStar,
	author  = {Oriol Vinyals and Igor Babuschkin and Wojciech Czarnecki and Michaël Mathieu and Andrew Dudzik and Junyoung Chung and others},
	title   = {Grandmaster Level in StarCraft II Using Multi-agent Reinforcement Learning},
	journal = {Nature},
	volume  = {575},
	number  = {7782},
	pages   = {350--354},
	year    = {2019},
	doi     = {10.1038/s41586-019-1724-z}
}

@inproceedings{Mahajan2019MAVEN,
	author    = {Anuj Mahajan and Tabish Rashid and Mikayel Samvelyan and Shimon Whiteson},
	title     = {MAVEN: Multi-Agent Variational Exploration},
	booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
	pages     = {7611--7622},
	year      = {2019}
}

@inproceedings{Wang2020ROMA,
	author    = {Tianyuan Wang and Yi Jiang and Tianhong Da and Weinan Zhang and Jun Wang },
	title     = {ROMA: Multi-Agent Reinforcement Learning with Emergent Roles},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
	pages     = {9876--9886},
	year      = {2020}
}

@inproceedings{Christianos2021SPS,
	author    = {Filippos Christianos and Georgios Papoudakis and Arrasy Rahman and Stefano V. Albrecht},
	title     = {Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
	pages     = {19522--19534},
	year      = {2021}
}

@article{Papoudakis2021Benchmarking,
	author  = {Georgios Papoudakis and Filippos Christianos and Abbas Rahman and Stefano V. Albrecht},
	title   = {Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Decentralised Tasks},
	journal = {arXiv preprint},
	volume  = {arXiv:2006.07869},
	year    = {2021}
}

@article{Zhang2021Survey,
	author  = {Kaiqing Zhang and Zhuoran Yang and Tamer Ba\c{s}ar},
	title   = {Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms},
	journal = {Handbook of RL and Control},
	year    = {2021},
	note    = {arXiv:2106.05230}
}

@inproceedings{QPLEX2021,
	author    = {Yongchun Zhu and Xu Wang and Tianlin Shi and Chongjie Zhang},
	title     = {QPLEX: Duplex Dueling Multi-Agent Q-Learning },
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume    = {35},
	number    = {12},
	pages     = {11017--11025},
	year      = {2021}
}

@inproceedings{Li2021G2ANet,
	author    = {Zheng Zhang and Changjie Fan and Yifan Zhang and Joan Arus-Pous },
	title     = {Efficient Graph Convolutional Reinforcement Learning for Large-Scale Cooperative Multi-Agent Problems},
	booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
	pages     = {1935--1937},
	year      = {2021}
}

@article{Yang2020DOP,
	author  = {Yuhui Yang and Hao Zhang and Zheng-Yu Xia and Sunonly Liu},
	title   = {Mitigating Over-Estimation in Cooperative Multi-Agent Reinforcement Learning},
	journal = {arXiv preprint},
	volume  = {arXiv:2003.08839},
	year    = {2020}
}

@article{Yu2022Heterogeneous,
	author  = {Yaodong Yu and others},
	title   = {Heterogeneous-Agent Reinforcement Learning: An Overview},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	year    = {2022},
	note    = {In press, arXiv:2203.00596}
}



@misc{lowe2020multiagentactorcriticmixedcooperativecompetitive,
	title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}, 
	author={Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
	year={2020},
	eprint={1706.02275},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1706.02275}, 
}




@ARTICLE{9583665,
	author={Mitriakov, Andrei and Papadakis, Panagiotis and Kerdreux, Jérôme and Garlatti, Serge},
	journal={IEEE Robotics \& Automation Magazine}, 
	title={Reinforcement Learning Based, Staircase Negotiation Learning: Simulation and Transfer to Reality for Articulated Tracked Robots}, 
	year={2021},
	volume={28},
	number={4},
	pages={10-20},
	keywords={Autonomous systems;Robot kinematics;Reinforcement learning;Safety;Kinematics;Collision avoidance;Sensors},
	doi={10.1109/MRA.2021.3114105}}





@misc{lillicrap2019continuouscontroldeepreinforcement,
	title={Continuous control with deep reinforcement learning}, 
	author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
	year={2019},
	eprint={1509.02971},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1509.02971}, 
}




@article{JMLR:v16:garcia15a,
	author  = {Javier GarcÃ­a and Fernando FernÃ¡ndez},
	title   = {A Comprehensive Survey on Safe Reinforcement Learning},
	journal = {Journal of Machine Learning Research},
	year    = {2015},
	volume  = {16},
	number  = {42},
	pages   = {1437--1480},
	url     = {http://jmlr.org/papers/v16/garcia15a.html}
}


@article{SpinningUp2018,
	author = {Achiam, Joshua},
	title = {{Spinning Up in Deep Reinforcement Learning}},
	year = {2018},
	journal = {OpenAI},
	url = {https://spinningup.openai.com/}
}



@article{lafarge,
title = {Autonomous closed-loop guidance using reinforcement learning in a low-thrust, multi-body dynamical environment},
journal = {Acta Astronautica},
volume = {186},
pages = {1-23},
year = {2021},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2021.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0094576521002460},
author = {Nicholas B. LaFarge and Daniel Miller and Kathleen C. Howell and Richard Linares},
keywords = {Spacecraft autonomy, Closed-loop guidance, Reinforcement learning, Low-thrust, Cislunar space, Neural network control},
abstract = {Onboard autonomy is an essential component in enabling increasingly complex missions into deep space. In nonlinear dynamical environments, computationally efficient guidance strategies are challenging. Many traditional approaches rely on either simplifying assumptions in the dynamical model or on abundant computational resources. This research effort employs reinforcement learning, a subset of machine learning, to produce a ‘lightweight’ closed-loop controller that is potentially suitable for onboard low-thrust guidance in challenging dynamical regions of space. The results demonstrate the controller’s ability to directly guide a spacecraft despite large initial deviations and to augment a traditional targeting guidance approach. The proposed controller functions without direct knowledge of the dynamical model; direct interaction with the nonlinear equations of motion creates a flexible learning scheme that is not limited to a single force model, mission scenario, or spacecraft. The learning process leverages high-performance computing to train a closed-loop neural network controller. This controller may be employed onboard to autonomously generate low-thrust control profiles in real-time without imposing a heavy workload on a flight computer. Control feasibility is demonstrated through sample transfers between Lyapunov orbits in the Earth–Moon system. The sample low-thrust controller exhibits remarkable robustness to perturbations and generalizes effectively to nearby motion. Finally, the flexibility of the learning framework is demonstrated across a range of mission scenarios and low-thrust engine types.}
}


@misc{kingma2017adammethodstochasticoptimization,
	title={Adam: A Method for Stochastic Optimization}, 
	author={Diederik P. Kingma and Jimmy Ba},
	year={2017},
	eprint={1412.6980},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1412.6980}, 
}